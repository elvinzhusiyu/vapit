{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==============================================================================\n",
    "# Copyright 2020 Google LLC. This software is provided as-is, without warranty\n",
    "# or representation for any use or purpose. Your use of it is subject to your\n",
    "# agreement with Google.\n",
    "# ==============================================================================\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found existing installation: xgboost 1.1.1\n",
      "Uninstalling xgboost-1.1.1:\n",
      "  Successfully uninstalled xgboost-1.1.1\n",
      "Found existing installation: google-api-python-client 1.9.3\n",
      "Uninstalling google-api-python-client-1.9.3:\n",
      "  Successfully uninstalled google-api-python-client-1.9.3\n",
      "Found existing installation: gcsfs 0.7.0\n",
      "Uninstalling gcsfs-0.7.0:\n",
      "  Successfully uninstalled gcsfs-0.7.0\n",
      "Found existing installation: cloudml-hypertune 0.1.0.dev6\n",
      "Uninstalling cloudml-hypertune-0.1.0.dev6:\n",
      "  Successfully uninstalled cloudml-hypertune-0.1.0.dev6\n",
      "Found existing installation: google-cloud 0.34.0\n",
      "Uninstalling google-cloud-0.34.0:\n",
      "  Successfully uninstalled google-cloud-0.34.0\n",
      "Found existing installation: google-cloud-storage 1.29.0\n",
      "Uninstalling google-cloud-storage-1.29.0:\n",
      "  Successfully uninstalled google-cloud-storage-1.29.0\n",
      "Found existing installation: numpy 1.18.5\n",
      "Uninstalling numpy-1.18.5:\n",
      "  Successfully uninstalled numpy-1.18.5\n",
      "Found existing installation: pandas 1.0.4\n",
      "Uninstalling pandas-1.0.4:\n",
      "  Successfully uninstalled pandas-1.0.4\n",
      "Found existing installation: scikit-learn 0.23.2\n",
      "Uninstalling scikit-learn-0.23.2:\n",
      "  Successfully uninstalled scikit-learn-0.23.2\n",
      "Found existing installation: matplotlib 3.2.1\n",
      "Uninstalling matplotlib-3.2.1:\n",
      "  Successfully uninstalled matplotlib-3.2.1\n"
     ]
    }
   ],
   "source": [
    "!pip uninstall -r requirements-uninstall.txt -y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting xgboost==1.1.1\n",
      "  Using cached xgboost-1.1.1-py3-none-manylinux2010_x86_64.whl (127.6 MB)\n",
      "Collecting google-api-python-client==1.9.3\n",
      "  Using cached google_api_python_client-1.9.3-py3-none-any.whl (59 kB)\n",
      "Collecting gcsfs==0.7.0\n",
      "  Using cached gcsfs-0.7.0-py2.py3-none-any.whl (20 kB)\n",
      "Processing /home/jupyter/.cache/pip/wheels/a7/ff/87/e7bed0c2741fe219b3d6da67c2431d7f7fedb183032e00f81e/cloudml_hypertune-0.1.0.dev6-py2.py3-none-any.whl\n",
      "Collecting google-cloud==0.34.0\n",
      "  Using cached google_cloud-0.34.0-py2.py3-none-any.whl (1.8 kB)\n",
      "Collecting google-cloud-storage==1.29.0\n",
      "  Using cached google_cloud_storage-1.29.0-py2.py3-none-any.whl (85 kB)\n",
      "Collecting numpy==1.18.5\n",
      "  Using cached numpy-1.18.5-cp37-cp37m-manylinux1_x86_64.whl (20.1 MB)\n",
      "Collecting pandas==1.0.4\n",
      "  Using cached pandas-1.0.4-cp37-cp37m-manylinux1_x86_64.whl (10.1 MB)\n",
      "Collecting scikit-learn==0.23.2\n",
      "  Using cached scikit_learn-0.23.2-cp37-cp37m-manylinux1_x86_64.whl (6.8 MB)\n",
      "Collecting matplotlib==3.2.1\n",
      "  Using cached matplotlib-3.2.1-cp37-cp37m-manylinux1_x86_64.whl (12.4 MB)\n",
      "Requirement already satisfied: scipy in /opt/conda/lib/python3.7/site-packages (from xgboost==1.1.1->-r requirements.txt (line 1)) (1.4.1)\n",
      "Requirement already satisfied: google-auth>=1.16.0 in /opt/conda/lib/python3.7/site-packages (from google-api-python-client==1.9.3->-r requirements.txt (line 2)) (1.20.1)\n",
      "Requirement already satisfied: uritemplate<4dev,>=3.0.0 in /opt/conda/lib/python3.7/site-packages (from google-api-python-client==1.9.3->-r requirements.txt (line 2)) (3.0.1)\n",
      "Requirement already satisfied: six<2dev,>=1.6.1 in /opt/conda/lib/python3.7/site-packages (from google-api-python-client==1.9.3->-r requirements.txt (line 2)) (1.15.0)\n",
      "Requirement already satisfied: google-api-core<2dev,>=1.18.0 in /opt/conda/lib/python3.7/site-packages (from google-api-python-client==1.9.3->-r requirements.txt (line 2)) (1.22.1)\n",
      "Requirement already satisfied: google-auth-httplib2>=0.0.3 in /opt/conda/lib/python3.7/site-packages (from google-api-python-client==1.9.3->-r requirements.txt (line 2)) (0.0.3)\n",
      "Requirement already satisfied: httplib2<1dev,>=0.9.2 in /opt/conda/lib/python3.7/site-packages (from google-api-python-client==1.9.3->-r requirements.txt (line 2)) (0.18.1)\n",
      "Requirement already satisfied: decorator in /opt/conda/lib/python3.7/site-packages (from gcsfs==0.7.0->-r requirements.txt (line 3)) (4.4.2)\n",
      "Requirement already satisfied: aiohttp in /opt/conda/lib/python3.7/site-packages (from gcsfs==0.7.0->-r requirements.txt (line 3)) (3.6.2)\n",
      "Requirement already satisfied: requests in /opt/conda/lib/python3.7/site-packages (from gcsfs==0.7.0->-r requirements.txt (line 3)) (2.24.0)\n",
      "Requirement already satisfied: google-auth-oauthlib in /opt/conda/lib/python3.7/site-packages (from gcsfs==0.7.0->-r requirements.txt (line 3)) (0.4.1)\n",
      "Requirement already satisfied: fsspec>=0.8.0 in /opt/conda/lib/python3.7/site-packages (from gcsfs==0.7.0->-r requirements.txt (line 3)) (0.8.0)\n",
      "Requirement already satisfied: google-resumable-media<0.6dev,>=0.5.0 in /opt/conda/lib/python3.7/site-packages (from google-cloud-storage==1.29.0->-r requirements.txt (line 6)) (0.5.1)\n",
      "Requirement already satisfied: google-cloud-core<2.0dev,>=1.2.0 in /opt/conda/lib/python3.7/site-packages (from google-cloud-storage==1.29.0->-r requirements.txt (line 6)) (1.4.2)\n",
      "Requirement already satisfied: python-dateutil>=2.6.1 in /opt/conda/lib/python3.7/site-packages (from pandas==1.0.4->-r requirements.txt (line 8)) (2.8.1)\n",
      "Requirement already satisfied: pytz>=2017.2 in /opt/conda/lib/python3.7/site-packages (from pandas==1.0.4->-r requirements.txt (line 8)) (2020.1)\n",
      "Requirement already satisfied: joblib>=0.11 in /opt/conda/lib/python3.7/site-packages (from scikit-learn==0.23.2->-r requirements.txt (line 9)) (0.16.0)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in /opt/conda/lib/python3.7/site-packages (from scikit-learn==0.23.2->-r requirements.txt (line 9)) (2.1.0)\n",
      "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /opt/conda/lib/python3.7/site-packages (from matplotlib==3.2.1->-r requirements.txt (line 10)) (2.4.7)\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in /opt/conda/lib/python3.7/site-packages (from matplotlib==3.2.1->-r requirements.txt (line 10)) (1.2.0)\n",
      "Requirement already satisfied: cycler>=0.10 in /opt/conda/lib/python3.7/site-packages (from matplotlib==3.2.1->-r requirements.txt (line 10)) (0.10.0)\n",
      "Requirement already satisfied: rsa<5,>=3.1.4; python_version >= \"3.5\" in /opt/conda/lib/python3.7/site-packages (from google-auth>=1.16.0->google-api-python-client==1.9.3->-r requirements.txt (line 2)) (4.6)\n",
      "Requirement already satisfied: pyasn1-modules>=0.2.1 in /opt/conda/lib/python3.7/site-packages (from google-auth>=1.16.0->google-api-python-client==1.9.3->-r requirements.txt (line 2)) (0.2.8)\n",
      "Requirement already satisfied: setuptools>=40.3.0 in /opt/conda/lib/python3.7/site-packages (from google-auth>=1.16.0->google-api-python-client==1.9.3->-r requirements.txt (line 2)) (49.6.0.post20200814)\n",
      "Requirement already satisfied: cachetools<5.0,>=2.0.0 in /opt/conda/lib/python3.7/site-packages (from google-auth>=1.16.0->google-api-python-client==1.9.3->-r requirements.txt (line 2)) (4.1.1)\n",
      "Requirement already satisfied: googleapis-common-protos<2.0dev,>=1.6.0 in /opt/conda/lib/python3.7/site-packages (from google-api-core<2dev,>=1.18.0->google-api-python-client==1.9.3->-r requirements.txt (line 2)) (1.51.0)\n",
      "Requirement already satisfied: protobuf>=3.12.0 in /opt/conda/lib/python3.7/site-packages (from google-api-core<2dev,>=1.18.0->google-api-python-client==1.9.3->-r requirements.txt (line 2)) (3.13.0)\n",
      "Requirement already satisfied: chardet<4.0,>=2.0 in /opt/conda/lib/python3.7/site-packages (from aiohttp->gcsfs==0.7.0->-r requirements.txt (line 3)) (3.0.4)\n",
      "Requirement already satisfied: async-timeout<4.0,>=3.0 in /opt/conda/lib/python3.7/site-packages (from aiohttp->gcsfs==0.7.0->-r requirements.txt (line 3)) (3.0.1)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in /opt/conda/lib/python3.7/site-packages (from aiohttp->gcsfs==0.7.0->-r requirements.txt (line 3)) (1.3.0)\n",
      "Requirement already satisfied: multidict<5.0,>=4.5 in /opt/conda/lib/python3.7/site-packages (from aiohttp->gcsfs==0.7.0->-r requirements.txt (line 3)) (4.7.5)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /opt/conda/lib/python3.7/site-packages (from aiohttp->gcsfs==0.7.0->-r requirements.txt (line 3)) (20.1.0)\n",
      "Requirement already satisfied: idna<3,>=2.5 in /opt/conda/lib/python3.7/site-packages (from requests->gcsfs==0.7.0->-r requirements.txt (line 3)) (2.10)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.7/site-packages (from requests->gcsfs==0.7.0->-r requirements.txt (line 3)) (2020.6.20)\n",
      "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /opt/conda/lib/python3.7/site-packages (from requests->gcsfs==0.7.0->-r requirements.txt (line 3)) (1.25.10)\n",
      "Requirement already satisfied: requests-oauthlib>=0.7.0 in /opt/conda/lib/python3.7/site-packages (from google-auth-oauthlib->gcsfs==0.7.0->-r requirements.txt (line 3)) (1.3.0)\n",
      "Requirement already satisfied: pyasn1>=0.1.3 in /opt/conda/lib/python3.7/site-packages (from rsa<5,>=3.1.4; python_version >= \"3.5\"->google-auth>=1.16.0->google-api-python-client==1.9.3->-r requirements.txt (line 2)) (0.4.8)\n",
      "Requirement already satisfied: oauthlib>=3.0.0 in /opt/conda/lib/python3.7/site-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib->gcsfs==0.7.0->-r requirements.txt (line 3)) (3.1.0)\n",
      "Installing collected packages: numpy, xgboost, google-api-python-client, gcsfs, cloudml-hypertune, google-cloud, google-cloud-storage, pandas, scikit-learn, matplotlib\n",
      "\u001b[31mERROR: After October 2020 you may experience errors when installing or updating packages. This is because pip will change the way that it resolves dependency conflicts.\n",
      "\n",
      "We recommend you use --use-feature=2020-resolver to test your packages with the new resolver before it becomes the default.\n",
      "\n",
      "tfx 0.22.1 requires pyarrow<0.17,>=0.16, but you'll have pyarrow 1.0.1 which is incompatible.\n",
      "tfx-bsl 0.22.1 requires pyarrow<0.17,>=0.16.0, but you'll have pyarrow 1.0.1 which is incompatible.\n",
      "tensorflow-transform 0.22.0 requires tensorflow!=2.0.*,<2.3,>=1.15, but you'll have tensorflow 2.3.0 which is incompatible.\n",
      "tensorflow-probability 0.11.0 requires cloudpickle==1.3, but you'll have cloudpickle 1.5.0 which is incompatible.\n",
      "tensorflow-model-analysis 0.22.2 requires pyarrow<0.17,>=0.16, but you'll have pyarrow 1.0.1 which is incompatible.\n",
      "tensorflow-data-validation 0.22.2 requires joblib<0.15,>=0.12, but you'll have joblib 0.16.0 which is incompatible.\n",
      "tensorflow-data-validation 0.22.2 requires pyarrow<0.17,>=0.16, but you'll have pyarrow 1.0.1 which is incompatible.\n",
      "pandas-profiling 2.8.0 requires visions[type_image_path]==0.4.4, but you'll have visions 0.5.0 which is incompatible.\n",
      "cloud-tpu-client 0.10 requires google-api-python-client==1.8.0, but you'll have google-api-python-client 1.9.3 which is incompatible.\n",
      "apache-beam 2.23.0 requires httplib2<0.18.0,>=0.8, but you'll have httplib2 0.18.1 which is incompatible.\n",
      "apache-beam 2.23.0 requires mock<3.0.0,>=1.0.1, but you'll have mock 4.0.2 which is incompatible.\n",
      "apache-beam 2.23.0 requires oauth2client<4,>=2.0.1, but you'll have oauth2client 4.1.3 which is incompatible.\n",
      "apache-beam 2.23.0 requires pyarrow<0.18.0,>=0.15.1; python_version >= \"3.0\" or platform_system != \"Windows\", but you'll have pyarrow 1.0.1 which is incompatible.\u001b[0m\n",
      "Successfully installed cloudml-hypertune-0.1.0.dev6 gcsfs-0.7.0 google-api-python-client-1.9.3 google-cloud-0.34.0 google-cloud-storage-1.29.0 matplotlib-3.2.1 numpy-1.18.5 pandas-1.0.4 scikit-learn-0.23.2 xgboost-1.1.1\n"
     ]
    }
   ],
   "source": [
    "# https://cloud.google.com/ai-platform/training/docs/runtime-version-list\n",
    "!pip install -r requirements.txt\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import packages\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import xgboost as xgb\n",
    "from xgboost import XGBClassifier\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "PROJECT_ID = 'codev-257422'\n",
    "USER = 'cchatterj'\n",
    "BUCKET_NAME = 'chanchal-sandbox'\n",
    "FOLDER_NAME = 'ht-xgb-data'\n",
    "REGION = 'us-central1'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting ./setup.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile ./setup.py\n",
    "\n",
    "# python3\n",
    "# ==============================================================================\n",
    "# Copyright 2020 Google LLC. This software is provided as-is, without warranty\n",
    "# or representation for any use or purpose. Your use of it is subject to your\n",
    "# agreement with Google.\n",
    "# ==============================================================================\n",
    "\n",
    "from setuptools import find_packages\n",
    "from setuptools import setup\n",
    "\n",
    "REQUIRED_PACKAGES = ['xgboost==1.1.1',\n",
    "                     'pandas==1.0.4',\n",
    "                     'scikit-learn==0.23.2',\n",
    "                     'google-cloud-storage==1.29.0',\n",
    "                     'cloudml-hypertune',\n",
    "                    ]\n",
    "setup(\n",
    "    name='trainer',\n",
    "    version='0.1',\n",
    "    install_requires=REQUIRED_PACKAGES,\n",
    "    packages=find_packages(),\n",
    "    include_package_data=True,\n",
    "    description='Trainer package for XGBoost Task'\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the trainer directory and load the trainer files in it\n",
    "!mkdir -p trainer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting ./trainer/__init__.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile ./trainer/__init__.py\n",
    "\n",
    "# python3\n",
    "# ==============================================================================\n",
    "# Copyright 2020 Google LLC. This software is provided as-is, without warranty\n",
    "# or representation for any use or purpose. Your use of it is subject to your\n",
    "# agreement with Google.\n",
    "# ==============================================================================\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the config directory and load the trainer files in it\n",
    "!mkdir -p config\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting ./config/config.yaml\n"
     ]
    }
   ],
   "source": [
    "%%writefile ./config/config.yaml\n",
    "\n",
    "# python3\n",
    "# ==============================================================================\n",
    "# Copyright 2020 Google LLC. This software is provided as-is, without warranty\n",
    "# or representation for any use or purpose. Your use of it is subject to your\n",
    "# agreement with Google.\n",
    "# ==============================================================================\n",
    "\n",
    "#trainingInput:\n",
    "#  scaleTier: CUSTOM\n",
    "#  masterType: n1-highmem-8\n",
    "#  masterConfig:\n",
    "#    acceleratorConfig:\n",
    "#      count: 1\n",
    "#      type: NVIDIA_TESLA_T4\n",
    "\n",
    "trainingInput:\n",
    "  scaleTier: STANDARD-1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting ./trainer/train.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile ./trainer/train.py\n",
    "\n",
    "# python3\n",
    "# ==============================================================================\n",
    "# Copyright 2020 Google LLC. This software is provided as-is, without warranty\n",
    "# or representation for any use or purpose. Your use of it is subject to your\n",
    "# agreement with Google.\n",
    "# ==============================================================================\n",
    "\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import xgboost as xgb\n",
    "from xgboost import XGBClassifier\n",
    "from google.cloud import storage\n",
    "from datetime import datetime\n",
    "from pytz import timezone\n",
    "import hypertune\n",
    "from sklearn import metrics\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "\n",
    "    INPUT_FILE_NAME = 'Step10_Final_dataset.csv'\n",
    "    BUCKET_NAME = 'chanchal-sandbox'\n",
    "    FOLDER_NAME = 'ht-xgb-data'\n",
    "    _TARGET_COLUMN = 'TARGET'\n",
    "\n",
    "    input_file = 'gs://' + BUCKET_NAME + '/' + FOLDER_NAME + '/' + INPUT_FILE_NAME\n",
    "\n",
    "    # Read the data\n",
    "    try:\n",
    "        dataset = pd.read_csv(input_file)\n",
    "    except:\n",
    "        print(\"Oops! That is invalid filename. Try again...\")\n",
    "\n",
    "    print(dataset.shape)\n",
    "\n",
    "    # ---------------------------------------\n",
    "    # Pre-processing code from customer\n",
    "    # ---------------------------------------\n",
    "\n",
    "    # Drop useless columns\n",
    "    dataset.drop(['LOAN_SEQUENCE_NUMBER'], axis=1, inplace=True)\n",
    "\n",
    "    # Inputs to an XGBoost model must be numeric. One hot encoding was previously found to yield better results \n",
    "    # than label encoding for the particular\n",
    "    strcols = [col for col in dataset.columns if dataset[col].dtype == 'object']\n",
    "    dataset = pd.get_dummies(dataset, columns=strcols)\n",
    "\n",
    "    # Train Test Split and write out the train-test files\n",
    "\n",
    "    # Split with a small test size so as to allow our model to train on more data\n",
    "    X_train, X_test, y_train, y_test = \\\n",
    "        train_test_split(dataset.drop(_TARGET_COLUMN, axis=1), \n",
    "                                      dataset[_TARGET_COLUMN], stratify=dataset[_TARGET_COLUMN], \n",
    "                                      shuffle=True, test_size=0.2\n",
    "                                     )\n",
    "    print(\"X_train shape = \", X_train.shape)\n",
    "    print(\"X_test  shape = \", X_test.shape)\n",
    "\n",
    "    # count number of classes\n",
    "    values, counts = np.unique(y_train, return_counts=True)\n",
    "    NUM_CLASSES = len(values)\n",
    "\n",
    "    # ---------------------------------------\n",
    "    # Train model\n",
    "    # ---------------------------------------\n",
    "\n",
    "    params = {\n",
    "        'n_estimators': 100,\n",
    "        'max_depth': 3,\n",
    "        'booster': 'gbtree',\n",
    "        'min_child_weight': 1,\n",
    "        'learning_rate': 0.1,\n",
    "        'gamma': 0,\n",
    "        'subsample': 1,\n",
    "        'colsample_bytree': 1,\n",
    "        'reg_alpha': 0,\n",
    "        'objective': 'multi:softprob',\n",
    "        'num_class': NUM_CLASSES\n",
    "        }\n",
    "    xgb_model = XGBClassifier(**params)\n",
    "    #xgb_model.set_params(**params)\n",
    "    xgb_model.fit(X_train, y_train)\n",
    "\n",
    "    # ---------------------------------------\n",
    "    # Save the model to GCS\n",
    "    # ---------------------------------------\n",
    "\n",
    "    bst_filename = 'model.bst'\n",
    "    bst = xgb_model.get_booster()\n",
    "    bst.save_model(bst_filename)\n",
    "    bucket = storage.Client().bucket(BUCKET_NAME)\n",
    "    blob = bucket.blob('{}/{}'.format(\n",
    "        datetime.now().strftime(FOLDER_NAME+'/models/model_%Y%m%d_%H%M%S'),\n",
    "        bst_filename))\n",
    "    blob.upload_from_filename(bst_filename)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "------\n",
    "# Hyper Parameter Tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting ./config/hptuning_config.yaml\n"
     ]
    }
   ],
   "source": [
    "%%writefile ./config/hptuning_config.yaml\n",
    "\n",
    "# python3\n",
    "# ==============================================================================\n",
    "# Copyright 2020 Google LLC. This software is provided as-is, without warranty\n",
    "# or representation for any use or purpose. Your use of it is subject to your\n",
    "# agreement with Google.\n",
    "# ==============================================================================\n",
    "\n",
    "\n",
    "# hptuning_config.yaml\n",
    "trainingInput:\n",
    "  scaleTier: STANDARD-1\n",
    "  hyperparameters:\n",
    "    goal: MAXIMIZE\n",
    "    maxTrials: 5\n",
    "    maxParallelTrials: 5\n",
    "    hyperparameterMetricTag: roc_auc\n",
    "    enableTrialEarlyStopping: TRUE\n",
    "    params:\n",
    "      - parameterName: max_depth\n",
    "        type: INTEGER\n",
    "        minValue: 3\n",
    "        maxValue: 8\n",
    "      - parameterName: n_estimators\n",
    "        type: INTEGER\n",
    "        minValue: 50\n",
    "        maxValue: 200\n",
    "      - parameterName: booster\n",
    "        type: CATEGORICAL\n",
    "        categoricalValues: [\n",
    "          \"gbtree\",\n",
    "          \"gblinear\",\n",
    "          \"dart\"\n",
    "        ]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the xgb_trainer directory and load the trainer files in it\n",
    "!cp ./trainer/train.py ./trainer/train_hpt.py\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Appending to ./trainer/train_hpt.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile -a ./trainer/train_hpt.py\n",
    "\n",
    "    # predict the model with test file\n",
    "    y_pred = xgb_model.predict(X_test)\n",
    "\n",
    "    # Binarize multiclass labels\n",
    "    from sklearn import preprocessing\n",
    "    lb = preprocessing.LabelBinarizer()\n",
    "    lb.fit(y_test)\n",
    "    y_test = lb.transform(y_test)\n",
    "    y_pred = lb.transform(y_pred)\n",
    "\n",
    "    # Define the score we want to use to evaluate the classifier on\n",
    "    #score = metrics.accuracy_score(y_test, y_pred)\n",
    "    #score = metrics.average_precision_score(y_test, y_pred, average='macro')\n",
    "    #score = metrics.f1_score(y_test, y_pred, average='macro')\n",
    "    #score = metrics.fbeta_score(y_test, y_pred, average='macro', beta=0.5)\n",
    "    #score = metrics.hamming_loss(y_test, y_pred)\n",
    "    #score = metrics.log_loss(y_test, y_pred)\n",
    "    #score = metrics.precision_score(y_test, y_pred, average='macro')\n",
    "    #score = metrics.recall_score(y_test, y_pred, average='macro')\n",
    "    score = metrics.roc_auc_score(y_test, y_pred, average='macro')\n",
    "    #score = metrics.zero_one_loss(y_test, y_pred)\n",
    "\n",
    "    # The default name of the metric is training/hptuning/metric. \n",
    "    # We recommend that you assign a custom name. The only functional difference is that \n",
    "    # if you use a custom name, you must set the hyperparameterMetricTag value in the \n",
    "    # HyperparameterSpec object in your job request to match your chosen name.\n",
    "    # https://cloud.google.com/ml-engine/reference/rest/v1/projects.jobs#HyperparameterSpec\n",
    "    hpt = hypertune.HyperTune()\n",
    "    hpt.report_hyperparameter_tuning_metric(\n",
    "        hyperparameter_metric_tag='roc_auc',\n",
    "        metric_value=score,\n",
    "        global_step=1000\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Job Name =  xgb_train_cchatterj_100520_2155_HPT\n",
      "Job Dir  =  gs://chanchal-sandbox/ht-xgb-data/jobdir\n"
     ]
    }
   ],
   "source": [
    "from datetime import datetime\n",
    "from pytz import timezone\n",
    "\n",
    "JOBNAME_HPT = 'xgb_train_' + USER + '_' + \\\n",
    "              datetime.now(timezone('US/Pacific')).strftime(\"%m%d%y_%H%M\") + '_HPT'\n",
    "JOB_DIR = 'gs://' + BUCKET_NAME + '/' + FOLDER_NAME + '/' + 'jobdir'\n",
    "JOB_CONFIG = \"./config/hptuning_config.yaml\"\n",
    "print(\"Job Name = \", JOBNAME_HPT)\n",
    "print(\"Job Dir  = \", JOB_DIR)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Job [xgb_train_cchatterj_100520_2155_HPT] submitted successfully.\n",
      "Your job is still active. You may view the status of your job with the command\n",
      "\n",
      "  $ gcloud ai-platform jobs describe xgb_train_cchatterj_100520_2155_HPT\n",
      "\n",
      "or continue streaming the logs with the command\n",
      "\n",
      "  $ gcloud ai-platform jobs stream-logs xgb_train_cchatterj_100520_2155_HPT\n",
      "jobId: xgb_train_cchatterj_100520_2155_HPT\n",
      "state: QUEUED\n"
     ]
    }
   ],
   "source": [
    "# submit the hyperparameter training job\n",
    "! gcloud ai-platform jobs submit training $JOBNAME_HPT \\\n",
    "  --package-path $(pwd)/trainer \\\n",
    "  --module-name trainer.train_hpt \\\n",
    "  --python-version 3.7 \\\n",
    "  --runtime-version 2.2 \\\n",
    "  --job-dir $JOB_DIR \\\n",
    "  --region $REGION \\\n",
    "  --config $JOB_CONFIG\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "createTime: '2020-10-06T04:55:06Z'\n",
      "etag: cZE056HS38g=\n",
      "jobId: xgb_train_cchatterj_100520_2155_HPT\n",
      "startTime: '2020-10-06T04:55:07Z'\n",
      "state: RUNNING\n",
      "trainingInput:\n",
      "  hyperparameters:\n",
      "    enableTrialEarlyStopping: true\n",
      "    goal: MAXIMIZE\n",
      "    hyperparameterMetricTag: roc_auc\n",
      "    maxParallelTrials: 5\n",
      "    maxTrials: 5\n",
      "    params:\n",
      "    - maxValue: 8.0\n",
      "      minValue: 3.0\n",
      "      parameterName: max_depth\n",
      "      type: INTEGER\n",
      "    - maxValue: 200.0\n",
      "      minValue: 50.0\n",
      "      parameterName: n_estimators\n",
      "      type: INTEGER\n",
      "    - categoricalValues:\n",
      "      - gbtree\n",
      "      - gblinear\n",
      "      - dart\n",
      "      parameterName: booster\n",
      "      type: CATEGORICAL\n",
      "  jobDir: gs://chanchal-sandbox/ht-xgb-data/jobdir\n",
      "  packageUris:\n",
      "  - gs://chanchal-sandbox/ht-xgb-data/jobdir/packages/21466ad77472afdd348fe852c64a2b49e7b02caa1ada41f96344774dc7e21577/trainer-0.1.tar.gz\n",
      "  pythonModule: trainer.train_hpt\n",
      "  pythonVersion: '3.7'\n",
      "  region: us-central1\n",
      "  runtimeVersion: '2.2'\n",
      "  scaleTier: STANDARD_1\n",
      "trainingOutput:\n",
      "  completedTrialCount: '4'\n",
      "  consumedMLUnits: 2.7\n",
      "  hyperparameterMetricTag: roc_auc\n",
      "  isHyperparameterTuningJob: true\n",
      "  trials:\n",
      "  - endTime: '2020-10-06T05:01:32Z'\n",
      "    finalMetric:\n",
      "      objectiveValue: 0.90449\n",
      "      trainingStep: '1000'\n",
      "    hyperparameters:\n",
      "      booster: gbtree\n",
      "      max_depth: '5'\n",
      "      n_estimators: '173'\n",
      "    startTime: '2020-10-06T04:55:45.340243937Z'\n",
      "    state: SUCCEEDED\n",
      "    trialId: '5'\n",
      "  - endTime: '2020-10-06T05:02:10Z'\n",
      "    finalMetric:\n",
      "      objectiveValue: 0.904366\n",
      "      trainingStep: '1000'\n",
      "    hyperparameters:\n",
      "      booster: dart\n",
      "      max_depth: '7'\n",
      "      n_estimators: '69'\n",
      "    startTime: '2020-10-06T04:55:45.340096224Z'\n",
      "    state: SUCCEEDED\n",
      "    trialId: '3'\n",
      "  - endTime: '2020-10-06T05:01:22Z'\n",
      "    finalMetric:\n",
      "      objectiveValue: 0.899116\n",
      "      trainingStep: '1000'\n",
      "    hyperparameters:\n",
      "      booster: gblinear\n",
      "      max_depth: '6'\n",
      "      n_estimators: '125'\n",
      "    startTime: '2020-10-06T04:55:45.339840502Z'\n",
      "    state: SUCCEEDED\n",
      "    trialId: '1'\n",
      "  - endTime: '2020-10-06T05:01:24Z'\n",
      "    finalMetric:\n",
      "      objectiveValue: 0.897606\n",
      "      trainingStep: '1000'\n",
      "    hyperparameters:\n",
      "      booster: gbtree\n",
      "      max_depth: '8'\n",
      "      n_estimators: '95'\n",
      "    startTime: '2020-10-06T04:55:45.340006969Z'\n",
      "    state: SUCCEEDED\n",
      "    trialId: '2'\n",
      "\n",
      "View job in the Cloud Console at:\n",
      "https://console.cloud.google.com/mlengine/jobs/xgb_train_cchatterj_100520_2155_HPT?project=codev-257422\n",
      "\n",
      "View logs at:\n",
      "https://console.cloud.google.com/logs?resource=ml_job%2Fjob_id%2Fxgb_train_cchatterj_100520_2155_HPT&project=codev-257422\n"
     ]
    }
   ],
   "source": [
    "#check the hyperparameter training job status\n",
    "! gcloud ai-platform jobs describe $JOBNAME_HPT\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Hyperparameters:\n",
      "{\n",
      "    \"trialId\": \"5\",\n",
      "    \"hyperparameters\": {\n",
      "        \"n_estimators\": \"173\",\n",
      "        \"booster\": \"gbtree\",\n",
      "        \"max_depth\": \"5\"\n",
      "    },\n",
      "    \"finalMetric\": {\n",
      "        \"trainingStep\": \"1000\",\n",
      "        \"objectiveValue\": 0.9044898550139779\n",
      "    },\n",
      "    \"startTime\": \"2020-10-06T04:55:45.340243937Z\",\n",
      "    \"endTime\": \"2020-10-06T05:01:32Z\",\n",
      "    \"state\": \"SUCCEEDED\"\n",
      "}\n",
      "  trial_id objective   booster max_depth n_estimators\n",
      "0        5   0.90449    gbtree         5          173\n",
      "1        3  0.904366      dart         7           69\n",
      "2        1  0.899116  gblinear         6          125\n",
      "3        2  0.897606    gbtree         8           95\n",
      "4        4  0.890275      dart         6          137\n"
     ]
    }
   ],
   "source": [
    "# Getthe best hypertuned model\n",
    "\n",
    "from googleapiclient import discovery\n",
    "#from google.oauth2 import service_account\n",
    "import pandas as pd\n",
    "import json\n",
    "\n",
    "# Define the credentials for the service account\n",
    "#credentials = service_account.Credentials.from_service_account_file(<PATH TO CREDENTIALS JSON>)\n",
    "\n",
    "# Define the project id and the job id and format it for the api request\n",
    "project_id = 'projects/{}'.format(PROJECT_ID)\n",
    "job_id = '{}/jobs/{}'.format(project_id, JOBNAME_HPT)\n",
    "\n",
    "# Build the service\n",
    "ml = discovery.build('ml', 'v1', cache_discovery=False) #, credentials=credentials)\n",
    "\n",
    "# Execute the request and pass in the job id\n",
    "request = ml.projects().jobs().get(name=job_id).execute()\n",
    "\n",
    "# Get just the best hp values\n",
    "best_model = request['trainingOutput']['trials'][0]\n",
    "print('Best Hyperparameters:')\n",
    "print(json.dumps(best_model, indent=4))\n",
    "\n",
    "# Or put all the results into a dataframe\n",
    "# Create a list for each field\n",
    "trial_id, objective, booster, max_depth, n_estimators  = [], [], [], [], []\n",
    "\n",
    "# Loop through the json and append the values of each field to the lists\n",
    "for each in request['trainingOutput']['trials']:\n",
    "    trial_id.append(each['trialId'])\n",
    "    objective.append(each['finalMetric']['objectiveValue']) \n",
    "    booster.append(each['hyperparameters']['booster']) \n",
    "    max_depth.append(each['hyperparameters']['max_depth']) \n",
    "    n_estimators.append(each['hyperparameters']['n_estimators'])\n",
    "\n",
    "# Put the lsits into a df, transpose and name the columns\n",
    "df = pd.DataFrame([trial_id, objective, booster, max_depth, n_estimators]).T\n",
    "df.columns = ['trial_id', 'objective', 'booster', 'max_depth', 'n_estimators']\n",
    "\n",
    "# Display the df\n",
    "print(df)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "------\n",
    "# Training with Tuned Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gbtree 5 173\n"
     ]
    }
   ],
   "source": [
    "# Getthe best hypertuned model parameters\n",
    "BOOSTER = best_model['hyperparameters']['booster']\n",
    "MAX_DEPTH = int(best_model['hyperparameters']['max_depth'])\n",
    "N_ESTIMATORS = int(best_model['hyperparameters']['n_estimators'])\n",
    "print(BOOSTER, MAX_DEPTH, N_ESTIMATORS)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Job Name =  xgb_train_cchatterj_100520_2210\n",
      "Job Dir  =  gs://chanchal-sandbox/ht-xgb-data/jobdir\n"
     ]
    }
   ],
   "source": [
    "from datetime import datetime\n",
    "from pytz import timezone\n",
    "\n",
    "JOBNAME = 'xgb_train_' + USER + '_' + \\\n",
    "              datetime.now(timezone('US/Pacific')).strftime(\"%m%d%y_%H%M\")\n",
    "JOB_DIR = 'gs://' + BUCKET_NAME + '/' + FOLDER_NAME + '/' + 'jobdir'\n",
    "JOB_CONFIG = \"./config/config.yaml\"\n",
    "print(\"Job Name = \", JOBNAME)\n",
    "print(\"Job Dir  = \", JOB_DIR)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Job [xgb_train_cchatterj_100520_2210] submitted successfully.\n",
      "Your job is still active. You may view the status of your job with the command\n",
      "\n",
      "  $ gcloud ai-platform jobs describe xgb_train_cchatterj_100520_2210\n",
      "\n",
      "or continue streaming the logs with the command\n",
      "\n",
      "  $ gcloud ai-platform jobs stream-logs xgb_train_cchatterj_100520_2210\n",
      "jobId: xgb_train_cchatterj_100520_2210\n",
      "state: QUEUED\n"
     ]
    }
   ],
   "source": [
    "# https://cloud.google.com/sdk/gcloud/reference/ai-platform/jobs/submit/training\n",
    "\n",
    "# submit the training job\n",
    "! gcloud ai-platform jobs submit training $JOBNAME \\\n",
    "  --package-path $(pwd)/trainer \\\n",
    "  --module-name trainer.train \\\n",
    "  --region $REGION \\\n",
    "  --python-version 3.7 \\\n",
    "  --runtime-version 2.2 \\\n",
    "  --job-dir $JOB_DIR \\\n",
    "  --config $JOB_CONFIG \\\n",
    "  -- \\\n",
    "  --max_depth=$MAX_DEPTH \\\n",
    "  --n_estimators=$N_ESTIMATORS \\\n",
    "  --booster=$BOOSTER\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "createTime: '2020-10-06T05:10:06Z'\n",
      "endTime: '2020-10-06T05:11:39Z'\n",
      "etag: kusm44RdTdc=\n",
      "jobId: xgb_train_cchatterj_100520_2210\n",
      "startTime: '2020-10-06T05:10:38Z'\n",
      "state: SUCCEEDED\n",
      "trainingInput:\n",
      "  args:\n",
      "  - --max_depth=5\n",
      "  - --n_estimators=173\n",
      "  - --booster=gbtree\n",
      "  jobDir: gs://chanchal-sandbox/ht-xgb-data/jobdir\n",
      "  packageUris:\n",
      "  - gs://chanchal-sandbox/ht-xgb-data/jobdir/packages/1217a8da84965ca6bbe79b3857364a0bb8b7ca61cd83a8e5874ab86222be53ed/trainer-0.1.tar.gz\n",
      "  pythonModule: trainer.train\n",
      "  pythonVersion: '3.7'\n",
      "  region: us-central1\n",
      "  runtimeVersion: '2.2'\n",
      "  scaleTier: STANDARD_1\n",
      "trainingOutput:\n",
      "  consumedMLUnits: 0.07\n",
      "\n",
      "View job in the Cloud Console at:\n",
      "https://console.cloud.google.com/mlengine/jobs/xgb_train_cchatterj_100520_2210?project=codev-257422\n",
      "\n",
      "View logs at:\n",
      "https://console.cloud.google.com/logs?resource=ml_job%2Fjob_id%2Fxgb_train_cchatterj_100520_2210&project=codev-257422\n"
     ]
    }
   ],
   "source": [
    "# check the training job status\n",
    "! gcloud ai-platform jobs describe $JOBNAME\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "--------\n",
    "# Deploy the Model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using endpoint [https://ml.googleapis.com/]\n",
      "NAME       DEFAULT_VERSION_NAME\n",
      "tf_model   cchatterj_tf\n",
      "xgb_model  cchatterj_xgb_bst\n"
     ]
    }
   ],
   "source": [
    "MODEL_NAME = \"xgb_model\"\n",
    "MODEL_VERSION = \"cchatterj_xgb_bst\"\n",
    "MODEL_DIR = 'gs://' + BUCKET_NAME + '/' + FOLDER_NAME + '/xgb_models'\n",
    "MODEL_FRAMEWORK = \"XGBOOST\"\n",
    "MODEL_DESCRIPTION = \"SET8_MSE_loss_0.87\"\n",
    "\n",
    "!gcloud ai-platform models list\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!gcloud ai-platform models list\n",
    "#!gcloud ai-platform versions delete \"Chanchals_Pkl_v1\" --model \"xgb_model\" -q\n",
    "#!gcloud ai-platform models delete \"xgb_model\" -q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Using endpoint [https://ml.googleapis.com/]', 'xgb_model  cchatterj_xgb_bst']\n",
      "Model xgb_model exist\n"
     ]
    }
   ],
   "source": [
    "# create the model if it doesn't already exist\n",
    "modelname = !gcloud ai-platform models list | grep -w $MODEL_NAME\n",
    "print(modelname)\n",
    "if len(modelname) < 1:\n",
    "    print(\"Creating model \" + MODEL_NAME)\n",
    "    ! gcloud ai-platform models create $MODEL_NAME --regions $REGION --enable-logging\n",
    "else:\n",
    "    print(\"Model \" + MODEL_NAME + \" exist\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Using endpoint [https://ml.googleapis.com/]', 'cchatterj_xgb_bst  gs://chanchal-sandbox/ht-xgb-data/xgb_models/model_20201005_052148/  READY']\n",
      "Deleting model xgb_model version cchatterj_xgb_bst\n",
      "Using endpoint [https://ml.googleapis.com/]\n",
      "Deleting version [cchatterj_xgb_bst]......done.                                \n"
     ]
    }
   ],
   "source": [
    "# delete the model version if it already exists\n",
    "modelver = !gcloud ai-platform versions list --model $MODEL_NAME | grep -w $MODEL_VERSION\n",
    "print(modelver)\n",
    "if modelver[1] == 'Listed 0 items.':\n",
    "    print(\"Model version \" + MODEL_VERSION + \" doesnot exist\")\n",
    "else:\n",
    "    print(\"Deleting model \" + MODEL_NAME + \" version \" + MODEL_VERSION)\n",
    "    !gcloud ai-platform versions delete $MODEL_VERSION --model $MODEL_NAME -q\n",
    "\n",
    "#List the models\n",
    "#!gcloud ai-platform models list\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using endpoint [https://ml.googleapis.com/]\n",
      "Listed 0 items.\n"
     ]
    }
   ],
   "source": [
    "!gcloud ai-platform versions list --model $MODEL_NAME"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using endpoint [https://ml.googleapis.com/]\n",
      "NAME       DEFAULT_VERSION_NAME\n",
      "tf_model   cchatterj_tf\n",
      "xgb_model\n"
     ]
    }
   ],
   "source": [
    "!gcloud ai-platform models list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model Directory:  gs://chanchal-sandbox/ht-xgb-data/xgb_models\n",
      "Latest Model Directory =  gs://chanchal-sandbox/ht-xgb-data/xgb_models/model_20201005_052148/\n",
      "Using endpoint [https://ml.googleapis.com/]\n",
      "Creating version (this might take a few minutes)......done.                    \n"
     ]
    }
   ],
   "source": [
    "print(\"Model Directory: \", MODEL_DIR)\n",
    "\n",
    "# Get a list of model directories\n",
    "ALL_MODEL_DIRS = ! gsutil ls $MODEL_DIR\n",
    "# Pick the directory with the latest timestamp, in case you have trained multiple times\n",
    "if (\"CommandException\" in ALL_MODEL_DIRS[0]):\n",
    "    print(\"Create the model directory first\")\n",
    "else:\n",
    "    LATEST_MODEL_DIR = ALL_MODEL_DIRS[-1]\n",
    "print(\"Latest Model Directory = \", LATEST_MODEL_DIR)\n",
    "\n",
    "# Deploy the model\n",
    "! gcloud beta ai-platform versions create $MODEL_VERSION \\\n",
    "  --model=$MODEL_NAME \\\n",
    "  --origin=$LATEST_MODEL_DIR \\\n",
    "  --framework=$MODEL_FRAMEWORK \\\n",
    "  --runtime-version=2.2 \\\n",
    "  --python-version=3.7 \\\n",
    "  --description=$MODEL_DESCRIPTION \\\n",
    "  #--region=$REGION \\\n",
    "  #--labels='some_key'=${'XYZ'},another_key=\"another_value\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using endpoint [https://ml.googleapis.com/]\n",
      "NAME       DEFAULT_VERSION_NAME\n",
      "tf_model   cchatterj_tf\n",
      "xgb_model  cchatterj_xgb_bst\n",
      "Using endpoint [https://ml.googleapis.com/]\n",
      "NAME               DEPLOYMENT_URI                                                       STATE\n",
      "cchatterj_xgb_bst  gs://chanchal-sandbox/ht-xgb-data/xgb_models/model_20201005_052148/  READY\n",
      "Using endpoint [https://ml.googleapis.com/]\n",
      "defaultVersion:\n",
      "  createTime: '2020-10-06T05:12:52Z'\n",
      "  deploymentUri: gs://chanchal-sandbox/ht-xgb-data/xgb_models/model_20201005_052148/\n",
      "  description: SET8_MSE_loss_0.87\n",
      "  etag: msePKkMZOjY=\n",
      "  framework: XGBOOST\n",
      "  isDefault: true\n",
      "  machineType: mls1-c1-m2\n",
      "  name: projects/codev-257422/models/xgb_model/versions/cchatterj_xgb_bst\n",
      "  pythonVersion: '3.7'\n",
      "  runtimeVersion: '2.2'\n",
      "  state: READY\n",
      "etag: fFEc_OLPHv4=\n",
      "name: projects/codev-257422/models/xgb_model\n",
      "onlinePredictionLogging: true\n",
      "regions:\n",
      "- us-central1\n"
     ]
    }
   ],
   "source": [
    "# List all models\n",
    "!gcloud ai-platform models list\n",
    "# List all versions of the created model\n",
    "!gcloud ai-platform versions list --model $MODEL_NAME\n",
    "# Describe the Model\n",
    "!gcloud ai-platform models describe $MODEL_NAME \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "------\n",
    "# Predictions from the deployed model with test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(104044, 47)\n",
      "X_train shape =  (83235, 148)\n",
      "X_test  shape =  (20809, 148)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "INPUT_FILE_NAME = 'Step10_Final_dataset.csv'\n",
    "BUCKET_NAME = 'chanchal-sandbox'\n",
    "FOLDER_NAME = 'ht-xgb-data'\n",
    "_TARGET_COLUMN = 'TARGET'\n",
    "\n",
    "input_file = 'gs://' + BUCKET_NAME + '/' + FOLDER_NAME + '/' + INPUT_FILE_NAME\n",
    "\n",
    "# Read the data\n",
    "try:\n",
    "    dataset = pd.read_csv(input_file)\n",
    "except:\n",
    "    print(\"Oops! That is invalid filename. Try again...\")\n",
    "\n",
    "print(dataset.shape)\n",
    "\n",
    "# ---------------------------------------\n",
    "# Pre-processing code from customer\n",
    "# ---------------------------------------\n",
    "\n",
    "# Drop useless columns\n",
    "dataset.drop(['LOAN_SEQUENCE_NUMBER'], axis=1, inplace=True)\n",
    "\n",
    "# Inputs to an XGBoost model must be numeric. One hot encoding was previously found to yield better results \n",
    "# than label encoding for the particular\n",
    "strcols = [col for col in dataset.columns if dataset[col].dtype == 'object']\n",
    "dataset = pd.get_dummies(dataset, columns=strcols)\n",
    "\n",
    "# Train Test Split and write out the train-test files\n",
    "\n",
    "# Split with a small test size so as to allow our model to train on more data\n",
    "X_train, X_test, y_train, y_test = \\\n",
    "    train_test_split(dataset.drop(_TARGET_COLUMN, axis=1), \n",
    "                                  dataset[_TARGET_COLUMN], stratify=dataset[_TARGET_COLUMN], \n",
    "                                  shuffle=True, test_size=0.2\n",
    "                                 )\n",
    "print(\"X_train shape = \", X_train.shape)\n",
    "print(\"X_test  shape = \", X_test.shape)\n",
    "\n",
    "# Write test for prediction\n",
    "xgb_test = pd.concat([X_test, y_test], axis=1)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing samples 0 1000\n",
      "Processing samples 1000 2000\n",
      "Processing samples 2000 3000\n",
      "Processing samples 3000 4000\n",
      "Processing samples 4000 5000\n",
      "Processing samples 5000 6000\n",
      "Processing samples 6000 7000\n",
      "Processing samples 7000 8000\n",
      "Processing samples 8000 9000\n",
      "Processing samples 9000 10000\n",
      "Processing samples 10000 11000\n",
      "Processing samples 11000 12000\n",
      "Processing samples 12000 13000\n",
      "Processing samples 13000 14000\n",
      "Processing samples 14000 15000\n",
      "Processing samples 15000 16000\n",
      "Processing samples 16000 17000\n",
      "Processing samples 17000 18000\n",
      "Processing samples 18000 19000\n",
      "Processing samples 19000 20000\n",
      "Processing samples 20000 20809\n",
      "Predict Proba array size =  (20809, 4)\n",
      "Predict array size =  (20809,)\n",
      "CPU times: user 1.34 s, sys: 8 ms, total: 1.35 s\n",
      "Wall time: 28.2 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "import googleapiclient.discovery\n",
    "\n",
    "project_id = 'projects/{}'.format(PROJECT_ID)\n",
    "model_name = '{}/models/{}'.format(project_id, MODEL_NAME)\n",
    "service = googleapiclient.discovery.build('ml', 'v1', cache_discovery=False)\n",
    "\n",
    "#if version is not None:\n",
    "#    name += '/versions/{}'.format(version)\n",
    "\n",
    "pprobas = []\n",
    "batch_size = 1000\n",
    "n_samples = len(xgb_test) # this upper limit takes several seconds to execute\n",
    "#for i,j in zip([i for i in range(0, n_samples-batch_size-1, batch_size)],\n",
    "#               [i for i in range(batch_size, n_samples, batch_size)]):\n",
    "for i in range(0, n_samples, batch_size):\n",
    "    j = min(i+batch_size, n_samples)\n",
    "    print(\"Processing samples\", i, j)\n",
    "    response1 = service.projects().predict(name=model_name, \\\n",
    "                        body={'instances': xgb_test.drop([_TARGET_COLUMN], axis=1).iloc[i:j].values.tolist()} \\\n",
    "                                          ).execute()\n",
    "    if 'error' in response1:\n",
    "        print(response1['error']) #raise RuntimeError(response['error'])\n",
    "    else:\n",
    "        pprobas += response1['predictions']\n",
    "preds = np.argmax(pprobas, axis=1)\n",
    "print(\"Predict Proba array size = \", np.array(pprobas).shape)\n",
    "print(\"Predict array size = \", np.array(preds).shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "body={'instances': xgb_test.drop([_TARGET_COLUMN], axis=1).iloc[0:1000].values.tolist()}\n",
    "import json\n",
    "with open('xgb_test_data.json', 'w') as fp:\n",
    "    json.dump(body, fp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 8 ms, sys: 12 ms, total: 20 ms\n",
      "Wall time: 4.3 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "predict_results = !gcloud ai-platform predict \\\n",
    "  --model=$MODEL_NAME \\\n",
    "  --version=$MODEL_VERSION \\\n",
    "  --format='text' \\\n",
    "  --json-request='xgb_test_data.json'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'IPython.utils.text.SList'>\n",
      "(4001,)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['predictions[35][0]:  0.963602',\n",
       " 'predictions[35][1]:  0.0322716',\n",
       " 'predictions[35][2]:  0.00266592',\n",
       " 'predictions[35][3]:  0.00146053',\n",
       " 'predictions[36][0]:  0.961014',\n",
       " 'predictions[36][1]:  0.0361663',\n",
       " 'predictions[36][2]:  0.00148224',\n",
       " 'predictions[36][3]:  0.00133717',\n",
       " 'predictions[37][0]:  0.998528',\n",
       " 'predictions[37][1]:  0.00127727',\n",
       " 'predictions[37][2]:  9.90401e-05',\n",
       " 'predictions[37][3]:  9.61582e-05']"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(type(predict_results))\n",
    "print(np.array(predict_results).shape)\n",
    "predict_results[35*4+1:38*4+1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1, 0, 0, 0],\n",
       "       [1, 0, 0, 0],\n",
       "       [1, 0, 0, 0],\n",
       "       [1, 0, 0, 0],\n",
       "       [0, 0, 0, 1],\n",
       "       [1, 0, 0, 0],\n",
       "       [0, 0, 0, 1],\n",
       "       [1, 0, 0, 0],\n",
       "       [1, 0, 0, 0],\n",
       "       [1, 0, 0, 0],\n",
       "       [1, 0, 0, 0],\n",
       "       [1, 0, 0, 0],\n",
       "       [1, 0, 0, 0],\n",
       "       [1, 0, 0, 0],\n",
       "       [1, 0, 0, 0],\n",
       "       [1, 0, 0, 0],\n",
       "       [1, 0, 0, 0],\n",
       "       [1, 0, 0, 0],\n",
       "       [1, 0, 0, 0],\n",
       "       [1, 0, 0, 0]])"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "n_samples = 1000\n",
    "n_dim = 4\n",
    "predict_r = np.zeros((n_samples,n_dim), dtype=int)\n",
    "for i in range(1,len(predict_results)):\n",
    "    x = predict_results[i].split(':')\n",
    "    predict_r[(i-1)//n_dim][(i-1)%n_dim] = round(float(x[1]))\n",
    "predict_r[20:40]\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "environment": {
   "name": "tf2-2-3-gpu.2-3.m55",
   "type": "gcloud",
   "uri": "gcr.io/deeplearning-platform-release/tf2-2-3-gpu.2-3:m55"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
