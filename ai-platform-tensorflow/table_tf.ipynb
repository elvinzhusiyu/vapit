{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==============================================================================\n",
    "# Copyright 2021 Google LLC. This software is provided as-is, without warranty\n",
    "# or representation for any use or purpose. Your use of it is subject to your\n",
    "# agreement with Google.\n",
    "# ==============================================================================\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Uninstall old packages\n",
    "#!pip3 uninstall -r requirements-uninstall.txt -y\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install packages\n",
    "# https://cloud.google.com/ai-platform/training/docs/runtime-version-list\n",
    "#!pip3 install -r requirements-rt2.1.txt --user --ignore-installed\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import packages\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "import os\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '2' \n",
    "#0 = all messages are logged (default behavior)\n",
    "#1 = INFO messages are not printed\n",
    "#2 = INFO and WARNING messages are not printed\n",
    "#3 = INFO, WARNING, and ERROR messages are not printed\n",
    "\n",
    "import numpy as np\n",
    "from google.cloud import storage\n",
    "import tensorflow as tf\n",
    "#import matplotlib.pyplot as plt\n",
    "#from tensorflow.keras import models\n",
    "\n",
    "print(\"TF Version= \", tf.__version__)\n",
    "print(\"Keras Version= \", tf.keras.__version__)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Utility functions\n",
    "\n",
    "#------\n",
    "def find_best_model_dir(model_dir, offset=1, maxFlag=1):\n",
    "    # Get a list of model directories\n",
    "    all_models = ! gsutil ls $model_dir\n",
    "    print(\"\")\n",
    "    print(\"All Models = \")\n",
    "    print(*all_models, sep='\\n')\n",
    "\n",
    "    # Check if model dirs exist\n",
    "    if ((\"CommandException\" in all_models[0]) or (len(all_models) <= 1)):\n",
    "        print(\"Create the models first.\")\n",
    "        return \"\"\n",
    "\n",
    "    # Find the best model from checkpoints\n",
    "    import re\n",
    "    best_acc = -np.Inf\n",
    "    if (maxFlag != 1):\n",
    "        best_acc = np.Inf\n",
    "    best_model_dir = \"\"\n",
    "    tup_list = []\n",
    "    for i in range(1,len(all_models)):\n",
    "        all_floats = re.findall(r\"[-+]?\\d*\\.\\d+|\\d+\", all_models[i]) #Find the floats in the string\n",
    "        cur_acc = -float(all_floats[-offset]) #which item is the model optimization metric\n",
    "        tup_list.append([all_models[i],cur_acc])\n",
    "        if (maxFlag*(cur_acc > best_acc) or (1-maxFlag)*(cur_acc < best_acc)):\n",
    "            best_acc = cur_acc\n",
    "            best_model_dir = all_models[i]\n",
    "    if maxFlag:\n",
    "        tup_list.sort(key=lambda tup: tup[1], reverse=False)\n",
    "    else:\n",
    "        tup_list.sort(key=lambda tup: tup[1], reverse=True)\n",
    "    for i in range(len(tup_list)):\n",
    "        print(tup_list[i][0])\n",
    "    print(\"Best Accuracy  from Checkpoints = \", best_acc)\n",
    "    print(\"Best Model Dir from Checkpoints = \", best_model_dir)\n",
    "    \n",
    "    return best_model_dir\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from oauth2client.client import GoogleCredentials\n",
    "from googleapiclient import discovery\n",
    "from googleapiclient import errors\n",
    "import json\n",
    "\n",
    "#------\n",
    "# Python module to get the best hypertuned model parameters\n",
    "def pyth_get_hypertuned_parameters(project_name, job_name, maxFlag):\n",
    "    # Define the credentials for the service account\n",
    "    #credentials = service_account.Credentials.from_service_account_file(<PATH TO CREDENTIALS JSON>)\n",
    "    credentials = GoogleCredentials.get_application_default()\n",
    "\n",
    "    # Define the project id and the job id and format it for the api request\n",
    "    project_id = 'projects/{}'.format(project_name)\n",
    "    job_id = '{}/jobs/{}'.format(project_id, job_name)\n",
    "\n",
    "    # Build the service\n",
    "    cloudml = discovery.build('ml', 'v1', cache_discovery=False, credentials=credentials)\n",
    "\n",
    "    # Execute the request and pass in the job id\n",
    "    request = cloudml.projects().jobs().get(name=job_id)\n",
    "\n",
    "    try:\n",
    "        response = request.execute()\n",
    "        # Handle a successful request\n",
    "    except errors.HttpError as err:\n",
    "        tf.compat.v1.logging.error('There was an error getting the hyperparameters. Check the details:')\n",
    "        tf.compat.v1.logging.error(err._get_reason())\n",
    "\n",
    "    # Get just the best hp values\n",
    "    if maxFlag:\n",
    "        best_model = response['trainingOutput']['trials'][0]\n",
    "    else:\n",
    "        best_model = response['trainingOutput']['trials'][-1]\n",
    "    #print('Best Hyperparameters:')\n",
    "    #print(json.dumps(best_model, indent=4))\n",
    "\n",
    "    nTrials = len(response['trainingOutput']['trials'])\n",
    "    for i in range(0,nTrials):\n",
    "        state = response['trainingOutput']['trials'][i]['state']\n",
    "        trialId = response['trainingOutput']['trials'][i]['trialId']\n",
    "        objV = -1\n",
    "        if (state == 'SUCCEEDED'):\n",
    "            objV = response['trainingOutput']['trials'][i]['finalMetric']['objectiveValue']\n",
    "        print('objective=', objV, ' trialId=', trialId, state)\n",
    "        d = response['trainingOutput']['trials'][i]['hyperparameters']\n",
    "        for key, value in d.items():\n",
    "            print('    ', key, value)\n",
    "    return best_model\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "USER = 'cchatterj'\n",
    "PROJECT_ID = 'cchatterjee-sandbox' #$(gcloud config list project --format \"value(core.project)\")\n",
    "BUCKET_NAME = 'tuti_data'\n",
    "FOLDER_RESULTS = 'tensorflow_results'\n",
    "FOLDER_DATA = 'data_sets'\n",
    "REGION = 'us-central1'\n",
    "ZONE1 = 'us-central1-a'\n",
    "RUNTIME_VERSION = 2.1\n",
    "JOB_DIR   = 'gs://' + BUCKET_NAME + '/' + FOLDER_RESULTS + '/jobdir'\n",
    "MODEL_DIR = 'gs://' + BUCKET_NAME + '/' + FOLDER_RESULTS + '/models'\n",
    "INPUT_FILE_NAME = 'freddie_mac_processed_data.csv'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!gcloud config set project $PROJECT_ID\n",
    "!gcloud config set compute/zone $ZONE1\n",
    "!gcloud config set compute/region $REGION\n",
    "!gcloud config list\n",
    "#!gcloud config config-helper --format \"value(configuration.properties.core.project)\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clean old job logs, job packages and models\n",
    "!gsutil -m -q rm $JOB_DIR/packages/**\n",
    "!gsutil -m -q rm $MODEL_DIR/model**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ML Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the tf_trainer directory and load the trainer files in it\n",
    "!mkdir -p trainer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile ./trainer/inputs.py\n",
    "\n",
    "# Create the train and label lists\n",
    "import math\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "#------\n",
    "def load_data(input_file):\n",
    "    # Read the data\n",
    "    print(input_file)\n",
    "    #try:\n",
    "    table_data = pd.read_csv(input_file)\n",
    "    #except:\n",
    "    #    print(\"Oops! That is invalid filename. Try again...\")\n",
    "    #    return\n",
    "\n",
    "    print(table_data.shape)\n",
    "\n",
    "    # ---------------------------------------\n",
    "    # Pre-processing\n",
    "    # ---------------------------------------\n",
    "\n",
    "    # Drop useless columns\n",
    "    table_data.drop(['LOAN_SEQUENCE_NUMBER'], axis=1, inplace=True)\n",
    "\n",
    "    # Inputs to an XGBoost model must be numeric. One hot encoding was \n",
    "    # previously found to yield better results \n",
    "    # than label encoding for the particular\n",
    "    strcols = [col for col in table_data.columns if table_data[col].dtype == 'object']\n",
    "    table_data = pd.get_dummies(table_data, columns=strcols)\n",
    "\n",
    "    # Train Test Split and write out the train-test files\n",
    "\n",
    "    # Split with a small test size so as to allow our model to train on more data\n",
    "    X_train, X_test, y_train, y_test = \\\n",
    "        train_test_split(table_data.drop('TARGET', axis=1), \n",
    "                         table_data['TARGET'],\n",
    "                         stratify=table_data['TARGET'], \n",
    "                         shuffle=True, test_size=0.2\n",
    "                        )\n",
    "\n",
    "    # Remove Null and NAN\n",
    "    X_train = X_train.fillna(0)\n",
    "    X_test = X_test.fillna(0)\n",
    "    \n",
    "    # Check the shape\n",
    "    print(\"X_train shape = \", X_train.shape)\n",
    "    print(\"X_test  shape = \", X_test.shape)\n",
    "    \n",
    "    y_train_cat = tf.keras.utils.to_categorical(y_train)\n",
    "    y_test_cat = tf.keras.utils.to_categorical(y_test)\n",
    "    print(\"y_train shape = \", y_train_cat.shape)\n",
    "    print(\"y_test  shape = \", y_test_cat.shape)\n",
    "\n",
    "    # count number of classes\n",
    "    #values, counts = np.unique(y_train, return_counts=True)\n",
    "    #NUM_CLASSES = len(values)\n",
    "    #print(\"Number of classes \", NUM_CLASSES)\n",
    "\n",
    "    #train_dataset = tf.data.Dataset.from_tensor_slices((X_train, y_train))\n",
    "    #train_dataset = train_dataset.shuffle(100).batch(batch_size)\n",
    "    #test_dataset = tf.data.Dataset.from_tensor_slices((X_test, y_test))\n",
    "    #test_dataset = test_dataset.shuffle(100).batch(batch_size)\n",
    "    \n",
    "    return [X_train, X_test, y_train_cat, y_test_cat]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile ./trainer/model.py\n",
    "\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "\n",
    "def tf_model(input_dim, output_dim, depth, dropout_rate):\n",
    "    from tensorflow.keras.models import Sequential\n",
    "    from tensorflow.keras.layers import Dense, Dropout\n",
    "\n",
    "    decr = int((input_dim-output_dim-16)/depth) ^ 1\n",
    "\n",
    "    model = Sequential()\n",
    "    model.add(Dense(128, input_dim=input_dim, activation=tf.nn.relu))\n",
    "    for i in range(1,depth):\n",
    "        model.add(Dense(input_dim-i*decr, activation=tf.nn.relu, kernel_regularizer='l2'))\n",
    "    model.add(Dropout(dropout_rate))\n",
    "    model.add(Dense(output_dim, activation=tf.nn.softmax))\n",
    "    print(model.summary())\n",
    "\n",
    "    return model\n",
    "\n",
    "def custom_loss(y_true, y_pred):\n",
    "    custom_loss = mean(square(y_true - y_pred), axis=-1)\n",
    "    return custom_loss\n",
    "\n",
    "def custom_metric(y_true, y_pred):\n",
    "    custom_metric = mean(square(y_true - y_pred), axis=-1)\n",
    "    return custom_metric\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Package for distributed training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile ./setup.py\n",
    "\n",
    "# python3\n",
    "\n",
    "# ==============================================================================\n",
    "# Copyright 2020 Google LLC. This software is provided as-is, without warranty\n",
    "# or representation for any use or purpose. Your use of it is subject to your\n",
    "# agreement with Google.\n",
    "# ==============================================================================\n",
    "\n",
    "# https://cloud.google.com/ai-platform/training/docs/runtime-version-list\n",
    "\n",
    "from setuptools import find_packages\n",
    "from setuptools import setup\n",
    "\n",
    "#Runtime 2.1\n",
    "REQUIRED_PACKAGES = ['tensorflow==2.1.0',\n",
    "                     'pandas==0.25.3',\n",
    "                     'scikit-learn==0.22',\n",
    "                     'google-cloud-storage==1.23.0',\n",
    "                     'gcsfs==0.6.1',\n",
    "                     'cloudml-hypertune',\n",
    "                    ]\n",
    "setup(\n",
    "    name='trainer',\n",
    "    version='0.1',\n",
    "    install_requires=REQUIRED_PACKAGES,\n",
    "    packages=find_packages(),\n",
    "    include_package_data=True,\n",
    "    description='Trainer package for Tensorflow Task'\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile ./trainer/__init__.py\n",
    "# python3\n",
    "\n",
    "# ==============================================================================\n",
    "# Copyright 2020 Google LLC. This software is provided as-is, without warranty\n",
    "# or representation for any use or purpose. Your use of it is subject to your\n",
    "# agreement with Google.\n",
    "# ==============================================================================\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile ./trainer/train.py\n",
    "\n",
    "# python3\n",
    "# ==============================================================================\n",
    "# Copyright 2020 Google LLC. This software is provided as-is, without warranty\n",
    "# or representation for any use or purpose. Your use of it is subject to your\n",
    "# agreement with Google.\n",
    "# ==============================================================================\n",
    "\n",
    "import os\n",
    "import json\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import datetime as datetime\n",
    "from pytz import timezone\n",
    "import hypertune\n",
    "import argparse\n",
    "from trainer import model\n",
    "from trainer import inputs\n",
    "\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "import os\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3' \n",
    "#0 = all messages are logged (default behavior)\n",
    "#1 = INFO messages are not printed\n",
    "#2 = INFO and WARNING messages are not printed\n",
    "#3 = INFO, WARNING, and ERROR messages are not printed\n",
    "\n",
    "\n",
    "def parse_arguments():\n",
    "    \"\"\"Argument parser.\n",
    "    Returns:\n",
    "      Dictionary of arguments.\n",
    "    \"\"\"\n",
    "\n",
    "    parser = argparse.ArgumentParser()\n",
    "    parser.add_argument('--depth', default=3, type=int, \n",
    "                        help='Hyperparameter: depth of network')\n",
    "    parser.add_argument('--dropout_rate', default=0.02, type=float, \n",
    "                        help='Hyperparameter: Drop out rate')\n",
    "    parser.add_argument('--learning_rate', default=0.0001, type=float, \n",
    "                        help='Hyperparameter: initial learning rate')\n",
    "    parser.add_argument('--batch_size', default=4, type=int, \n",
    "                        help='batch size of the deep network')\n",
    "    parser.add_argument('--epochs', default=1, type=int, \n",
    "                        help='number of epochs.')\n",
    "    parser.add_argument('--model_dir', default=\"\",\n",
    "                        help='Directory to store model checkpoints and logs.')\n",
    "    parser.add_argument('--input_file', default=\"\",\n",
    "                        help='Directory to store model checkpoints and logs.')\n",
    "    parser.add_argument('--verbosity', choices=['DEBUG','ERROR','FATAL','INFO','WARN'],\n",
    "                        default='FATAL')\n",
    "    args, _ = parser.parse_known_args()\n",
    "    return args\n",
    "\n",
    "\n",
    "def get_callbacks(args, early_stop_patience: int = 3):\n",
    "    \"\"\"Creates Keras callbacks for model training.\"\"\"\n",
    "\n",
    "    # Get trialId\n",
    "    trialId = json.loads(os.environ.get(\"TF_CONFIG\", \"{}\")).get(\"task\", {}).get(\"trial\", \"\")\n",
    "    if trialId == '':\n",
    "        trialId = '0'\n",
    "    print(\"trialId=\", trialId)\n",
    "\n",
    "    curTime = datetime.datetime.now(timezone('US/Pacific')).strftime('%H%M%S')\n",
    "    \n",
    "    # Modify model_dir paths to include trialId\n",
    "    model_dir = args.model_dir + \"/checkpoints/cp-\"+curTime+\"-\"+trialId+\"-{val_accuracy:.4f}\"\n",
    "    log_dir   = args.model_dir + \"/log_dir\"\n",
    "\n",
    "    tensorboard_cb = tf.keras.callbacks.TensorBoard(log_dir, histogram_freq=1)\n",
    "    checkpoint_cb  = tf.keras.callbacks.ModelCheckpoint(model_dir, monitor='val_accuracy', mode='max', \n",
    "                                                        verbose=0, save_best_only=True,\n",
    "                                                        save_weights_only=False)\n",
    "    earlystop_cb   = tf.keras.callbacks.EarlyStopping(monitor='val_accuracy', patience=3)\n",
    "\n",
    "    return [checkpoint_cb, tensorboard_cb, earlystop_cb]\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "\n",
    "    # ---------------------------------------\n",
    "    # Parse Arguments\n",
    "    # ---------------------------------------\n",
    "    args = parse_arguments()\n",
    "    #args.model_dir = MODEL_DIR + datetime.datetime.now(timezone('US/Pacific')).strftime('/model_%m%d%Y_%H%M')\n",
    "    #args.input_file = 'gs://' + BUCKET_NAME + '/' + FOLDER_DATA + '/' + INPUT_FILE_NAME\n",
    "    print(args)\n",
    "\n",
    "    # ---------------------------------------\n",
    "    # Input Data & Preprocessing\n",
    "    # ---------------------------------------\n",
    "    print(\"Input and pre-process data ...\")\n",
    "    # Extract train_seismic, train_label\n",
    "    train_test_data = inputs.load_data(args.input_file)\n",
    "    X_train = train_test_data[0]\n",
    "    X_test  = train_test_data[1]\n",
    "    y_train = train_test_data[2]\n",
    "    y_test  = train_test_data[3]\n",
    "\n",
    "    # ---------------------------------------\n",
    "    # Train model\n",
    "    # ---------------------------------------\n",
    "    print(\"Creating model ...\")\n",
    "    tf_model = model.tf_model(X_train.shape[1], y_train.shape[1], \n",
    "                              depth=args.depth,\n",
    "                              dropout_rate=args.dropout_rate)\n",
    "    \n",
    "    tf_model.compile(optimizer=tf.keras.optimizers.Adam(lr=args.learning_rate),\n",
    "                     loss='mean_squared_error',\n",
    "                     metrics=['accuracy'])\n",
    "    \n",
    "    print(\"Fitting model ...\")\n",
    "    callbacks = get_callbacks(args, 3)\n",
    "    histy = tf_model.fit(np.array(X_train), y_train, \n",
    "                         epochs=args.epochs,\n",
    "                         batch_size=args.batch_size,\n",
    "                         validation_data=[np.array(X_test),y_test],\n",
    "                         callbacks=callbacks)\n",
    "\n",
    "    # TBD save history for visualization\n",
    "\n",
    "    final_epoch_accuracy = histy.history['accuracy'][-1]\n",
    "    final_epoch_count = len(histy.history['accuracy'])\n",
    "\n",
    "    print('final_epoch_accuracy = %.6f' % final_epoch_accuracy)\n",
    "    print('final_epoch_count = %2d' % final_epoch_count)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "# Run the training manually\n",
    "# Training parameters\n",
    "from datetime import datetime\n",
    "from pytz import timezone\n",
    "\n",
    "DEPTH = 2\n",
    "DROPOUT_RATE = 0.01\n",
    "LEARNING_RATE = 0.00005\n",
    "EPOCHS = 1\n",
    "BATCH_SIZE = 2\n",
    "\n",
    "MODEL_DIR_PYTH = MODEL_DIR + datetime.now(timezone('US/Pacific')).strftime('/model_%m%d%Y_%H%M')\n",
    "INPUT_FILE = 'gs://' + BUCKET_NAME + '/' + FOLDER_DATA + '/' + INPUT_FILE_NAME\n",
    "\n",
    "print('DEPTH = %2d' % DEPTH)\n",
    "print('DROPOUT_RATE = %.4f' % DROPOUT_RATE)\n",
    "print('LEARNING_RATE = %.6f' % LEARNING_RATE)\n",
    "print('EPOCHS = %2d' % EPOCHS)\n",
    "print('BATCH_SIZE = %2d' % BATCH_SIZE)\n",
    "print(\"MODEL_DIR =\", MODEL_DIR_PYTH)\n",
    "print(\"INPUT_FILE =\", INPUT_FILE)\n",
    "\n",
    "# Run training\n",
    "! python3 -m trainer.train --depth=$DEPTH --dropout_rate=$DROPOUT_RATE \\\n",
    "    --learning_rate=$LEARNING_RATE \\\n",
    "    --epochs=$EPOCHS \\\n",
    "    --batch_size=$BATCH_SIZE \\\n",
    "    --model_dir=$MODEL_DIR_PYTH \\\n",
    "    --input_file=$INPUT_FILE\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test with latest saved model\n",
    "best_model_dir_pyth = find_best_model_dir(MODEL_DIR_PYTH+'/checkpoints', offset=1, maxFlag=1)\n",
    "#acc = test_saved_model(best_model_dir_pyth, 0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "#***CREATE model_dir in local VM***\n",
    "\n",
    "from trainer import model\n",
    "\n",
    "# Copy the model from storage to local memory\n",
    "!gsutil -m cp -r $best_model_dir_pyth* ./model_dir\n",
    "\n",
    "# Load the model\n",
    "loaded_model = tf.keras.models.load_model('./model_dir', compile=False)#, \n",
    "               #custom_objects={\"custom_loss\": model.custom_loss, \"custom_mse\": model.custom_mse})\n",
    "print(\"Signature \", loaded_model.signatures)\n",
    "print(\"\")\n",
    "\n",
    "# Display model\n",
    "tf.keras.utils.plot_model(loaded_model, show_shapes=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "------\n",
    "# Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the config directory and load the trainer files in it\n",
    "!mkdir -p config\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile ./config/config.yaml\n",
    "\n",
    "# python3\n",
    "# ==============================================================================\n",
    "# Copyright 2020 Google LLC. This software is provided as-is, without warranty\n",
    "# or representation for any use or purpose. Your use of it is subject to your\n",
    "# agreement with Google.\n",
    "# ==============================================================================\n",
    "\n",
    "# https://cloud.google.com/sdk/gcloud/reference/ai-platform/jobs/submit/training#--scale-tier\n",
    "# https://www.kaggle.com/c/passenger-screening-algorithm-challenge/discussion/37087\n",
    "# https://cloud.google.com/ai-platform/training/docs/using-gpus\n",
    "\n",
    "#trainingInput:\n",
    "#    scaleTier: CUSTOM\n",
    "#    masterType: n1-highmem-16\n",
    "#    masterConfig:\n",
    "#        acceleratorConfig:\n",
    "#            count: 2\n",
    "#            type: NVIDIA_TESLA_V100\n",
    "\n",
    "#trainingInput:\n",
    "#  scaleTier: CUSTOM\n",
    "#  masterType: n1-highmem-8\n",
    "#  masterConfig:\n",
    "#    acceleratorConfig:\n",
    "#      count: 1\n",
    "#      type: NVIDIA_TESLA_T4\n",
    "\n",
    "#  masterType: n1-highcpu-16\n",
    "#  workerType: cloud_tpu\n",
    "#  workerCount: 1\n",
    "#  workerConfig:\n",
    "#    acceleratorConfig:\n",
    "#      type: TPU_V3\n",
    "#      count: 8\n",
    "\n",
    "#trainingInput:\n",
    "#  scaleTier: CUSTOM\n",
    "#  masterType: complex_model_m\n",
    "#  workerType: complex_model_m\n",
    "#  parameterServerType: large_model\n",
    "#  workerCount: 6\n",
    "#  parameterServerCount: 1\n",
    "#  scheduling:\n",
    "#    maxWaitTime: 3600s\n",
    "#    maxRunningTime: 7200s\n",
    "\n",
    "#trainingInput:\n",
    "#  runtimeVersion: \"2.1\"\n",
    "#  scaleTier: CUSTOM\n",
    "#  masterType: standard_gpu\n",
    "#  workerCount: 9\n",
    "#  workerType: standard_gpu\n",
    "#  parameterServerCount: 3\n",
    "#  parameterServerType: standard\n",
    "\n",
    "#trainingInput:\n",
    "#  scaleTier: BASIC-GPU\n",
    "    \n",
    "#trainingInput:\n",
    "#  region: us-central1\n",
    "#  scaleTier: CUSTOM\n",
    "#  masterType: complex_model_m\n",
    "#  workerType: complex_model_m_gpu\n",
    "#  parameterServerType: large_model\n",
    "#  workerCount: 4\n",
    "#  parameterServerCount: 2\n",
    "\n",
    "trainingInput:\n",
    "  scaleTier: standard-1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "from pytz import timezone\n",
    "JOBNAME_TRN = 'tf_train_'+ USER + '_' + \\\n",
    "           datetime.now(timezone('US/Pacific')).strftime(\"%m%d%y_%H%M\")\n",
    "JOB_CONFIG = \"config/config.yaml\"\n",
    "MODEL_DIR_TRN = MODEL_DIR + datetime.now(timezone('US/Pacific')).strftime('/model_%m%d%Y_%H%M')\n",
    "INPUT_FILE = 'gs://' + BUCKET_NAME + '/' + FOLDER_DATA + '/' + INPUT_FILE_NAME\n",
    "\n",
    "print(\"Job Name = \", JOBNAME_TRN)\n",
    "print(\"Job Dir  = \", JOB_DIR)\n",
    "print(\"MODEL_DIR =\", MODEL_DIR_TRN)\n",
    "print(\"INPUT_FILE =\", INPUT_FILE)\n",
    "\n",
    "# Training parameters\n",
    "DEPTH = 3\n",
    "DROPOUT_RATE = 0.02\n",
    "LEARNING_RATE = 0.0001\n",
    "EPOCHS = 2\n",
    "BATCH_SIZE = 2\n",
    "\n",
    "print('DEPTH = %2d' % DEPTH)\n",
    "print('DROPOUT_RATE = %.4f' % DROPOUT_RATE)\n",
    "print('LEARNING_RATE = %.6f' % LEARNING_RATE)\n",
    "print('EPOCHS = %2d' % EPOCHS)\n",
    "print('BATCH_SIZE = %2d' % BATCH_SIZE)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://cloud.google.com/sdk/gcloud/reference/ai-platform/jobs/submit/training\n",
    "\n",
    "TRAIN_LABELS = \"mode=train,owner=\"+USER\n",
    "\n",
    "# submit the training job\n",
    "! gcloud ai-platform jobs submit training $JOBNAME_TRN \\\n",
    "  --package-path $(pwd)/trainer \\\n",
    "  --module-name trainer.train \\\n",
    "  --region $REGION \\\n",
    "  --python-version 3.7 \\\n",
    "  --runtime-version $RUNTIME_VERSION \\\n",
    "  --job-dir $JOB_DIR \\\n",
    "  --config $JOB_CONFIG \\\n",
    "  --labels $TRAIN_LABELS \\\n",
    "  -- \\\n",
    "  --depth=$DEPTH \\\n",
    "  --dropout_rate=$DROPOUT_RATE \\\n",
    "  --learning_rate=$LEARNING_RATE \\\n",
    "  --epochs=$EPOCHS \\\n",
    "  --batch_size=$BATCH_SIZE \\\n",
    "  --model_dir=$MODEL_DIR_TRN \\\n",
    "  --input_file=$INPUT_FILE\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check the training job status\n",
    "! gcloud ai-platform jobs describe $JOBNAME_TRN\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print Errors\n",
    "response = ! gcloud logging read \"resource.labels.job_id=$JOBNAME_TRN severity>=ERROR\"\n",
    "for i in range(0,len(response)):\n",
    "    if 'message' in response[i]:\n",
    "        print(response[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test with latest saved model\n",
    "best_model_dir_trn = find_best_model_dir(MODEL_DIR_TRN+'/checkpoints', offset=1, maxFlag=1)\n",
    "#acc = test_saved_model(best_model_dir_trn, 0)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "------\n",
    "# Hyper Parameter Tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the tf directory and load the trainer files in it\n",
    "!cp ./trainer/train.py ./trainer/train_hpt.py\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile -a ./trainer/train_hpt.py\n",
    "\n",
    "    \"\"\"This method updates a CAIP HPTuning Job with a final metric for the job.\n",
    "    In TF2.X the user must either use hypertune or a custom callback with\n",
    "    tf.summary.scalar to update CAIP HP Tuning jobs. This function uses\n",
    "    hypertune, which appears to be the preferred solution. Hypertune also works\n",
    "    with containers, without code change.\n",
    "    Args:\n",
    "        metric_tag: The metric being optimized.  This MUST MATCH the\n",
    "          hyperparameterMetricTag specificed in the hyperparameter tuning yaml.\n",
    "        metric_value: The value to report at the end of model training.\n",
    "        global_step: An int value to specify the number of trainin steps completed\n",
    "          at the time the metric was reported.\n",
    "    \"\"\"\n",
    "\n",
    "    hpt = hypertune.HyperTune()\n",
    "    hpt.report_hyperparameter_tuning_metric(\n",
    "        hyperparameter_metric_tag='accuracy',\n",
    "        metric_value=final_epoch_accuracy,\n",
    "        global_step=final_epoch_count\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile ./config/hptuning_config.yaml\n",
    "\n",
    "# python3\n",
    "# ==============================================================================\n",
    "# Copyright 2020 Google LLC. This software is provided as-is, without warranty\n",
    "# or representation for any use or purpose. Your use of it is subject to your\n",
    "# agreement with Google.\n",
    "# ==============================================================================\n",
    "\n",
    "# https://cloud.google.com/ai-platform/training/docs/reference/rest/v1/projects.jobs\n",
    "# https://cloud.google.com/sdk/gcloud/reference/ai-platform/jobs/submit/training\n",
    "\n",
    "#trainingInput:\n",
    "#  scaleTier: CUSTOM\n",
    "#  masterType: n1-highmem-8\n",
    "#  masterConfig:\n",
    "#    acceleratorConfig:\n",
    "#      count: 1\n",
    "#      type: NVIDIA_TESLA_T4\n",
    "#\n",
    "#  masterType: standard_p100\n",
    "#  workerType: standard_p100\n",
    "#  parameterServerType: standard_p100\n",
    "#  workerCount: 8\n",
    "#  parameterServerCount: 1\n",
    "#  runtimeVersion: $RUNTIME_VERSION\n",
    "#  pythonVersion: '3.7'\n",
    "\n",
    "#trainingInput:\n",
    "#  scaleTier: CUSTOM\n",
    "#  masterType: complex_model_m\n",
    "#  workerType: complex_model_m\n",
    "#  parameterServerType: large_model\n",
    "#  workerCount: 9\n",
    "#  parameterServerCount: 3\n",
    "#  scheduling:\n",
    "#    maxWaitTime: 3600s\n",
    "#    maxRunningTime: 7200s\n",
    "\n",
    "#trainingInput:\n",
    "#  scaleTier: BASIC-GPU\n",
    "\n",
    "#trainingInput:\n",
    "#  scaleTier: CUSTOM\n",
    "#  masterType: n1-highmem-16\n",
    "#  masterConfig:\n",
    "#    acceleratorConfig:\n",
    "#      count: 2\n",
    "#      type: NVIDIA_TESLA_V100\n",
    "\n",
    "trainingInput:\n",
    "  scaleTier: STANDARD-1\n",
    "  hyperparameters:\n",
    "    goal: MAXIMIZE\n",
    "    hyperparameterMetricTag: accuracy\n",
    "    maxTrials: 8\n",
    "    maxParallelTrials: 4\n",
    "    enableTrialEarlyStopping: True\n",
    "    params:\n",
    "    - parameterName: depth\n",
    "      type: INTEGER\n",
    "      minValue: 2\n",
    "      maxValue: 4\n",
    "      scaleType: UNIT_LINEAR_SCALE\n",
    "    - parameterName: epochs\n",
    "      type: INTEGER\n",
    "      minValue: 1\n",
    "      maxValue: 4\n",
    "      scaleType: UNIT_LINEAR_SCALE\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "from pytz import timezone\n",
    "\n",
    "JOBNAME_HPT = 'tf_hptrn_' + USER + '_' + \\\n",
    "              datetime.now(timezone('US/Pacific')).strftime(\"%m%d%y_%H%M\")\n",
    "JOB_CONFIG = \"./config/hptuning_config.yaml\"\n",
    "MODEL_DIR_HPT = MODEL_DIR + datetime.now(timezone('US/Pacific')).strftime('/model_%m%d%Y_%H%M')\n",
    "INPUT_FILE = 'gs://' + BUCKET_NAME + '/' + FOLDER_DATA + '/' + INPUT_FILE_NAME\n",
    "\n",
    "print(\"Job Name = \", JOBNAME_HPT)\n",
    "print(\"Job Dir  = \", JOB_DIR)\n",
    "print(\"MODEL_DIR =\", MODEL_DIR_HPT)\n",
    "print(\"INPUT_FILE =\", INPUT_FILE)\n",
    "\n",
    "# Training parameters\n",
    "DROPOUT_RATE = 0.02\n",
    "LEARNING_RATE = 0.0001\n",
    "BATCH_SIZE = 2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# submit the training job\n",
    "HT_LABELS = \"mode=hypertrain,owner=\"+USER\n",
    "\n",
    "! gcloud ai-platform jobs submit training $JOBNAME_HPT \\\n",
    "  --package-path $(pwd)/trainer \\\n",
    "  --module-name trainer.train_hpt \\\n",
    "  --python-version 3.7 \\\n",
    "  --runtime-version $RUNTIME_VERSION \\\n",
    "  --region $REGION \\\n",
    "  --job-dir $JOB_DIR \\\n",
    "  --config $JOB_CONFIG \\\n",
    "  --labels $HT_LABELS \\\n",
    "  -- \\\n",
    "  --dropout_rate=$DROPOUT_RATE \\\n",
    "  --learning_rate=$LEARNING_RATE \\\n",
    "  --batch_size=$BATCH_SIZE \\\n",
    "  --model_dir=$MODEL_DIR_HPT \\\n",
    "  --input_file=$INPUT_FILE\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check the hyperparameter training job status\n",
    "! gcloud ai-platform jobs describe $JOBNAME_HPT\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print Errors\n",
    "response = ! gcloud logging read \"resource.labels.job_id=$JOBNAME_HPT severity>=ERROR\"\n",
    "for i in range(0,len(response)):\n",
    "    if 'message' in response[i]:\n",
    "        print(response[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the best model parameters from Cloud API\n",
    "best_model = pyth_get_hypertuned_parameters(PROJECT_ID, JOBNAME_HPT, 1)\n",
    "DEPTH = best_model['hyperparameters']['depth']\n",
    "EPOCHS = best_model['hyperparameters']['epochs']\n",
    "print('')\n",
    "print('Objective=', best_model['finalMetric']['objectiveValue'])\n",
    "print('DEPTH =', DEPTH)\n",
    "print('EPOCHS =', EPOCHS)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find count of checkpoints\n",
    "all_models = ! gsutil ls {MODEL_DIR_HPT+'/checkpoints'}\n",
    "print(\"Total Hypertrained Models=\", len(all_models))\n",
    "\n",
    "# Test with latest saved model\n",
    "best_model_dir_hyp = find_best_model_dir(MODEL_DIR_HPT+'/checkpoints', offset=1, maxFlag=1)\n",
    "#acc = test_saved_model(best_model_dir_hyp, 0)\n",
    "\n",
    "#import keras.backend as K\n",
    "#loaded_model = tf.keras.models.load_model(MODEL_DIR_PARAM+'/checkpoints')\n",
    "#print(\"learning_rate=\", K.eval(loaded_model.optimizer.lr))\n",
    "#tf.keras.utils.plot_model(loaded_model, show_shapes=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "--------\n",
    "# Deploy the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## https://cloud.google.com/ai-platform/prediction/docs/machine-types-online-prediction#available_machine_types\n",
    "# We need 2 versions of the same model:\n",
    "# 1. Batch prediction model deployed on a mls1-c1-m2 cluster\n",
    "# 2. Online prediction model deployed on a n1-standard-16 cluster\n",
    "# Batch prediction does not support GPU and n1-standard-16 clusters.\n",
    "\n",
    "# Run the Deploy Model section twice:\n",
    "# 1. As a BATCH Mode version use MODEL_VERSION = MODEL_VERSION_BATCH\n",
    "# 2. As a ONLINE Mode version use MODEL_VERSION = MODEL_VERSION_ONLINE\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Regional End points with python\n",
    "#https://cloud.google.com/ai-platform/prediction/docs/regional-endpoints#python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 480,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "List of Models in Global Endpoint)\n",
      "Using endpoint [https://ml.googleapis.com/]\n",
      "NAME                    DEFAULT_VERSION_NAME\n",
      "model                   model_0\n",
      "model1                  model1_0\n",
      "model_tf                model_tf_0\n",
      "model_tf_kfp            train_model_tf_200519_May0515899031777fc1f30775031217\n",
      "sklearn_demo_model      sklearn_demo_model_0\n",
      "sklearn_demo_model_kfp  ver_bc06438cefc734b2904ca4f658fb9390\n",
      "sklearn_model_demo_v1   sklearn_model_demo_v1_1\n",
      "sklearn_taxi_demo_v1    sklearn_taxi_demo_v1_1\n",
      "smh_tf_model_demo_v1    smh_tf_model_demo_v1_4\n",
      "test_seq_model_1        batch_v1\n",
      "tf_model_demo_v1        tf_model_demo_v1_0\n",
      "tf_model_demo_v1_kfp    train_tf_model_demo_v1_200217_Feb0215819918658b7d7aa9adb2a351\n",
      "tf_taxi_demo_v1         tf_taxi_demo_v1_0\n",
      "xgb_model               xgb_model_0\n",
      "xgboost_taxi_demo_v1    xgboost_taxi_demo_v1_3\n",
      "\n",
      "List of Versions in Global Endpoint)\n",
      "Using endpoint [https://ml.googleapis.com/]\n",
      "NAME       DEPLOYMENT_URI                                                                                STATE\n",
      "batch_v1   gs://tuti_data/tensorflow_results/models/model_02072021_1100/checkpoints/cp-111637-6-0.9560/  READY\n",
      "online_v1  gs://tuti_data/tensorflow_results/models/model_02072021_1100/checkpoints/cp-111637-6-0.9560/  READY\n"
     ]
    }
   ],
   "source": [
    "MODEL_NAME = \"test_seq_model_1\"\n",
    "MODEL_VERSION_BATCH  = \"batch_v1\"\n",
    "MODEL_VERSION_ONLINE = \"online_v1\"\n",
    "\n",
    "#Run this as Batch first then Online\n",
    "#MODEL_VERSION = MODEL_VERSION_BATCH\n",
    "MODEL_VERSION = MODEL_VERSION_ONLINE\n",
    "\n",
    "# List all models\n",
    "\n",
    "print(\"\\nList of Models in Global Endpoint)\")\n",
    "!gcloud ai-platform models list --region=global\n",
    "\n",
    "# List all versions of model\n",
    "\n",
    "print(\"\\nList of Versions in Global Endpoint)\")\n",
    "!gcloud ai-platform versions list --model $MODEL_NAME --region=global\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 456,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using endpoint [https://ml.googleapis.com/]\n",
      "Deleting model [test_seq_model_1]...done.                                      \n",
      "\n",
      "List of Models in Global Endpoint)\n",
      "Using endpoint [https://ml.googleapis.com/]\n",
      "NAME                    DEFAULT_VERSION_NAME\n",
      "model                   model_0\n",
      "model1                  model1_0\n",
      "model_tf                model_tf_0\n",
      "model_tf_kfp            train_model_tf_200519_May0515899031777fc1f30775031217\n",
      "sklearn_demo_model      sklearn_demo_model_0\n",
      "sklearn_demo_model_kfp  ver_bc06438cefc734b2904ca4f658fb9390\n",
      "sklearn_model_demo_v1   sklearn_model_demo_v1_1\n",
      "sklearn_taxi_demo_v1    sklearn_taxi_demo_v1_1\n",
      "smh_tf_model_demo_v1    smh_tf_model_demo_v1_4\n",
      "tf_model_demo_v1        tf_model_demo_v1_0\n",
      "tf_model_demo_v1_kfp    train_tf_model_demo_v1_200217_Feb0215819918658b7d7aa9adb2a351\n",
      "tf_taxi_demo_v1         tf_taxi_demo_v1_0\n",
      "xgb_model               xgb_model_0\n",
      "xgboost_taxi_demo_v1    xgboost_taxi_demo_v1_3\n",
      "\n",
      "List of Models in Regional Endpoint)\n",
      "Using endpoint [https://us-central1-ml.googleapis.com/]\n",
      "Listed 0 items.\n",
      "\n",
      "List of Versions in Global Endpoint)\n",
      "Using endpoint [https://ml.googleapis.com/]\n",
      "\u001b[1;31mERROR:\u001b[0m (gcloud.ai-platform.versions.list) NOT_FOUND: Field: parent Error: The model resource: \"test_seq_model_1\" was not found. Please create the model resource first by using 'gcloud ai-platform models create test_seq_model_1'.\n",
      "- '@type': type.googleapis.com/google.rpc.BadRequest\n",
      "  fieldViolations:\n",
      "  - description: \"The model resource: \\\"test_seq_model_1\\\" was not found. Please create\\\n",
      "      \\ the model resource first by using 'gcloud ai-platform models create test_seq_model_1'.\"\n",
      "    field: parent\n",
      "\n",
      "List of Versions in Regional Endpoint)\n",
      "Using endpoint [https://us-central1-ml.googleapis.com/]\n",
      "\u001b[1;31mERROR:\u001b[0m (gcloud.ai-platform.versions.list) NOT_FOUND: Field: parent Error: The model resource: \"test_seq_model_1\" was not found. Please create the model resource first by using 'gcloud ai-platform models create test_seq_model_1'.\n",
      "- '@type': type.googleapis.com/google.rpc.BadRequest\n",
      "  fieldViolations:\n",
      "  - description: \"The model resource: \\\"test_seq_model_1\\\" was not found. Please create\\\n",
      "      \\ the model resource first by using 'gcloud ai-platform models create test_seq_model_1'.\"\n",
      "    field: parent\n"
     ]
    }
   ],
   "source": [
    "#!gcloud ai-platform versions delete $MODEL_VERSION_ONLINE --model $MODEL_NAME --quiet --region=global\n",
    "#!gcloud ai-platform models delete $MODEL_NAME --quiet --region=global\n",
    "\n",
    "# List all models\n",
    "\n",
    "print(\"\\nList of Models in Global Endpoint)\")\n",
    "!gcloud ai-platform models list --region=global\n",
    "\n",
    "# List all versions of model\n",
    "\n",
    "print(\"\\nList of Versions in Global Endpoint)\")\n",
    "!gcloud ai-platform versions list --model $MODEL_NAME --region=global\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 457,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Using endpoint [https://us-central1-ml.googleapis.com/]', 'Listed 0 items.']\n",
      "Creating model test_seq_model_1\n",
      "Using endpoint [https://ml.googleapis.com/]\n",
      "Created ai platform model [projects/cchatterjee-sandbox/models/test_seq_model_1].\n",
      "\n",
      "List of Models in Global Endpoint)\n",
      "Using endpoint [https://ml.googleapis.com/]\n",
      "NAME                    DEFAULT_VERSION_NAME\n",
      "model                   model_0\n",
      "model1                  model1_0\n",
      "model_tf                model_tf_0\n",
      "model_tf_kfp            train_model_tf_200519_May0515899031777fc1f30775031217\n",
      "sklearn_demo_model      sklearn_demo_model_0\n",
      "sklearn_demo_model_kfp  ver_bc06438cefc734b2904ca4f658fb9390\n",
      "sklearn_model_demo_v1   sklearn_model_demo_v1_1\n",
      "sklearn_taxi_demo_v1    sklearn_taxi_demo_v1_1\n",
      "smh_tf_model_demo_v1    smh_tf_model_demo_v1_4\n",
      "test_seq_model_1\n",
      "tf_model_demo_v1        tf_model_demo_v1_0\n",
      "tf_model_demo_v1_kfp    train_tf_model_demo_v1_200217_Feb0215819918658b7d7aa9adb2a351\n",
      "tf_taxi_demo_v1         tf_taxi_demo_v1_0\n",
      "xgb_model               xgb_model_0\n",
      "xgboost_taxi_demo_v1    xgboost_taxi_demo_v1_3\n",
      "\n",
      "List of Models in Regional Endpoint)\n",
      "Using endpoint [https://us-central1-ml.googleapis.com/]\n",
      "Listed 0 items.\n"
     ]
    }
   ],
   "source": [
    "# create the model if it doesn't already exist\n",
    "modelname = !gcloud ai-platform models list | grep -w $MODEL_NAME\n",
    "print(modelname)\n",
    "if (len(modelname) <= 1) or ('Listed 0 items.' in modelname[1]):\n",
    "    print(\"Creating model \" + MODEL_NAME)\n",
    "    # Global endpoint\n",
    "    !gcloud ai-platform models create $MODEL_NAME --enable-logging --regions $REGION\n",
    "else:\n",
    "    print(\"Model \" + MODEL_NAME + \" exist\")\n",
    "    \n",
    "print(\"\\nList of Models in Global Endpoint)\")\n",
    "!gcloud ai-platform models list --region=global\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 465,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model Name = test_seq_model_1\n",
      "Model Versions = online_v1\n",
      "Best Model Dir:  gs://tuti_data/tensorflow_results/models/model_02072021_1100/checkpoints/cp-111637-6-0.9560/\n",
      "Using endpoint [https://ml.googleapis.com/]\n",
      "Creating version (this might take a few minutes)......done.                    \n",
      "CPU times: user 1.7 s, sys: 344 ms, total: 2.04 s\n",
      "Wall time: 54.4 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "print(\"Model Name =\", MODEL_NAME)\n",
    "print(\"Model Versions =\", MODEL_VERSION)\n",
    "\n",
    "# Get a list of model directories\n",
    "best_model_dir = best_model_dir_hyp\n",
    "print(\"Best Model Dir: \", best_model_dir)\n",
    "\n",
    "MODEL_FRAMEWORK = \"TENSORFLOW\"\n",
    "MODEL_DESCRIPTION = \"SEQ_MODEL_1\"\n",
    "MODEL_LABELS=\"team=ourteam,phase=test,owner=\"+USER\n",
    "\n",
    "MACHINE_TYPE = \"mls1-c1-m2\"\n",
    "if (MODEL_VERSION == MODEL_VERSION_BATCH):\n",
    "    MACHINE_TYPE = \"mls1-c1-m2\"\n",
    "    MODEL_LABELS = MODEL_LABELS+\",mode=batch\"\n",
    "if (MODEL_VERSION == MODEL_VERSION_ONLINE):\n",
    "    MACHINE_TYPE = \"mls1-c1-m2\" #\"n1-standard-32\"\n",
    "    MODEL_LABELS = MODEL_LABELS+\",mode=online\"\n",
    "\n",
    "# Deploy the model\n",
    "! gcloud beta ai-platform versions create $MODEL_VERSION \\\n",
    "  --model $MODEL_NAME \\\n",
    "  --origin $best_model_dir \\\n",
    "  --runtime-version $RUNTIME_VERSION \\\n",
    "  --python-version=3.7 \\\n",
    "  --description=$MODEL_DESCRIPTION \\\n",
    "  --labels $MODEL_LABELS \\\n",
    "  --machine-type=$MACHINE_TYPE  \\\n",
    "  --framework $MODEL_FRAMEWORK \\\n",
    "  --region global\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 481,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "List of Models in Global Endpoint)\n",
      "Using endpoint [https://ml.googleapis.com/]\n",
      "NAME                    DEFAULT_VERSION_NAME\n",
      "model                   model_0\n",
      "model1                  model1_0\n",
      "model_tf                model_tf_0\n",
      "model_tf_kfp            train_model_tf_200519_May0515899031777fc1f30775031217\n",
      "sklearn_demo_model      sklearn_demo_model_0\n",
      "sklearn_demo_model_kfp  ver_bc06438cefc734b2904ca4f658fb9390\n",
      "sklearn_model_demo_v1   sklearn_model_demo_v1_1\n",
      "sklearn_taxi_demo_v1    sklearn_taxi_demo_v1_1\n",
      "smh_tf_model_demo_v1    smh_tf_model_demo_v1_4\n",
      "test_seq_model_1        batch_v1\n",
      "tf_model_demo_v1        tf_model_demo_v1_0\n",
      "tf_model_demo_v1_kfp    train_tf_model_demo_v1_200217_Feb0215819918658b7d7aa9adb2a351\n",
      "tf_taxi_demo_v1         tf_taxi_demo_v1_0\n",
      "xgb_model               xgb_model_0\n",
      "xgboost_taxi_demo_v1    xgboost_taxi_demo_v1_3\n",
      "\n",
      "List of Models in Regional Endpoint)\n",
      "Using endpoint [https://us-central1-ml.googleapis.com/]\n",
      "Listed 0 items.\n",
      "\n",
      "List of Versions in Global Endpoint)\n",
      "Using endpoint [https://ml.googleapis.com/]\n",
      "NAME       DEPLOYMENT_URI                                                                                STATE\n",
      "batch_v1   gs://tuti_data/tensorflow_results/models/model_02072021_1100/checkpoints/cp-111637-6-0.9560/  READY\n",
      "online_v1  gs://tuti_data/tensorflow_results/models/model_02072021_1100/checkpoints/cp-111637-6-0.9560/  READY\n",
      "\n",
      "List of Versions in Regional Endpoint)\n",
      "Using endpoint [https://us-central1-ml.googleapis.com/]\n",
      "\u001b[1;31mERROR:\u001b[0m (gcloud.ai-platform.versions.list) NOT_FOUND: Field: parent Error: The model resource: \"test_seq_model_1\" was not found. Please create the model resource first by using 'gcloud ai-platform models create test_seq_model_1'.\n",
      "- '@type': type.googleapis.com/google.rpc.BadRequest\n",
      "  fieldViolations:\n",
      "  - description: \"The model resource: \\\"test_seq_model_1\\\" was not found. Please create\\\n",
      "      \\ the model resource first by using 'gcloud ai-platform models create test_seq_model_1'.\"\n",
      "    field: parent\n"
     ]
    }
   ],
   "source": [
    "# List all models\n",
    "\n",
    "print(\"\\nList of Models in Global Endpoint)\")\n",
    "!gcloud ai-platform models list --region=global\n",
    "\n",
    "print(\"\\nList of Models in Regional Endpoint)\")\n",
    "!gcloud ai-platform models list --region=$REGION\n",
    "\n",
    "# List all versions of model\n",
    "\n",
    "print(\"\\nList of Versions in Global Endpoint)\")\n",
    "!gcloud ai-platform versions list --model $MODEL_NAME --region=global\n",
    "\n",
    "print(\"\\nList of Versions in Regional Endpoint)\")\n",
    "!gcloud ai-platform versions list --model $MODEL_NAME --region=$REGION\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "------\n",
    "# Predictions with the deployed model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "from trainer import model\n",
    "\n",
    "# Copy the model from storage to local memory\n",
    "!gsutil -m cp -r $best_model_dir_hyp* ./model_dir\n",
    "\n",
    "# Load the model\n",
    "loaded_model = tf.keras.models.load_model('./model_dir', compile=False) #, \n",
    "               #custom_objects={\"custom_loss\": model.custom_loss,\"custom_mse\": model.custom_mse})\n",
    "print(\"Signature \", loaded_model.signatures)\n",
    "\n",
    "# Check the model layers\n",
    "model_layers = [layer.name for layer in loaded_model.layers]\n",
    "print(\"\")\n",
    "print(\"Model Input  Layer=\", model_layers[0])\n",
    "print(\"Model Output Layer=\", model_layers[-1])\n",
    "print(\"\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from trainer import inputs\n",
    "input_file = 'gs://' + BUCKET_NAME + '/' + FOLDER_DATA + '/' + INPUT_FILE_NAME\n",
    "train_test_data = inputs.load_data(input_file)\n",
    "X_test  = train_test_data[1]\n",
    "y_test  = train_test_data[3]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Online Prediction with python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 468,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Using endpoint [https://ml.googleapis.com/]', 'DENSE_4', '[0.9990240335464478, 0.0007911111460998654, 0.0001398640451952815, 4.499879651120864e-05]', '[0.5240583419799805, 0.12177881598472595, 0.16440680623054504, 0.18975602090358734]', '[0.5109434127807617, 0.12250146269798279, 0.16919846832752228, 0.19735673069953918]', '[0.999991774559021, 7.856284355511889e-06, 3.54128559365563e-07, 4.410637899354697e-08]', '[0.9997126460075378, 0.00024830870097503066, 3.113750790362246e-05, 7.892016583355144e-06]', '[0.58899986743927, 0.11681321263313293, 0.14048288762569427, 0.15370409190654755]', '[0.7020284533500671, 0.10241729021072388, 0.09844402968883514, 0.09711024165153503]', '[0.9980670809745789, 0.0014961755368858576, 0.00031956605380401015, 0.00011723955685738474]']\n",
      "CPU times: user 12 ms, sys: 36 ms, total: 48 ms\n",
      "Wall time: 2.6 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "# Online Prediction with GCLOUD - works for global & regional end points\n",
    "\n",
    "# Use MODEL_VERSION_ONLINE not MODEL_VERSION_BATCH\n",
    "MODEL_VERSION = MODEL_VERSION_ONLINE\n",
    "SIGNATURE_NAME=\"serving_default\"\n",
    "JSON_FILE = 'json.txt'\n",
    "json_data = {'instances': np.array(X_test)[0:8].tolist()}\n",
    "with open(JSON_FILE, 'w') as outfile:\n",
    "    json.dump(json_data, outfile)\n",
    "\n",
    "pprobas_temp1 = \\\n",
    "!gcloud ai-platform predict --model=$MODEL_NAME \\\n",
    "                            --json-request=$JSON_FILE \\\n",
    "                            --signature-name=$SIGNATURE_NAME \\\n",
    "                            --version=$MODEL_VERSION_ONLINE \\\n",
    "                            --region=global\n",
    "\n",
    "print(pprobas_temp1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 472,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1., 1., 1., 1.],\n",
       "       [0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0.]])"
      ]
     },
     "execution_count": 472,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Show the prediction results as an array\n",
    "nPreds = len(pprobas_temp1) - 2\n",
    "pprobas = np.zeros((nPreds,y_test.shape[1]))\n",
    "for i in range(nPreds):\n",
    "    pprobas[i,:] = np.array(eval(pprobas_temp1[i+2]))\n",
    "pprobas = np.round(pprobas, 2)\n",
    "\n",
    "pprobas\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 473,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Project ID = cchatterjee-sandbox\n",
      "Model Name = test_seq_model_1\n",
      "Model Version = online_v1\n",
      "model_name= projects/cchatterjee-sandbox/models/test_seq_model_1/versions/online_v1\n",
      "batch_size= 32\n",
      "n_samples= 1000\n",
      "Processing samples 0 32\n",
      "Processing samples 32 64\n",
      "Processing samples 64 96\n",
      "Processing samples 96 128\n",
      "Processing samples 128 160\n",
      "Processing samples 160 192\n",
      "Processing samples 192 224\n",
      "Processing samples 224 256\n",
      "Processing samples 256 288\n",
      "Processing samples 288 320\n",
      "Processing samples 320 352\n",
      "Processing samples 352 384\n",
      "Processing samples 384 416\n",
      "Processing samples 416 448\n",
      "Processing samples 448 480\n",
      "Processing samples 480 512\n",
      "Processing samples 512 544\n",
      "Processing samples 544 576\n",
      "Processing samples 576 608\n",
      "Processing samples 608 640\n",
      "Processing samples 640 672\n",
      "Processing samples 672 704\n",
      "Processing samples 704 736\n",
      "Processing samples 736 768\n",
      "Processing samples 768 800\n",
      "Processing samples 800 832\n",
      "Processing samples 832 864\n",
      "Processing samples 864 896\n",
      "Processing samples 896 928\n",
      "Processing samples 928 960\n",
      "Processing samples 960 992\n",
      "Processing samples 992 1000\n",
      "CPU times: user 420 ms, sys: 496 ms, total: 916 ms\n",
      "Wall time: 6.15 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "# Online Prediction with Python - works for global end points only\n",
    "\n",
    "# Use MODEL_VERSION_ONLINE not MODEL_VERSION_BATCH\n",
    "MODEL_VERSION = MODEL_VERSION_ONLINE\n",
    "\n",
    "from oauth2client.client import GoogleCredentials\n",
    "from googleapiclient import discovery\n",
    "from googleapiclient import errors\n",
    "import json\n",
    "\n",
    "#tf.compat.v1.logging.set_verbosity(tf.compat.v1.logging.ERROR)\n",
    "#tf.get_logger().setLevel('ERROR')\n",
    "\n",
    "print(\"Project ID =\", PROJECT_ID)\n",
    "print(\"Model Name =\", MODEL_NAME)\n",
    "print(\"Model Version =\", MODEL_VERSION)\n",
    "\n",
    "model_name = 'projects/{}/models/{}'.format(PROJECT_ID, MODEL_NAME)\n",
    "if MODEL_VERSION is not None:\n",
    "    model_name += '/versions/{}'.format(MODEL_VERSION)\n",
    "credentials = GoogleCredentials.get_application_default()\n",
    "service = discovery.build('ml', 'v1', cache_discovery=False, credentials=credentials)\n",
    "print(\"model_name=\", model_name)\n",
    "\n",
    "pprobas_temp = []\n",
    "batch_size = 32\n",
    "n_samples = min(1000,X_test.shape[0])\n",
    "print(\"batch_size=\", batch_size)\n",
    "print(\"n_samples=\", n_samples)\n",
    "\n",
    "for i in range(0, n_samples, batch_size):\n",
    "    j = min(i+batch_size, n_samples)\n",
    "    print(\"Processing samples\", i, j)\n",
    "    request = service.projects().predict(name=model_name, \\\n",
    "                                         body={'instances': np.array(X_test)[i:j].tolist()})\n",
    "    try:\n",
    "        response = request.execute()\n",
    "        pprobas_temp += response['predictions']\n",
    "    except errors.HttpError as err:\n",
    "        # Something went wrong, print out some information.\n",
    "        tf.compat.v1.logging.error('There was an error getting the job info, Check the details:')\n",
    "        tf.compat.v1.logging.error(err._get_reason())\n",
    "        break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 474,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1.  , 0.  , 0.  , 0.  ],\n",
       "       [0.52, 0.12, 0.16, 0.19],\n",
       "       [0.51, 0.12, 0.17, 0.2 ],\n",
       "       ...,\n",
       "       [1.  , 0.  , 0.  , 0.  ],\n",
       "       [0.99, 0.01, 0.  , 0.  ],\n",
       "       [1.  , 0.  , 0.  , 0.  ]])"
      ]
     },
     "execution_count": 474,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Show the prediction results as an array\n",
    "\n",
    "nPreds = len(pprobas_temp)\n",
    "pprobas = np.zeros((nPreds,y_test.shape[1]))\n",
    "for i in range(nPreds):\n",
    "    pprobas[i,:] = np.array(pprobas_temp[i][model_layers[-1]])\n",
    "pprobas = np.round(pprobas, 2)\n",
    "pprobas\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Batch Prediction with GCLOUD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write batch data to file in GCS\n",
    "\n",
    "import shutil\n",
    "\n",
    "# Clean current directory\n",
    "DATA_DIR = './batch_data'\n",
    "shutil.rmtree(DATA_DIR, ignore_errors=True)\n",
    "os.makedirs(DATA_DIR)\n",
    "\n",
    "n_samples = min(1000,X_test.shape[0])\n",
    "nFiles = 10\n",
    "nRecsPerFile = min(1000,n_samples//nFiles)\n",
    "print(\"n_samples =\", n_samples)\n",
    "print(\"nFiles =\", nFiles)\n",
    "print(\"nRecsPerFile =\", nRecsPerFile)\n",
    "\n",
    "# Create nFiles files with nImagesPerFile images each\n",
    "for i in range(nFiles):\n",
    "    with open(f'{DATA_DIR}/unkeyed_batch_{i}.json', \"w\") as file:\n",
    "        for z in range(nRecsPerFile):\n",
    "            print(f'{{\"dense_input\": {np.array(X_test)[i*nRecsPerFile+z].tolist()}}}', file=file)\n",
    "            #print(f'{{\"{model_layers[0]}\": {np.array(X_test)[i*nRecsPerFile+z].tolist()}}}', file=file)\n",
    "            #key = f'key_{i}_{z}'\n",
    "            #print(f'{{\"image\": {X_test_images[z].tolist()}, \"key\": \"{key}\"}}', file=file)\n",
    "\n",
    "# Write batch data to gcs file\n",
    "!gsutil -m cp -r ./batch_data gs://$BUCKET_NAME/$FOLDER_RESULTS/\n",
    "    \n",
    "# Remove old batch prediction results\n",
    "!gsutil -m rm -r gs://$BUCKET_NAME/$FOLDER_RESULTS/batch_predictions\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "from pytz import timezone\n",
    "\n",
    "DATA_FORMAT=\"text\" # JSON data format\n",
    "INPUT_PATHS='gs://' + BUCKET_NAME + '/' + FOLDER_RESULTS + '/batch_data/*'\n",
    "OUTPUT_PATH='gs://' + BUCKET_NAME + '/' + FOLDER_RESULTS + '/batch_predictions'\n",
    "PRED_LABELS=\"mode=batch,team=ourteam,phase=test,owner=\"+USER\n",
    "SIGNATURE_NAME=\"serving_default\"\n",
    "\n",
    "JOBNAME_BATCH = 'tf_batch_predict_'+ USER + '_' + \\\n",
    "           datetime.now(timezone('US/Pacific')).strftime(\"%m%d%y_%H%M\")\n",
    "\n",
    "print(\"INPUT_PATHS = \", INPUT_PATHS)\n",
    "print(\"OUTPUT_PATH = \", OUTPUT_PATH)\n",
    "print(\"Job Name    = \", JOBNAME_BATCH)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 477,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Job [tf_batch_predict_cchatterj_021721_0031] submitted successfully.\n",
      "Your job is still active. You may view the status of your job with the command\n",
      "\n",
      "  $ gcloud ai-platform jobs describe tf_batch_predict_cchatterj_021721_0031\n",
      "\n",
      "or continue streaming the logs with the command\n",
      "\n",
      "  $ gcloud ai-platform jobs stream-logs tf_batch_predict_cchatterj_021721_0031\n",
      "jobId: tf_batch_predict_cchatterj_021721_0031\n",
      "state: QUEUED\n"
     ]
    }
   ],
   "source": [
    "# Only works with global endpoint\n",
    "# Submit batch predict job\n",
    "# Use  MODEL_VERSION_BATCH not MODEL_VERSION_ONLINE\n",
    "MODEL_VERSION = MODEL_VERSION_BATCH\n",
    "\n",
    "!gcloud ai-platform jobs submit prediction $JOBNAME_BATCH \\\n",
    "    --model=$MODEL_NAME \\\n",
    "    --version=$MODEL_VERSION \\\n",
    "    --input-paths=$INPUT_PATHS \\\n",
    "    --output-path=$OUTPUT_PATH \\\n",
    "    --data-format=$DATA_FORMAT \\\n",
    "    --labels=$PRED_LABELS \\\n",
    "    --signature-name=$SIGNATURE_NAME \\\n",
    "    --region=$REGION\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 478,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "createTime: '2021-02-18T05:20:02Z'\n",
      "endTime: '2021-02-18T05:26:31Z'\n",
      "etag: mIaZleiyIQk=\n",
      "jobId: tf_batch_predict_cchatterj_021721_0031\n",
      "labels:\n",
      "  mode: batch\n",
      "  owner: cchatterj\n",
      "  phase: test\n",
      "  team: ourteam\n",
      "predictionInput:\n",
      "  dataFormat: JSON\n",
      "  inputPaths:\n",
      "  - gs://tuti_data/tensorflow_results/batch_data/*\n",
      "  outputPath: gs://tuti_data/tensorflow_results/batch_predictions\n",
      "  region: us-central1\n",
      "  runtimeVersion: '2.1'\n",
      "  signatureName: serving_default\n",
      "  versionName: projects/cchatterjee-sandbox/models/test_seq_model_1/versions/batch_v1\n",
      "predictionOutput:\n",
      "  nodeHours: 0.12\n",
      "  outputPath: gs://tuti_data/tensorflow_results/batch_predictions\n",
      "  predictionCount: '1000'\n",
      "startTime: '2021-02-18T05:20:03Z'\n",
      "state: SUCCEEDED\n",
      "\n",
      "View job in the Cloud Console at:\n",
      "https://console.cloud.google.com/mlengine/jobs/tf_batch_predict_cchatterj_021721_0031?project=cchatterjee-sandbox\n",
      "\n",
      "View logs at:\n",
      "https://console.cloud.google.com/logs?resource=ml_job%2Fjob_id%2Ftf_batch_predict_cchatterj_021721_0031&project=cchatterjee-sandbox\n"
     ]
    }
   ],
   "source": [
    "# check the batch prediction job status\n",
    "! gcloud ai-platform jobs describe $JOBNAME_BATCH\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print Errors\n",
    "response = ! gcloud logging read \"resource.labels.job_id=$JOBNAME_BATCH severity>=ERROR\"\n",
    "for i in range(0,len(response)):\n",
    "    if 'message' in response[i]:\n",
    "        print(response[i])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!gsutil cat $OUTPUT_PATH/prediction.results-00000-of-00010\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "environment": {
   "name": "tf2-2-3-gpu.2-3.m56",
   "type": "gcloud",
   "uri": "gcr.io/deeplearning-platform-release/tf2-2-3-gpu.2-3:m56"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
