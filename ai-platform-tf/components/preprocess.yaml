name: Data preprocess
inputs:
- {name: bucket_name, type: String}
- {name: input_file, type: String}
- {name: target_column, type: String}
outputs:
- {name: x_train_name, type: String}
- {name: x_test_name, type: String}
- {name: y_train_name, type: String}
- {name: y_test_name, type: String}
- {name: n_classes, type: Integer}
implementation:
  container:
    image: gcr.io/deeplearning-platform-release/tf2-gpu.2-1
    command:
    - sh
    - -ec
    - |
      program_path=$(mktemp)
      printf "%s" "$0" > "$program_path"
      python3 -u "$program_path" "$@"
    - "def data_preprocess(\n    bucket_name,\n    input_file,\n    target_column,\n\
      \    ):\n\n    from collections import namedtuple\n    from sklearn.model_selection\
      \ import train_test_split\n    import pandas as pd\n    import os\n    import\
      \ logging\n\n    logging.info(\"Preprocessing raw data:\")\n    logging.info(\"\
      \ => Drop id column:\")\n    logging.info(\" => One hot encoding categorical\
      \ features\")\n    logging.info(\" => Count number of classes\")\n    logging.info(\"\
      \ => Perform train/test split\")\n\n    logging.info(\"Reading raw data file:\
      \ {}\".format(input_file))\n    dataset = pd.read_csv(input_file)\n    # Drop\
      \ unique id column which is not useful for ML\n    logging.info(\"Drop unique\
      \ id column which is not an useful feature for ML: {}\".format('LOAN_SEQUENCE_NUMBER'))\n\
      \    dataset.drop(['LOAN_SEQUENCE_NUMBER'], axis=1, inplace=True)\n\n    # Convert\
      \ categorical columns into one-hot encodings\n    logging.info(\"Convert categorical\
      \ columns into one-hot encodings\")\n    [logging.info(\"categorical feature:\
      \ {}\".format(col)) for col in dataset.columns if dataset[col].dtype == 'object']\n\
      \    str_cols = [col for col in dataset.columns if dataset[col].dtype == 'object']\n\
      \    dataset = pd.get_dummies(dataset, columns=str_cols)\n\n    # Count number\
      \ of unique classes\n    logging.info(\"Count number of unique classes ...\"\
      )\n    n_classes = dataset[target_column].nunique()\n    logging.info(\"No.\
      \ of Classes: {}\".format(n_classes))\n\n    # Split with a small test size\
      \ so as to allow our model to train on more data\n    logging.info(\"Perform\
      \ train/test split ...\")\n    x_train, x_test, y_train, y_test = train_test_split(\n\
      \        dataset.drop(target_column, axis=1), \n        dataset[target_column],\
      \ \n        test_size=0.1,\n        random_state=1,\n        shuffle=True, \n\
      \        stratify=dataset[target_column], \n        )\n\n    # Fill Nan value\
      \ with zeros\n    x_train = x_train.fillna(0)\n    x_test = x_test.fillna(0)\n\
      \n    logging.info(\"Get feature/label shapes ...\")\n    logging.info(\"x_train\
      \ shape = {}\".format(x_train.shape))\n    logging.info(\"x_test shape = {}\"\
      .format(x_test.shape))\n    logging.info(\"y_train shape = {}\".format(y_train.shape))\n\
      \    logging.info(\"y_test shape = {}\".format(y_test.shape))\n\n    # Build\
      \ data split file name from input file name\n    base_file_name = os.path.basename(input_file)\n\
      \    base_name, ext_name = os.path.splitext(base_file_name)\n    x_train_name\
      \ = \"{}_x_train{}\".format(base_name, ext_name)\n    x_test_name = \"{}_x_test{}\"\
      .format(base_name, ext_name)\n    y_train_name = \"{}_y_train{}\".format(base_name,\
      \ ext_name)\n    y_test_name = \"{}_y_test{}\".format(base_name, ext_name)\n\
      \n    x_train_name = os.path.join(\"gs://\", bucket_name, \"data_split\", x_train_name)\n\
      \    x_test_name = os.path.join(\"gs://\", bucket_name, \"data_split\", x_test_name)\n\
      \    y_train_name = os.path.join(\"gs://\", bucket_name, \"data_split\", y_train_name)\n\
      \    y_test_name = os.path.join(\"gs://\", bucket_name, \"data_split\", y_test_name)\n\
      \n    # Save split data to gcs\n    x_train.to_csv(x_train_name, index=False)\n\
      \    x_test.to_csv(x_test_name, index=False)\n\n    # The preprocessing for\
      \ label column is different \n    # between tensorflow and XGBoost models\n\
      \    pd.get_dummies(y_train).to_csv(y_train_name, index=False, header=None)\n\
      \    pd.get_dummies(y_test).to_csv(y_test_name, index=False, header=None)\n\n\
      \    # Saving data\n    logging.info(\"Saving data ...\")\n    logging.info(\"\
      x_train saved to {}\".format(x_train_name))\n    logging.info(\"y_train saved\
      \ to {}\".format(y_train_name))\n    logging.info(\"x_test saved to {}\".format(x_test_name))\n\
      \    logging.info(\"y_test saved to {}\".format(y_test_name))\n    logging.info(\"\
      finished\")\n\n    PreprocessOutput = namedtuple('PreprocessOutput', \n    \
      \    ['x_train_name', 'x_test_name', 'y_train_name', 'y_test_name', 'n_classes'])\n\
      \    return PreprocessOutput(\n        x_train_name=x_train_name,\n        x_test_name=x_test_name,\n\
      \        y_train_name=y_train_name,\n        y_test_name=y_test_name,\n    \
      \    n_classes=n_classes,\n    )\n\ndef _serialize_str(str_value: str) -> str:\n\
      \    if not isinstance(str_value, str):\n        raise TypeError('Value \"{}\"\
      \ has type \"{}\" instead of str.'.format(str(str_value), str(type(str_value))))\n\
      \    return str_value\n\ndef _serialize_int(int_value: int) -> str:\n    if\
      \ isinstance(int_value, str):\n        return int_value\n    if not isinstance(int_value,\
      \ int):\n        raise TypeError('Value \"{}\" has type \"{}\" instead of int.'.format(str(int_value),\
      \ str(type(int_value))))\n    return str(int_value)\n\nimport argparse\n_parser\
      \ = argparse.ArgumentParser(prog='Data preprocess', description='')\n_parser.add_argument(\"\
      --bucket-name\", dest=\"bucket_name\", type=str, required=True, default=argparse.SUPPRESS)\n\
      _parser.add_argument(\"--input-file\", dest=\"input_file\", type=str, required=True,\
      \ default=argparse.SUPPRESS)\n_parser.add_argument(\"--target-column\", dest=\"\
      target_column\", type=str, required=True, default=argparse.SUPPRESS)\n_parser.add_argument(\"\
      ----output-paths\", dest=\"_output_paths\", type=str, nargs=5)\n_parsed_args\
      \ = vars(_parser.parse_args())\n_output_files = _parsed_args.pop(\"_output_paths\"\
      , [])\n\n_outputs = data_preprocess(**_parsed_args)\n\n_output_serializers =\
      \ [\n    _serialize_str,\n    _serialize_str,\n    _serialize_str,\n    _serialize_str,\n\
      \    _serialize_int,\n\n]\n\nimport os\nfor idx, output_file in enumerate(_output_files):\n\
      \    try:\n        os.makedirs(os.path.dirname(output_file))\n    except OSError:\n\
      \        pass\n    with open(output_file, 'w') as f:\n        f.write(_output_serializers[idx](_outputs[idx]))\n"
    args:
    - --bucket-name
    - {inputValue: bucket_name}
    - --input-file
    - {inputValue: input_file}
    - --target-column
    - {inputValue: target_column}
    - '----output-paths'
    - {outputPath: x_train_name}
    - {outputPath: x_test_name}
    - {outputPath: y_train_name}
    - {outputPath: y_test_name}
    - {outputPath: n_classes}
