{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ============================================================================== \\\n",
    " Copyright 2020 Google LLC. This software is provided as-is, without warranty \\\n",
    " or representation for any use or purpose. Your use of it is subject to your \\\n",
    " agreement with Google. \\\n",
    " ============================================================================== \n",
    " \n",
    " Author: Elvin Zhu, Chanchal Chatterjee \\\n",
    " Email: elvinzhu@google.com \\\n",
    "<img src=\"img/google-cloud-icon.jpg\" alt=\"Drawing\" style=\"width: 200px;\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Install pakcages requried for training, deployment and prediction with ai platform.\n",
    "\n",
    "https://cloud.google.com/ai-platform/training/docs/runtime-version-list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "cd /home/jupyter/tuti-repo/ai-platform-xgboost\n",
    "python3 -m pip install -r ./requirements.txt --user"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create training application package\n",
    "\n",
    "The easiest (and recommended) way to create a training application package uses gcloud to package and upload the application when you submit your training job. This method allows you to create a very simple file structure. For this tutorial, the file structure of your training application package should appear similar to the following:\n",
    "\n",
    "```\n",
    "config/\n",
    "    config.yaml\n",
    "    config_hpt.yaml\n",
    "    \n",
    "trainer/ \n",
    "    __init__.py\n",
    "    train.py\n",
    "    train_hpt.py\n",
    "```\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile ./setup.py\n",
    "\n",
    "# python3\n",
    "# ==============================================================================\n",
    "# Copyright 2020 Google LLC. This software is provided as-is, without warranty\n",
    "# or representation for any use or purpose. Your use of it is subject to your\n",
    "# agreement with Google.\n",
    "# ==============================================================================\n",
    "\n",
    "from setuptools import find_packages\n",
    "from setuptools import setup\n",
    "\n",
    "REQUIRED_PACKAGES = [\n",
    "    'tensorflow==2.1.0',\n",
    "    'numpy==1.18.0',\n",
    "    'pandas==1.2.1',\n",
    "    'scipy==1.4.1',\n",
    "    'scikit-learn==0.22',\n",
    "    'google-cloud-storage==1.23.0',\n",
    "    'xgboost==1.3.3',\n",
    "    'cloudml-hypertune',\n",
    "    ]\n",
    " \n",
    "setup(\n",
    "    name='trainer',\n",
    "    version='0.1',\n",
    "    install_requires=REQUIRED_PACKAGES,\n",
    "    packages=find_packages(),\n",
    "    include_package_data=True,\n",
    "    description='Trainer package for XGBoost Task'\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing ./trainer/__init__.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile ./trainer/__init__.py\n",
    "\n",
    "# python3\n",
    "# ==============================================================================\n",
    "# Copyright 2020 Google LLC. This software is provided as-is, without warranty\n",
    "# or representation for any use or purpose. Your use of it is subject to your\n",
    "# agreement with Google.\n",
    "# ==============================================================================\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create your training code (Example showed here is to use XGBoost to classify structured mortgage data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing ./trainer/model.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile ./trainer/model.py\n",
    "\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting ./trainer/train.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile ./trainer/train.py\n",
    "\n",
    "# python3\n",
    "# ==============================================================================\n",
    "# Copyright 2020 Google LLC. This software is provided as-is, without warranty\n",
    "# or representation for any use or purpose. Your use of it is subject to your\n",
    "# agreement with Google.\n",
    "# ==============================================================================\n",
    "\n",
    "import argparse\n",
    "import json\n",
    "import hypertune\n",
    "import os\n",
    "import warnings\n",
    "\n",
    "import datetime as datetime\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "\n",
    "from pytz import timezone\n",
    "\n",
    "# from .trainer import model\n",
    "# from .trainer import inputs\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "#0 = all messages are logged (default behavior)\n",
    "#1 = INFO messages are not printed\n",
    "#2 = INFO and WARNING messages are not printed\n",
    "#3 = INFO, WARNING, and ERROR messages are not printed\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3' \n",
    "\n",
    "def parse_arguments():\n",
    "    \"\"\"Argument parser.\n",
    "    Returns:\n",
    "      Dictionary of arguments.\n",
    "    \"\"\"\n",
    "\n",
    "    parser = argparse.ArgumentParser()\n",
    "    parser.add_argument('--depth', default=3, type=int, \n",
    "                        help='Hyperparameter: depth of network')\n",
    "    parser.add_argument('--dropout_rate', default=0.02, type=float, \n",
    "                        help='Hyperparameter: Drop out rate')\n",
    "    parser.add_argument('--learning_rate', default=0.0001, type=float, \n",
    "                        help='Hyperparameter: initial learning rate')\n",
    "    parser.add_argument('--batch_size', default=4, type=int, \n",
    "                        help='Hyperparameter: batch size of the deep network')\n",
    "    parser.add_argument('--epochs', default=1, type=int, \n",
    "                        help='number of epochs.')\n",
    "    parser.add_argument('--job-dir', default=\"\",\n",
    "                        help='Directory to store model checkpoints and logs.')\n",
    "    parser.add_argument('--train_feature_name', default=\"\",\n",
    "                        help='GCS path to train feature csv.')\n",
    "    parser.add_argument('--test_feature_name', default=\"\",\n",
    "                        help='GCS path to test feature csv.')\n",
    "    parser.add_argument('--train_label_name', default=\"\",\n",
    "                        help='GCS path to train label csv.')\n",
    "    parser.add_argument('--test_label_name', default=\"\",\n",
    "                        help='GCS path to test label csv.')\n",
    "    parser.add_argument('--verbosity', choices=['DEBUG','ERROR','FATAL','INFO','WARN'],\n",
    "                        default='FATAL')\n",
    "    args, _ = parser.parse_known_args()\n",
    "    return args\n",
    "\n",
    "def tf_model(input_dim, output_dim, depth, dropout_rate):\n",
    "    from tensorflow.keras.models import Sequential\n",
    "    from tensorflow.keras.layers import Dense, Dropout\n",
    "\n",
    "    decr = int((input_dim-output_dim-16)/depth) ^ 1\n",
    "\n",
    "    model = Sequential()\n",
    "    model.add(Dense(128, input_dim=input_dim, activation=tf.nn.relu))\n",
    "    for i in range(1,depth):\n",
    "        model.add(Dense(input_dim-i*decr, activation=tf.nn.relu, kernel_regularizer='l2'))\n",
    "    model.add(Dropout(dropout_rate))\n",
    "    model.add(Dense(output_dim, activation=tf.nn.softmax))\n",
    "    print(model.summary())\n",
    "\n",
    "    return model\n",
    "\n",
    "# def custom_loss(y_true, y_pred):\n",
    "#     custom_loss = mean(square(y_true - y_pred), axis=-1)\n",
    "#     return custom_loss\n",
    "\n",
    "# def custom_metric(y_true, y_pred):\n",
    "#     custom_metric = mean(square(y_true - y_pred), axis=-1)\n",
    "#     return custom_metric\n",
    "\n",
    "def get_callbacks(args, early_stop_patience: int = 3):\n",
    "    \"\"\"Creates Keras callbacks for model training.\"\"\"\n",
    "\n",
    "    # Get trialId\n",
    "    trialId = json.loads(os.environ.get(\"TF_CONFIG\", \"{}\")).get(\"task\", {}).get(\"trial\", \"\")\n",
    "    if trialId == '':\n",
    "        trialId = '0'\n",
    "    print(\"trialId=\", trialId)\n",
    "\n",
    "    curTime = datetime.datetime.now(timezone('US/Pacific')).strftime('%H%M%S')\n",
    "    \n",
    "    # Modify model_dir paths to include trialId\n",
    "    model_dir = args.job_dir + \"/checkpoints/cp-\"+curTime+\"-\"+trialId+\"-{val_accuracy:.4f}\"\n",
    "    log_dir   = args.job_dir + \"/log_dir\"\n",
    "\n",
    "    tensorboard_cb = tf.keras.callbacks.TensorBoard(log_dir, histogram_freq=1)\n",
    "    checkpoint_cb  = tf.keras.callbacks.ModelCheckpoint(model_dir, monitor='val_accuracy', mode='max', \n",
    "                                                        verbose=0, save_best_only=True,\n",
    "                                                        save_weights_only=False)\n",
    "    earlystop_cb   = tf.keras.callbacks.EarlyStopping(monitor='val_accuracy', patience=3)\n",
    "\n",
    "    return [checkpoint_cb, tensorboard_cb, earlystop_cb]\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "\n",
    "    args = parse_arguments()\n",
    "    print(args)\n",
    "    print(\"Input and pre-process data ...\")   \n",
    "    x_train = pd.read_csv(args.train_feature_name)\n",
    "    y_train = pd.read_csv(args.train_label_name, header=None)\n",
    "    x_test = pd.read_csv(args.test_feature_name)\n",
    "    y_test = pd.read_csv(args.test_label_name, header=None)\n",
    "\n",
    "    print(\"Shapes:\")\n",
    "    print(x_train.shape)\n",
    "    print(y_train.shape)\n",
    "    # Train model\n",
    "    print(\"Creating model ...\")\n",
    "    model = tf_model(x_train.shape[1], y_train.shape[1], \n",
    "                              depth=args.depth,\n",
    "                              dropout_rate=args.dropout_rate)\n",
    "    \n",
    "    model.compile(optimizer=tf.keras.optimizers.Adam(lr=args.learning_rate),\n",
    "                     loss='mean_squared_error',\n",
    "                     metrics=['accuracy'])\n",
    "    \n",
    "    print(\"Fitting model ...\")\n",
    "    callbacks = get_callbacks(args, 3)\n",
    "    hist = model.fit(np.array(x_train), np.array(y_train), \n",
    "                         epochs=args.epochs,\n",
    "                         batch_size=args.batch_size,\n",
    "                         validation_data=(np.array(x_test),y_test),\n",
    "                         callbacks=callbacks)\n",
    "\n",
    "    # TBD save history for visualization\n",
    "    final_epoch_accuracy = hist.history['accuracy'][-1]\n",
    "    final_epoch_count = len(hist.history['accuracy'])\n",
    "\n",
    "    print('final_epoch_accuracy = %.6f' % final_epoch_accuracy)\n",
    "    print('final_epoch_count = %2d' % final_epoch_count)\n",
    "\n",
    "    model.save(args.job_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create another version of training script which implement metric reporting summary for hyperparameter tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting ./trainer/train_hpt.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile ./trainer/train_hpt.py\n",
    "\n",
    "# python3\n",
    "# ==============================================================================\n",
    "# Copyright 2020 Google LLC. This software is provided as-is, without warranty\n",
    "# or representation for any use or purpose. Your use of it is subject to your\n",
    "# agreement with Google.\n",
    "# ==============================================================================\n",
    "\n",
    "import argparse\n",
    "import json\n",
    "import hypertune\n",
    "import os\n",
    "import warnings\n",
    "\n",
    "import datetime as datetime\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "\n",
    "from pytz import timezone\n",
    "\n",
    "# from .trainer import model\n",
    "# from .trainer import inputs\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "#0 = all messages are logged (default behavior)\n",
    "#1 = INFO messages are not printed\n",
    "#2 = INFO and WARNING messages are not printed\n",
    "#3 = INFO, WARNING, and ERROR messages are not printed\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3' \n",
    "\n",
    "def parse_arguments():\n",
    "    \"\"\"Argument parser.\n",
    "    Returns:\n",
    "      Dictionary of arguments.\n",
    "    \"\"\"\n",
    "\n",
    "    parser = argparse.ArgumentParser()\n",
    "    parser.add_argument('--depth', default=3, type=int, \n",
    "                        help='Hyperparameter: depth of network')\n",
    "    parser.add_argument('--dropout_rate', default=0.02, type=float, \n",
    "                        help='Hyperparameter: Drop out rate')\n",
    "    parser.add_argument('--learning_rate', default=0.0001, type=float, \n",
    "                        help='Hyperparameter: initial learning rate')\n",
    "    parser.add_argument('--batch_size', default=4, type=int, \n",
    "                        help='Hyperparameter: batch size of the deep network')\n",
    "    parser.add_argument('--epochs', default=1, type=int, \n",
    "                        help='number of epochs.')\n",
    "    parser.add_argument('--job-dir', default=\"\",\n",
    "                        help='Directory to store model checkpoints and logs.')\n",
    "    parser.add_argument('--train_feature_name', default=\"\",\n",
    "                        help='GCS path to train feature csv.')\n",
    "    parser.add_argument('--test_feature_name', default=\"\",\n",
    "                        help='GCS path to test feature csv.')\n",
    "    parser.add_argument('--train_label_name', default=\"\",\n",
    "                        help='GCS path to train label csv.')\n",
    "    parser.add_argument('--test_label_name', default=\"\",\n",
    "                        help='GCS path to test label csv.')\n",
    "    parser.add_argument('--verbosity', choices=['DEBUG','ERROR','FATAL','INFO','WARN'],\n",
    "                        default='FATAL')\n",
    "    args, _ = parser.parse_known_args()\n",
    "    return args\n",
    "\n",
    "def tf_model(input_dim, output_dim, depth, dropout_rate):\n",
    "    from tensorflow.keras.models import Sequential\n",
    "    from tensorflow.keras.layers import Dense, Dropout\n",
    "\n",
    "    decr = int((input_dim-output_dim-16)/depth) ^ 1\n",
    "\n",
    "    model = Sequential()\n",
    "    model.add(Dense(128, input_dim=input_dim, activation=tf.nn.relu))\n",
    "    for i in range(1,depth):\n",
    "        model.add(Dense(input_dim-i*decr, activation=tf.nn.relu, kernel_regularizer='l2'))\n",
    "    model.add(Dropout(dropout_rate))\n",
    "    model.add(Dense(output_dim, activation=tf.nn.softmax))\n",
    "    print(model.summary())\n",
    "\n",
    "    return model\n",
    "\n",
    "# def custom_loss(y_true, y_pred):\n",
    "#     custom_loss = mean(square(y_true - y_pred), axis=-1)\n",
    "#     return custom_loss\n",
    "\n",
    "# def custom_metric(y_true, y_pred):\n",
    "#     custom_metric = mean(square(y_true - y_pred), axis=-1)\n",
    "#     return custom_metric\n",
    "\n",
    "def get_callbacks(args, early_stop_patience: int = 3):\n",
    "    \"\"\"Creates Keras callbacks for model training.\"\"\"\n",
    "\n",
    "    # Get trialId\n",
    "    trialId = json.loads(os.environ.get(\"TF_CONFIG\", \"{}\")).get(\"task\", {}).get(\"trial\", \"\")\n",
    "    if trialId == '':\n",
    "        trialId = '0'\n",
    "    print(\"trialId=\", trialId)\n",
    "\n",
    "    curTime = datetime.datetime.now(timezone('US/Pacific')).strftime('%H%M%S')\n",
    "    \n",
    "    # Modify model_dir paths to include trialId\n",
    "    model_dir = args.job_dir + \"/checkpoints/cp-\"+curTime+\"-\"+trialId+\"-{val_accuracy:.4f}\"\n",
    "    log_dir   = args.job_dir + \"/log_dir\"\n",
    "\n",
    "    tensorboard_cb = tf.keras.callbacks.TensorBoard(log_dir, histogram_freq=1)\n",
    "    checkpoint_cb  = tf.keras.callbacks.ModelCheckpoint(model_dir, monitor='val_accuracy', mode='max', \n",
    "                                                        verbose=0, save_best_only=True,\n",
    "                                                        save_weights_only=False)\n",
    "    earlystop_cb   = tf.keras.callbacks.EarlyStopping(monitor='val_accuracy', patience=3)\n",
    "\n",
    "    return [checkpoint_cb, tensorboard_cb, earlystop_cb]\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "\n",
    "    args = parse_arguments()\n",
    "    print(args)\n",
    "    print(\"Input and pre-process data ...\")   \n",
    "    x_train = pd.read_csv(args.train_feature_name)\n",
    "    y_train = pd.read_csv(args.train_label_name, header=None)\n",
    "    x_test = pd.read_csv(args.test_feature_name)\n",
    "    y_test = pd.read_csv(args.test_label_name, header=None)\n",
    "\n",
    "    print(\"Shapes:\")\n",
    "    print(x_train.shape)\n",
    "    print(y_train.shape)\n",
    "    # Train model\n",
    "    print(\"Creating model ...\")\n",
    "    model = tf_model(x_train.shape[1], y_train.shape[1], \n",
    "                              depth=args.depth,\n",
    "                              dropout_rate=args.dropout_rate)\n",
    "    \n",
    "    model.compile(optimizer=tf.keras.optimizers.Adam(lr=args.learning_rate),\n",
    "                     loss='mean_squared_error',\n",
    "                     metrics=['accuracy'])\n",
    "    \n",
    "    print(\"Fitting model ...\")\n",
    "    callbacks = get_callbacks(args, 3)\n",
    "    hist = model.fit(np.array(x_train), np.array(y_train), \n",
    "                         epochs=args.epochs,\n",
    "                         batch_size=args.batch_size,\n",
    "                         validation_data=(np.array(x_test), y_test),\n",
    "                         callbacks=callbacks)\n",
    "\n",
    "    # TBD save history for visualization\n",
    "    final_epoch_accuracy = hist.history['accuracy'][-1]\n",
    "    final_epoch_count = len(hist.history['accuracy'])\n",
    "\n",
    "    print('final_epoch_accuracy = %.6f' % final_epoch_accuracy)\n",
    "    print('final_epoch_count = %2d' % final_epoch_count)\n",
    "    \n",
    "    # The default name of the metric is training/hptuning/metric. \n",
    "    # We recommend that you assign a custom name. The only functional difference is that \n",
    "    # if you use a custom name, you must set the hyperparameterMetricTag value in the \n",
    "    # HyperparameterSpec object in your job request to match your chosen name.\n",
    "    # https://cloud.google.com/ml-engine/reference/rest/v1/projects.jobs#HyperparameterSpec\n",
    "    hpt = hypertune.HyperTune()\n",
    "    hpt.report_hyperparameter_tuning_metric(\n",
    "        metric_value=final_epoch_accuracy,\n",
    "        hyperparameter_metric_tag='accuracy',\n",
    "        global_step=final_epoch_count\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Configure for AI Platform Training\n",
    "Create config file for Cloud AI Platform training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing ./config/config.yaml\n"
     ]
    }
   ],
   "source": [
    "%%writefile ./config/config.yaml\n",
    "# python3\n",
    "# ==============================================================================\n",
    "# Copyright 2020 Google LLC. This software is provided as-is, without warranty\n",
    "# or representation for any use or purpose. Your use of it is subject to your\n",
    "# agreement with Google.\n",
    "# ==============================================================================\n",
    "\n",
    "#trainingInput:\n",
    "#  scaleTier: CUSTOM\n",
    "#  masterType: n1-highmem-8\n",
    "#  masterConfig:\n",
    "#    acceleratorConfig:\n",
    "#      count: 1\n",
    "#      type: NVIDIA_TESLA_T4\n",
    "\n",
    "trainingInput:\n",
    "  scaleTier: STANDARD-1\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Configure for Hyperparameter Tuning\n",
    "Similarly create config file for Cloud AI Platform Hyperparameter tuning. Moreover, the hyperparameter search space is needed to be configured.\n",
    "\n",
    "The supported hyperparameter types are listed in the job reference documentation. In the ParameterSpec object, you specify the type for each hyperparameter and the related value ranges as described in the following table:\n",
    "\n",
    "|Type        | Value ranges        |Value data            |\n",
    "|------------|---------------------|----------------------|\n",
    "|DOUBLE      |minValue & maxValue  | Floating-point values|\n",
    "|INTEGER     |minValue & maxValue  |Integer values        |\n",
    "|CATEGORICAL |categoricalValues    |List of category strings|\n",
    "|DISCRETE    |discreteValues       |List of values in ascending order|\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing ./config/config_hpt.yaml\n"
     ]
    }
   ],
   "source": [
    "%%writefile ./config/config_hpt.yaml\n",
    "\n",
    "# python3\n",
    "# ==============================================================================\n",
    "# Copyright 2020 Google LLC. This software is provided as-is, without warranty\n",
    "# or representation for any use or purpose. Your use of it is subject to your\n",
    "# agreement with Google.\n",
    "# ==============================================================================\n",
    "\n",
    "\n",
    "# hptuning_config.yaml\n",
    "trainingInput:\n",
    "  scaleTier: STANDARD-1\n",
    "  hyperparameters:\n",
    "    goal: MAXIMIZE\n",
    "    hyperparameterMetricTag: accuracy\n",
    "    maxTrials: 5\n",
    "    maxParallelTrials: 2\n",
    "    enableTrialEarlyStopping: True\n",
    "    params:\n",
    "    - parameterName: depth\n",
    "      type: INTEGER\n",
    "      minValue: 2\n",
    "      maxValue: 10\n",
    "      scaleType: UNIT_LINEAR_SCALE\n",
    "    - parameterName: dropout_rate\n",
    "      type: DOUBLE\n",
    "      minValue: 0.001\n",
    "      maxValue: 0.01\n",
    "      scaleType: UNIT_LOG_SCALE\n",
    "    - parameterName: learning_rate\n",
    "      type: DOUBLE\n",
    "      minValue: 0.00001\n",
    "      maxValue: 0.01\n",
    "      scaleType: UNIT_LOG_SCALE\n",
    "    - parameterName: batch_size\n",
    "      type: INTEGER\n",
    "      minValue: 1\n",
    "      maxValue: 10\n",
    "      scaleType: UNIT_LINEAR_SCALE\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "environment": {
   "name": "tf2-gpu.2-1.m61",
   "type": "gcloud",
   "uri": "gcr.io/deeplearning-platform-release/tf2-gpu.2-1:m61"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
