{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "ur8xi4C7S06n"
   },
   "outputs": [],
   "source": [
    "# @title Copyright & License (click to expand)\n",
    "# Copyright 2021 Google LLC\n",
    "#\n",
    "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
    "# you may not use this file except in compliance with the License.\n",
    "# You may obtain a copy of the License at\n",
    "#\n",
    "#     https://www.apache.org/licenses/LICENSE-2.0\n",
    "#\n",
    "# Unless required by applicable law or agreed to in writing, software\n",
    "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
    "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
    "# See the License for the specific language governing permissions and\n",
    "# limitations under the License.\n",
    "#\n",
    "# Author: Chanchal Chatterjee\n",
    "# Email: cchatterjee@google.com"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fsv4jGuU89rX"
   },
   "source": [
    "# Vertex Model Monitoring\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lA32H1oKGgpf"
   },
   "source": [
    "## Overview"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "t6Cd51FkG09E"
   },
   "source": [
    "### What is Model Monitoring?\n",
    "\n",
    "Modern applications rely on a well established set of capabilities to monitor the health of their services. Examples include:\n",
    "\n",
    "* software versioning\n",
    "* rigorous deployment processes\n",
    "* event logging\n",
    "* alerting/notication of situations requiring intervention\n",
    "* on-demand and automated diagnostic tracing\n",
    "* automated performance and functional testing\n",
    "\n",
    "You should be able to manage your ML services with the same degree of power and flexibility with which you can manage your applications. That's what MLOps is all about - managing ML services with the best practices Google and the broader computing industry have learned from generations of experience deploying well engineered, reliable, and scalable services.\n",
    "\n",
    "Model monitoring is only one piece of the ML Ops puzzle - it helps answer the following questions:\n",
    "\n",
    "* How well do recent service requests match the training data used to build your model? This is called **training-serving skew**.\n",
    "* How significantly are service requests evolving over time? This is called **drift detection**.\n",
    "\n",
    "If production traffic differs from  training data, or varies substantially over time, that's likely to impact the quality of the answers your model produces. When that happens, you'd like to be alerted automatically and responsively, so that **you can anticipate problems before they affect your customer experiences or your revenue streams**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yG7FcXWKHOhC"
   },
   "source": [
    "### Objective\n",
    "\n",
    "In this notebook, you will learn how to... \n",
    "\n",
    "* deploy a pre-trained model\n",
    "* configure model monitoring\n",
    "* generate some artificial traffic\n",
    "* understand how to interpret the statistics, visualizations, other data reported by the model monitoring feature"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8yVpQt-JHKPF"
   },
   "source": [
    "### Costs \n",
    "\n",
    "This tutorial uses billable components of Google Cloud:\n",
    "\n",
    "* Vertext AI\n",
    "* BigQuery\n",
    "\n",
    "Learn about [Vertext AI\n",
    "pricing](https://cloud.google.com/vertex-ai/pricing) and [Cloud Storage\n",
    "pricing](https://cloud.google.com/storage/pricing), and use the [Pricing\n",
    "Calculator](https://cloud.google.com/products/calculator/)\n",
    "to generate a cost estimate based on your projected usage."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tvgnzT1CKxrO"
   },
   "source": [
    "### The example model\n",
    "\n",
    "The model you'll use in this notebook is based on [this blog post](https://cloud.google.com/blog/topics/developers-practitioners/churn-prediction-game-developers-using-google-analytics-4-ga4-and-bigquery-ml). The idea behind this model is that your company has extensive log data describing how your game users have interacted with the site. The raw data contains the following categories of information:\n",
    "\n",
    "- identity - unique player identitity numbers\n",
    "- demographic features - information about the player, such as the geographic region in which a player is located\n",
    "- behavioral features - counts of the number of times a  player has triggered certain game events, such as reaching a new level\n",
    "- churn propensity - this is the label or target feature, it provides an estimated probability that this player will churn, i.e. stop being an active player.\n",
    "\n",
    "The blog article referenced above explains how to use BigQuery to store the raw data, pre-process it for use in machine learning, and train a model. Because this notebook focuses on model monitoring, rather than training models, you're going to reuse a pre-trained version of this model, which has been exported to Google Cloud Storage. In the next section, you will setup your environment and import this model into your own project."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ze4-nDLfK4pw"
   },
   "source": [
    "## Before you begin"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "i7EUnXsZhAGF"
   },
   "source": [
    "### Setup your dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "id": "020040f91150"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mWARNING: Ignoring invalid distribution -ensorflow (/opt/conda/lib/python3.7/site-packages)\u001b[0m\n",
      "\u001b[33mWARNING: Ignoring invalid distribution -ensorflow (/opt/conda/lib/python3.7/site-packages)\u001b[0m\n",
      "\u001b[33mWARNING: Ignoring invalid distribution -ensorflow (/opt/conda/lib/python3.7/site-packages)\u001b[0m\n",
      "\u001b[33mWARNING: Error parsing requirements for google-cloud-storage: [Errno 2] No such file or directory: '/opt/conda/lib/python3.7/site-packages/google_cloud_storage-1.39.0.dist-info/METADATA'\u001b[0m\n",
      "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "tfx 0.21.5 requires absl-py<0.9,>=0.1.6, but you have absl-py 0.11.0 which is incompatible.\n",
      "tfx 0.21.5 requires apache-beam[gcp]<2.18,>=2.17, but you have apache-beam 2.30.0 which is incompatible.\n",
      "tfx 0.21.5 requires docker<5,>=4.1, but you have docker 5.0.0 which is incompatible.\n",
      "tfx 0.21.5 requires google-api-python-client<2,>=1.7.8, but you have google-api-python-client 2.13.0 which is incompatible.\n",
      "tfx 0.21.5 requires kubernetes<11,>=10.0.1, but you have kubernetes 12.0.1 which is incompatible.\n",
      "tfx 0.21.5 requires pyarrow<0.16,>=0.15, but you have pyarrow 2.0.0 which is incompatible.\n",
      "tfx 0.21.5 requires tensorflow-data-validation<0.22,>=0.21.4, but you have tensorflow-data-validation 1.1.0 which is incompatible.\n",
      "tfx 0.21.5 requires tfx-bsl<0.22,>=0.21.3, but you have tfx-bsl 1.1.0 which is incompatible.\n",
      "tfx-bsl 1.1.0 requires google-api-python-client<2,>=1.7.11, but you have google-api-python-client 2.13.0 which is incompatible.\n",
      "tfx-bsl 1.1.0 requires tensorflow!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.*,!=2.4.*,<3,>=1.15.2, but you have tensorflow 2.1.0 which is incompatible.\n",
      "tensorflow-transform 0.21.2 requires absl-py<0.9,>=0.7, but you have absl-py 0.11.0 which is incompatible.\n",
      "tensorflow-transform 0.21.2 requires tensorflow-metadata<0.22,>=0.21, but you have tensorflow-metadata 1.1.0 which is incompatible.\n",
      "tensorflow-transform 0.21.2 requires tfx-bsl<0.22,>=0.21.3, but you have tfx-bsl 1.1.0 which is incompatible.\n",
      "tensorflow-model-analysis 0.21.6 requires absl-py<0.9,>=0.7, but you have absl-py 0.11.0 which is incompatible.\n",
      "tensorflow-model-analysis 0.21.6 requires pyarrow<1,>=0.15, but you have pyarrow 2.0.0 which is incompatible.\n",
      "tensorflow-model-analysis 0.21.6 requires tensorflow-metadata<0.22,>=0.21, but you have tensorflow-metadata 1.1.0 which is incompatible.\n",
      "tensorflow-model-analysis 0.21.6 requires tfx-bsl<0.22,>=0.21.3, but you have tfx-bsl 1.1.0 which is incompatible.\n",
      "tensorflow-data-validation 1.1.0 requires tensorflow!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.*,!=2.4.*,<3,>=1.15.2, but you have tensorflow 2.1.0 which is incompatible.\n",
      "tensorflow-cloud 0.1.13 requires tensorboard>=2.3.0, but you have tensorboard 2.1.1 which is incompatible.\n",
      "kfp 1.6.2 requires google-api-python-client<2,>=1.7.8, but you have google-api-python-client 2.13.0 which is incompatible.\u001b[0m\n",
      "\u001b[33mWARNING: Ignoring invalid distribution -ensorflow (/opt/conda/lib/python3.7/site-packages)\u001b[0m\n",
      "\u001b[33mWARNING: Ignoring invalid distribution -ensorflow (/opt/conda/lib/python3.7/site-packages)\u001b[0m\n",
      "\u001b[33mWARNING: Ignoring invalid distribution -ensorflow (/opt/conda/lib/python3.7/site-packages)\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "\n",
    "assert sys.version_info.major == 3, \"This notebook requires Python 3.\"\n",
    "\n",
    "# Google Cloud Notebook requires dependencies to be installed with '--user'\n",
    "IS_GOOGLE_CLOUD_NOTEBOOK = os.path.exists(\"/opt/deeplearning/metadata/env_version\")\n",
    "USER_FLAG = \"\"\n",
    "if IS_GOOGLE_CLOUD_NOTEBOOK:\n",
    "    USER_FLAG = \"--user\"\n",
    "\n",
    "if 'google.colab' in sys.modules: \n",
    "    from google.colab import auth\n",
    "    auth.authenticate_user()\n",
    "\n",
    "# Install Python package dependencies.\n",
    "! pip3 install {USER_FLAG} --quiet --upgrade google-api-python-client google-auth-oauthlib \\\n",
    "                                             google-auth-httplib2 oauth2client requests \\\n",
    "                                             google-cloud-aiplatform google-cloud-storage==1.32.0\n",
    "\n",
    "if not os.getenv(\"IS_TESTING\"): \n",
    "    # Automatically restart kernel after installs\n",
    "    import IPython\n",
    "    app = IPython.Application.instance() \n",
    "    app.kernel.do_shutdown(True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BF1j6f9HApxa"
   },
   "source": [
    "### Set up your Google Cloud project\n",
    "\n",
    "**The following steps are required, regardless of your notebook environment.**\n",
    "\n",
    "1. [Select or create a Google Cloud project](https://console.cloud.google.com/cloud-resource-manager). When you first create an account, you get a $300 free credit towards your compute/storage costs.\n",
    "\n",
    "1. Enter your project id in the first line of the cell below.\n",
    "\n",
    "1. [Make sure that billing is enabled for your project](https://cloud.google.com/billing/docs/how-to/modify-project).\n",
    "\n",
    "1. If you are running this notebook locally, you will need to install the [Cloud SDK](https://cloud.google.com/sdk).\n",
    "\n",
    "1. You'll use the *gcloud* command throughout this notebook. In the following cell, enter your project name and run the cell to authenticate yourself with the Google Cloud and initialize your *gcloud* configuration settings.\n",
    "\n",
    "**Model monitoring is currently supported in regions us-central1, europe-west4, asia-east1, and asia-southeast1. To keep things simple for this lab, we're going to use region us-central1 for all our resources (BigQuery training data, Cloud Storage bucket, model and endpoint locations, etc.). You can use any supported region, so long as all resources are co-located.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'status': 'ok', 'restart': True}"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Automatically restart kernel after installs\n",
    "import IPython\n",
    "app = IPython.Application.instance()\n",
    "app.kernel.do_shutdown(True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "cellView": "form",
    "id": "wxiE6dEWOFm3"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Updated property [core/project].\n",
      "Updated property [ai/region].\n"
     ]
    }
   ],
   "source": [
    "# Import globally needed dependencies here, after kernel restart.\n",
    "import copy\n",
    "import numpy as np\n",
    "import os\n",
    "import random\n",
    "import sys\n",
    "import time\n",
    "\n",
    "PROJECT_ID = \"cchatterjee-sandbox\"  # @param {type:\"string\"}\n",
    "REGION = \"us-central1\"  # @param {type:\"string\"}\n",
    "SUFFIX = \"aiplatform.googleapis.com\"\n",
    "API_ENDPOINT = f\"{REGION}-{SUFFIX}\"\n",
    "PREDICT_API_ENDPOINT = f\"{REGION}-prediction-{SUFFIX}\"\n",
    "if os.getenv(\"IS_TESTING\"):\n",
    "    !gcloud --quiet components install beta\n",
    "    !gcloud --quiet components update\n",
    "!gcloud config set project $PROJECT_ID\n",
    "!gcloud config set ai/region $REGION\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "sECqTau7Oh6M"
   },
   "source": [
    "### Login to your Google Cloud account and enable AI services"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "C6H1vZYjvT6w"
   },
   "outputs": [],
   "source": [
    "# The Google Cloud Notebook product has specific requirements\n",
    "IS_GOOGLE_CLOUD_NOTEBOOK = os.path.exists(\"/opt/deeplearning/metadata/env_version\")\n",
    "\n",
    "# If on Google Cloud Notebooks, then don't execute this code\n",
    "if not IS_GOOGLE_CLOUD_NOTEBOOK:\n",
    "    if \"google.colab\" in sys.modules:\n",
    "        from google.colab import auth as google_auth\n",
    "\n",
    "        google_auth.authenticate_user()\n",
    "\n",
    "    # If you are running this notebook locally, replace the string below with the\n",
    "    # path to your service account key and run this cell to authenticate your GCP\n",
    "    # account.\n",
    "    elif not os.getenv(\"IS_TESTING\"):\n",
    "        %env GOOGLE_APPLICATION_CREDENTIALS ''\n",
    "\n",
    "# Execute this line on cloud shell\n",
    "!gcloud services enable aiplatform.googleapis.com\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "btZeLzqQ7pXc"
   },
   "source": [
    "### Define utilities\n",
    "\n",
    "Run the following cells to define some utility functions and distributions used later in this notebook. Although these utilities are not critical to understand the main concepts, feel free to expand the cells\n",
    "in this section if you're curious or want to dive deeper into how some of your API requests are made."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "yhDFSB2YDvfT"
   },
   "outputs": [],
   "source": [
    "# @title Utility imports and constants\n",
    "from google.protobuf.struct_pb2 import Value\n",
    "from google.protobuf.duration_pb2 import Duration\n",
    "from google.cloud.aiplatform_v1beta1.services.job_service import JobServiceClient\n",
    "\n",
    "from google.cloud.aiplatform_v1beta1.types.model_monitoring import ThresholdConfig\n",
    "from google.cloud.aiplatform_v1beta1.types.model_monitoring import SamplingStrategy\n",
    "from google.cloud.aiplatform_v1beta1.types.model_monitoring import ModelMonitoringAlertConfig\n",
    "from google.cloud.aiplatform_v1beta1.types.model_monitoring import ModelMonitoringObjectiveConfig\n",
    "from google.cloud.aiplatform_v1beta1.types.model_deployment_monitoring_job import ModelDeploymentMonitoringJob\n",
    "from google.cloud.aiplatform_v1beta1.types.model_deployment_monitoring_job import ModelDeploymentMonitoringObjectiveConfig\n",
    "from google.cloud.aiplatform_v1beta1.types.model_deployment_monitoring_job import ModelDeploymentMonitoringScheduleConfig\n",
    "\n",
    "from google.cloud.aiplatform_v1beta1.services.endpoint_service import EndpointServiceClient\n",
    "from google.cloud.aiplatform_v1beta1.services.prediction_service import PredictionServiceClient\n",
    "from google.cloud.aiplatform_v1beta1.types.io import BigQuerySource\n",
    "from google.cloud.aiplatform_v1beta1.types.io import GcsSource\n",
    "from google.cloud.aiplatform_v1beta1.types.prediction_service import PredictRequest\n",
    "from google.protobuf import json_format\n",
    "\n",
    "# This is the default value at which you would like the monitoring function to trigger an alert.\n",
    "# In other words, this value fine tunes the alerting sensitivity. This threshold can be customized\n",
    "# on a per feature basis but this is the global default setting.\n",
    "DEFAULT_THRESHOLD_VALUE = 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# @title Utility functions\n",
    "\n",
    "def create_monitoring_job(objective_configs):\n",
    "    # Create sampling configuration.\n",
    "    random_sampling = SamplingStrategy.RandomSampleConfig(sample_rate=LOG_SAMPLE_RATE)\n",
    "    sampling_config = SamplingStrategy(random_sample_config=random_sampling)\n",
    "\n",
    "    # Create schedule configuration.\n",
    "    duration = Duration(seconds=MONITOR_INTERVAL_IN_SECONDS)\n",
    "    schedule_config = ModelDeploymentMonitoringScheduleConfig(monitor_interval=duration)\n",
    "\n",
    "    # Create alerting configuration.\n",
    "    emails = [USER_EMAIL]\n",
    "    email_config = ModelMonitoringAlertConfig.EmailAlertConfig(user_emails=emails)\n",
    "    alerting_config = ModelMonitoringAlertConfig(email_alert_config=email_config)\n",
    "\n",
    "    # Create the monitoring job.\n",
    "    endpoint = f\"projects/{PROJECT_ID}/locations/{REGION}/endpoints/{ENDPOINT_ID}\"\n",
    "    predict_schema = \"\"\n",
    "    analysis_schema = \"\"\n",
    "    job = ModelDeploymentMonitoringJob(\n",
    "        display_name=JOB_NAME,\n",
    "        endpoint=endpoint,\n",
    "        model_deployment_monitoring_objective_configs=objective_configs,\n",
    "        logging_sampling_strategy=sampling_config,\n",
    "        model_deployment_monitoring_schedule_config=schedule_config,\n",
    "        model_monitoring_alert_config=alerting_config,\n",
    "        predict_instance_schema_uri=predict_schema,\n",
    "        analysis_instance_schema_uri=analysis_schema,\n",
    "    )\n",
    "    options = dict(api_endpoint=API_ENDPOINT)\n",
    "    client = JobServiceClient(client_options=options)\n",
    "    parent = f\"projects/{PROJECT_ID}/locations/{REGION}\"\n",
    "    response = client.create_model_deployment_monitoring_job(\n",
    "        parent=parent, model_deployment_monitoring_job=job\n",
    "    )\n",
    "    print(\"Created monitoring job:\")\n",
    "    print(response)\n",
    "    return response\n",
    "\n",
    "\n",
    "def get_thresholds(default_thresholds, custom_thresholds):\n",
    "    thresholds = {}\n",
    "    default_threshold = ThresholdConfig(value=DEFAULT_THRESHOLD_VALUE)\n",
    "    for feature in default_thresholds.split(\",\"):\n",
    "        feature = feature.strip()\n",
    "        thresholds[feature] = default_threshold\n",
    "    for custom_threshold in custom_thresholds.split(\",\"):\n",
    "        pair = custom_threshold.split(\":\")\n",
    "        if len(pair) != 2:\n",
    "            print(f\"Invalid custom skew threshold: {custom_threshold}\")\n",
    "            return\n",
    "        feature, value = pair\n",
    "        thresholds[feature] = ThresholdConfig(value=float(value))\n",
    "    return thresholds\n",
    "\n",
    "\n",
    "def get_deployed_model_ids(endpoint_id):\n",
    "    client_options = dict(api_endpoint=API_ENDPOINT)\n",
    "    client = EndpointServiceClient(client_options=client_options)\n",
    "    parent = f\"projects/{PROJECT_ID}/locations/{REGION}\"\n",
    "    response = client.get_endpoint(name=f\"{parent}/endpoints/{endpoint_id}\")\n",
    "    model_ids = []\n",
    "    for model in response.deployed_models:\n",
    "        model_ids.append(model.id)\n",
    "    return model_ids\n",
    "\n",
    "\n",
    "def set_objectives(model_ids, objective_template):\n",
    "    # Use the same objective config for all models.\n",
    "    objective_configs = []\n",
    "    for model_id in model_ids:\n",
    "        objective_config = copy.deepcopy(objective_template)\n",
    "        objective_config.deployed_model_id = model_id\n",
    "        objective_configs.append(objective_config)\n",
    "    return objective_configs\n",
    "\n",
    "\n",
    "def send_predict_request(endpoint, input_list):\n",
    "    client_options = {\"api_endpoint\": PREDICT_API_ENDPOINT}\n",
    "    client = PredictionServiceClient(client_options=client_options)\n",
    "    params = {}\n",
    "    params = json_format.ParseDict(params, Value())\n",
    "    #request = PredictRequest(endpoint=endpoint, parameters=params)\n",
    "    #inputs = [json_format.ParseDict(input_list, Value())]\n",
    "    #request.instances.extend(inputs)\n",
    "    #response = client.predict(request)\n",
    "    response = client.predict(endpoint=endpoint, instances=input_list, parameters=params)\n",
    "    return response\n",
    "\n",
    "\n",
    "def list_monitoring_jobs():\n",
    "    client_options = dict(api_endpoint=API_ENDPOINT)\n",
    "    parent = f\"projects/{PROJECT_ID}/locations/us-central1\"\n",
    "    client = JobServiceClient(client_options=client_options)\n",
    "    response = client.list_model_deployment_monitoring_jobs(parent=parent)\n",
    "    jobs_list = []\n",
    "    for job in response:\n",
    "        jobs_list.append(\n",
    "            {\n",
    "                \"name\": job.name,\n",
    "                \"state\": job.state.name\n",
    "             }\n",
    "        )\n",
    "    return(jobs_list)\n",
    "\n",
    "def pause_monitoring_job(job):\n",
    "    client_options = dict(api_endpoint=API_ENDPOINT)\n",
    "    client = JobServiceClient(client_options=client_options)\n",
    "    response = client.pause_model_deployment_monitoring_job(name=job)\n",
    "    print(response)\n",
    "\n",
    "\n",
    "def delete_monitoring_job(job):\n",
    "    client_options = dict(api_endpoint=API_ENDPOINT)\n",
    "    client = JobServiceClient(client_options=client_options)\n",
    "    response = client.delete_model_deployment_monitoring_job(name=job)\n",
    "    print(response)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No monitoring jobs\n"
     ]
    }
   ],
   "source": [
    "# Print current job names and status\n",
    "jobs_list = list_monitoring_jobs()\n",
    "njobs = len(jobs_list)\n",
    "if (njobs == 0):\n",
    "    print(\"No monitoring jobs\")\n",
    "else:\n",
    "    for i in range(len(jobs_list)):\n",
    "        print(jobs_list[i]['name'])\n",
    "        print(jobs_list[i]['state'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No jobs to delete.\n"
     ]
    }
   ],
   "source": [
    "# Delete existing monitoring jobs\n",
    "jobs_list = list_monitoring_jobs()\n",
    "if (len(jobs_list) == 0):\n",
    "    print(\"No jobs to delete.\")\n",
    "for i in range(len(jobs_list)):\n",
    "    if (jobs_list[i]['state'] == \"JOB_STATE_RUNNING\"):\n",
    "        pause_monitoring_job(jobs_list[i]['name'])\n",
    "    if (jobs_list[i]['state'] == \"JOB_STATE_PENDING\"):\n",
    "        print(\"{} pending cannot be stopped\".format(jobs_list[i]['name']))\n",
    "    else:\n",
    "        delete_monitoring_job(jobs_list[i]['name'])\n",
    "        print(\"Deleted job\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lAOk8UqvCL0S"
   },
   "source": [
    "## Import your model\n",
    "\n",
    "The churn propensity model you'll be using in this notebook has been trained in BigQuery ML and exported to a Google Cloud Storage bucket. This illustrates how you can easily export a trained model and move a model from one cloud service to another. \n",
    "\n",
    "Run the next cell to import this model into your project. **If you've already imported your model, you can skip this step.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'name': 'projects/901951554789/locations/us-central1/models/4920050651506933760',\n",
       "  'display_name': 'image_tensorflow_model',\n",
       "  'create_time': DatetimeWithNanoseconds(2021, 7, 16, 3, 51, 13, 308910, tzinfo=datetime.timezone.utc),\n",
       "  'container': 'us-docker.pkg.dev/vertex-ai/prediction/tf2-cpu.2-2:latest',\n",
       "  'artifact_uri': 'gs://vapit_data/tf_models/models/model_07152021_2019/checkpoints/cp-203503-0-0.9740/'},\n",
       " {'name': 'projects/901951554789/locations/us-central1/models/6353180495429238784',\n",
       "  'display_name': 'freddiemacdata_20216247028',\n",
       "  'create_time': DatetimeWithNanoseconds(2021, 6, 24, 7, 1, 20, 984405, tzinfo=datetime.timezone.utc),\n",
       "  'container': '',\n",
       "  'artifact_uri': ''},\n",
       " {'name': 'projects/901951554789/locations/us-central1/models/7870893569853095936',\n",
       "  'display_name': 'my_first_tensorflow_model',\n",
       "  'create_time': DatetimeWithNanoseconds(2021, 6, 23, 5, 7, 45, 691387, tzinfo=datetime.timezone.utc),\n",
       "  'container': 'us-docker.pkg.dev/cloud-aiplatform/prediction/tf2-cpu.2-4:latest',\n",
       "  'artifact_uri': 'gs://vapit_data/tf_models/jobdir/model_062121_0825/checkpoints/cp-084311-0-0.9555/'},\n",
       " {'name': 'projects/901951554789/locations/us-central1/models/2825173137337876480',\n",
       "  'display_name': 'my_first_tensorflow_model',\n",
       "  'create_time': DatetimeWithNanoseconds(2021, 6, 21, 16, 20, 45, 560741, tzinfo=datetime.timezone.utc),\n",
       "  'container': 'us-docker.pkg.dev/vertex-ai/prediction/tf2-cpu.2-2:latest',\n",
       "  'artifact_uri': 'gs://vapit_data/tf_models/jobdir/model_062121_0825/checkpoints/cp-084311-0-0.9555/'}]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# List all deployed models\n",
    "from google.cloud.aiplatform import gapic as aip\n",
    "def list_models():\n",
    "    PARENT = \"projects/\" + PROJECT_ID + \"/locations/\" + REGION\n",
    "    API_ENDPOINT = \"{}-aiplatform.googleapis.com\".format(REGION)\n",
    "    client_options = {\"api_endpoint\": API_ENDPOINT}\n",
    "    client = aip.ModelServiceClient(client_options=client_options)\n",
    "    response = client.list_models(parent=PARENT)\n",
    "    model_list = []\n",
    "    for model in response:\n",
    "        model_list.append(\n",
    "            {\n",
    "                \"name\": model.name,\n",
    "                \"display_name\": model.display_name,\n",
    "                \"create_time\": model.create_time,\n",
    "                \"container\":  model.container_spec.image_uri,\n",
    "                \"artifact_uri\": model.artifact_uri\n",
    "            }\n",
    "        )\n",
    "    return(model_list)\n",
    "\n",
    "model_list = list_models()\n",
    "model_list\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'name': 'projects/901951554789/locations/us-central1/endpoints/5417038703354707968',\n",
       "  'display_name': 'freddimac_deployed',\n",
       "  'create_time': DatetimeWithNanoseconds(2021, 6, 24, 15, 26, 21, 375987, tzinfo=datetime.timezone.utc),\n",
       "  'deployed_models': 'projects/901951554789/locations/us-central1/models/6353180495429238784'},\n",
       " {'name': 'projects/901951554789/locations/us-central1/endpoints/5035921584888479744',\n",
       "  'display_name': 'my_first_tensorflow_model_endpoint',\n",
       "  'create_time': DatetimeWithNanoseconds(2021, 6, 23, 5, 14, 35, 882989, tzinfo=datetime.timezone.utc),\n",
       "  'deployed_models': 'projects/901951554789/locations/us-central1/models/7870893569853095936'},\n",
       " {'name': 'projects/901951554789/locations/us-central1/endpoints/7537108227939368960',\n",
       "  'display_name': 'my_first_tensorflow_model_endpoint',\n",
       "  'create_time': DatetimeWithNanoseconds(2021, 6, 21, 16, 20, 50, 482709, tzinfo=datetime.timezone.utc),\n",
       "  'deployed_models': 'projects/901951554789/locations/us-central1/models/2825173137337876480'}]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# List all Endpoints\n",
    "from google.cloud.aiplatform import gapic as aip\n",
    "def list_endpoints():\n",
    "    PARENT = \"projects/\" + PROJECT_ID + \"/locations/\" + REGION\n",
    "    API_ENDPOINT = \"{}-aiplatform.googleapis.com\".format(REGION)\n",
    "    client_options = {\"api_endpoint\": API_ENDPOINT}\n",
    "    client = aip.EndpointServiceClient(client_options=client_options)\n",
    "    response = client.list_endpoints(parent=PARENT)\n",
    "    endpoint_list = []\n",
    "    for endpoint in response:\n",
    "        model_name = ''\n",
    "        if (len(endpoint.deployed_models) > 0):\n",
    "            model_name = endpoint.deployed_models[0].model\n",
    "        endpoint_list.append(\n",
    "            {\n",
    "                \"name\": endpoint.name,\n",
    "                \"display_name\": endpoint.display_name,\n",
    "                \"create_time\": endpoint.create_time,\n",
    "                \"deployed_models\": model_name\n",
    "            }\n",
    "        )\n",
    "    return(endpoint_list)\n",
    "\n",
    "endpoint_list = list_endpoints()\n",
    "endpoint_list\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lAOk8UqvCL0S"
   },
   "source": [
    "### If you already have a deployed endpoint\n",
    "\n",
    "You can reuse your existing endpoint by filling in the value of your endpoint ID in the next cell and running it. **If you've just deployed an endpoint in the previous cell, you should skip this step.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Endpoint found with name freddimac_deployed\n",
      "name = projects/901951554789/locations/us-central1/endpoints/5417038703354707968\n",
      "id = 5417038703354707968\n"
     ]
    }
   ],
   "source": [
    "# Find Endpoint ID for a given display name\n",
    "ENDPOINT_NAME = 'freddimac_deployed'\n",
    "endpoint = ''\n",
    "endpoint_id = ''\n",
    "for i in range(len(endpoint_list)):\n",
    "    if (endpoint_list[i]['display_name'] == ENDPOINT_NAME):\n",
    "        endpoint = endpoint_list[i]['name']\n",
    "        endpoint_id = endpoint.split('/')[-1]\n",
    "        break\n",
    "\n",
    "if (endpoint != ''):\n",
    "    print(\"Endpoint found with name {}\".format(ENDPOINT_NAME))\n",
    "    print(\"name = {}\".format(endpoint))\n",
    "    print(\"id = {}\".format(endpoint_id))\n",
    "\n",
    "if (endpoint == ''):\n",
    "    print(\"No endpoint found with name {}\".format(ENDPOINT_NAME))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "QzZS5piBeBFI"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using endpoint projects/901951554789/locations/us-central1/endpoints/5417038703354707968\n"
     ]
    }
   ],
   "source": [
    "# @title Run this cell only if you want to reuse an existing endpoint.\n",
    "if not os.getenv(\"IS_TESTING\"):\n",
    "    ENDPOINT_ID = endpoint_id  # @param {type:\"string\"}\n",
    "    if ENDPOINT_ID:\n",
    "        ENDPOINT = endpoint\n",
    "        print(f\"Using endpoint {ENDPOINT}\")\n",
    "    else:\n",
    "        print(\"If you want to reuse an existing endpoint, you must specify the endpoint id above.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get Training and Test data\n",
    "\n",
    " -----------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "projects/901951554789/locations/us-central1/endpoints/5417038703354707968\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(10405, 148)"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Read and prepare the test data\n",
    "import pandas as pd\n",
    "print(ENDPOINT)\n",
    "TEST_FEATURE_PATH = f\"gs://tuti_asset/datasets/mortgage_structured_x_test.csv\" \n",
    "x_test = pd.read_csv(TEST_FEATURE_PATH)\n",
    "x_test = x_test.fillna(0)\n",
    "x_test.drop(['Unnamed: 0'], axis=1, inplace=True)\n",
    "cols = x_test.columns\n",
    "a = [x for x in range(len(cols))]\n",
    "for cnt, val in enumerate(cols):\n",
    "    x1 = val.replace('\\\"', '')\n",
    "    x2 = x1.replace(',', '_')\n",
    "    x3 = x2.replace('&', '_')\n",
    "    x4 = x3.replace(' ', '_')\n",
    "    a[cnt] = x4\n",
    "x_test.columns = a\n",
    "x_test.shape\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NKsA_lfl9Ryw"
   },
   "source": [
    "## Run a prediction test\n",
    "\n",
    "Now that you have imported a model and deployed that model to an endpoint, you are ready to verify that it's working. Run the next cell to send a test prediction request. If everything works as expected, you should receive a response encoded in a text representation called JSON.\n",
    "\n",
    "**Try this now by running the next cell and examine the results.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the instances for AutoML Tables model prediction\n",
    "instances_dict = x_test.iloc[0:100].astype(str).to_dict(orient=\"index\")\n",
    "instances_list = []\n",
    "for i in range(len(instances_dict)):\n",
    "    instances_list.append(instances_dict[i])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "id": "QNEb7fDJ9NXc"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "projects/901951554789/locations/us-central1/endpoints/5417038703354707968\n",
      "Prediction Succeeded\n"
     ]
    }
   ],
   "source": [
    "# Predict with AutoML Table model\n",
    "import pprint as pp\n",
    "print(ENDPOINT)\n",
    "#print(\"request:\")\n",
    "#pp.pprint(instances_list)\n",
    "try:\n",
    "    resp = send_predict_request(ENDPOINT, instances_list)\n",
    "    print(\"Prediction Succeeded\")\n",
    "    #pp.pprint(resp)\n",
    "except Exception:\n",
    "    print(\"Prediction Failed\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8rF5iLuXCT7i"
   },
   "source": [
    "## Start your monitoring job\n",
    "\n",
    "Now that you've created an endpoint to serve prediction requests on your model, you're ready to start a monitoring job to keep an eye on model quality and to alert you if and when input begins to deviate in way that may impact your model's prediction quality.\n",
    "\n",
    "In this section, you will configure and create a model monitoring job based on the churn propensity model you imported from BigQuery ML."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'credit_score:1, metropolitan_division:1, mortgage_insurance_percentage:1, Number_of_units:1, cltv:1, original_upb:1, ltv:1, original_interest_rate:1, original_loan_term:1, number_of_borrowers:1, min_CURRENT_ACTUAL_UPB:1, max_CURRENT_ACTUAL_UPB:1, Range_CURRENT_ACTUAL_UPB:1, stdev_CURRENT_ACTUAL_UPB:1, mode_CURRENT_ACTUAL_UPB:1, average_CURRENT_ACTUAL_UPB:1, min_CURRENT_DEFERRED_UPB:1, max_CURRENT_DEFERRED_UPB:1, Range_CURRENT_DEFERRED_UPB:1, mode_CURRENT_DEFERRED_UPB:1, average_CURRENT_DEFERRED_UPB:1, stdev_CURRENT_DEFERRED_UPB:1, min_CURRENT_INTEREST_RATE:1, max_CURRENT_INTEREST_RATE:1, Range_CURRENT_INTEREST_RATE:1, mode_CURRENT_INTEREST_RATE:1, stdev_CURRENT_INTEREST_RATE:1, average_CURRENT_INTEREST_RATE:1, PREFINAL_LOAN_DELINQUENCY_STATUS:1, frequency_0:1, frequency_1:1, frequency_2:1, frequency_3:1, Recency_0:1, Recency_1:1, Recency_2:1, Recency_3:1, first_time_home_buyer_flag_9:1, first_time_home_buyer_flag_N:1, occupancy_status_I:1, occupancy_status_P:1, occupancy_status_S:1, channel_B:1, channel_C:1, channel_R:1, property_state_AK:1, property_state_AL:1, property_state_AR:1, property_state_AZ:1, property_state_CA:1, property_state_CO:1, property_state_CT:1, property_state_DC:1, property_state_DE:1, property_state_FL:1, property_state_GA:1, property_state_HI:1, property_state_IA:1, property_state_ID:1, property_state_IL:1, property_state_IN:1, property_state_KS:1, property_state_KY:1, property_state_LA:1, property_state_MA:1, property_state_MD:1, property_state_ME:1, property_state_MI:1, property_state_MN:1, property_state_MO:1, property_state_MS:1, property_state_MT:1, property_state_NC:1, property_state_ND:1, property_state_NE:1, property_state_NH:1, property_state_NJ:1, property_state_NM:1, property_state_NV:1, property_state_NY:1, property_state_OH:1, property_state_OK:1, property_state_OR:1, property_state_PA:1, property_state_PR:1, property_state_RI:1, property_state_SC:1, property_state_SD:1, property_state_TN:1, property_state_TX:1, property_state_UT:1, property_state_VA:1, property_state_VT:1, property_state_WA:1, property_state_WI:1, property_state_WV:1, property_state_WY:1, property_type_CO:1, property_type_CP:1, property_type_MH:1, property_type_PU:1, property_type_SF:1, loan_purpose_N:1, loan_purpose_P:1, seller_name_BANKOFAMERICA_NA:1, seller_name_BANKOFOKLAHOMA_NATL:1, seller_name_CITIMORTGAGE_INC:1, seller_name_GMACMORTGAGE_LLC:1, seller_name_METLIFEHOMELOANS_ADI:1, seller_name_PNCBANK_NATL:1, seller_name_WELLSFARGOBANK_NA:1, seller_name_BRANCHBANKING_TRUSTC:1, seller_name_CHASEHOMEFINANCELLC:1, seller_name_FIFTHTHIRDBANK:1, seller_name_FIRSTFEDOFNORTHERNMI:1, seller_name_FIRSTNATLCOMMUNTIYBA:1, seller_name_FLAGSTARCAPITALMARKE:1, seller_name_HEARTLANDBANKANDTRUS:1, seller_name_INDEPENDENTBANK:1, seller_name_JOHNSONBANK:1, seller_name_MONONASTATEBANK:1, seller_name_Other_sellers:1, seller_name_SOVEREIGNBANK:1, seller_name_USBANKNA:1, service_name_BANKOFAMERICA_NA:1, service_name_BOKF_NATL:1, service_name_CALIBERHOMELOANS_INC:1, service_name_CITIMORTGAGE_INC:1, service_name_JPMORGANCHASEBANK_NA:1, service_name_OCWENLOANSERVICING_L:1, service_name_PNCBANK_NATL:1, service_name_WELLSFARGOBANK_NA:1, service_name_ALLYBANK:1, service_name_BRANCHBANKING_TRUSTC:1, service_name_CENTRALMTGECO:1, service_name_FIFTHTHIRDBANK:1, service_name_FIFTHTHIRDMTGECO:1, service_name_FIRSTFEDOFNORTHERNMI:1, service_name_FIRSTNATLCOMMUNITYBA:1, service_name_HEARTLANDBANKANDTRUS:1, service_name_INDEPENDENTBANK:1, service_name_JOHNSONBANK:1, service_name_MONONASTATEBANK:1, service_name_NATIONSTARMTGELLC:1, service_name_NATIONSTARMTGELLCDBA:1, service_name_NEWRESIDENTIALMTGELL:1, service_name_Other_servicers:1, service_name_USBANKNA:1'"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Organize the features to monitor\n",
    "def convert_list_to_string(org_list, seperator=' '):\n",
    "    \"\"\" Convert list to string, by joining all item in list with given separator.\n",
    "        Returns the concatenated string \"\"\"\n",
    "    return seperator.join(org_list)\n",
    "# Join all the strings in list\n",
    "feature_list = convert_list_to_string(list(x_test.columns), ', ')\n",
    "feature_list_thresh = convert_list_to_string(list(x_test.columns), ':1, ')\n",
    "feature_list_thresh+=':1'\n",
    "feature_list_thresh\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wW2gLBQ3Zkhq"
   },
   "source": [
    "### Configure the following fields:\n",
    "\n",
    "1. User email - The email address to which you would like monitoring alerts sent.\n",
    "1. Log sample rate - Your prediction requests and responses are logged to BigQuery tables, which are automatically created when you create a monitoring job. This parameter specifies the desired logging frequency for those tables.\n",
    "1. Monitor interval - The  time window over which to analyze your data and report anomalies. The minimum window is one hour (3600 seconds).\n",
    "1. Target field - The prediction target column name in training dataset.\n",
    "1. Skew detection threshold - The skew threshold for each feature you want to monitor.\n",
    "1. Prediction drift threshold - The drift threshold for each feature you want to monitor."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "id": "plpASmM2YIVO"
   },
   "outputs": [],
   "source": [
    "USER_EMAIL = \"cchatterjee@google.com\"  # @param {type:\"string\"}\n",
    "JOB_NAME = \"monitoring_job\"\n",
    "TRAIN_DATA = f\"gs://tuti_asset/datasets/chanchal_mortgage_structured_train.csv\"\n",
    "\n",
    "# Sampling rate (optional, default=.8)\n",
    "LOG_SAMPLE_RATE = 0.8  # @param {type:\"number\"}\n",
    "\n",
    "# Monitoring Interval in seconds (optional, default=3600).\n",
    "MONITOR_INTERVAL_IN_SECONDS = 3600  # @param {type:\"number\"}\n",
    "\n",
    "# URI to training dataset.\n",
    "DATASET_GCS_URI = TRAIN_DATA  # @param {type:\"string\"}\n",
    "# Prediction target column name in training dataset.\n",
    "TARGET = \"label\"\n",
    "\n",
    "# Skew and drift thresholds.\n",
    "SKEW_DEFAULT_THRESHOLDS = feature_list  # @param {type:\"string\"}\n",
    "SKEW_CUSTOM_THRESHOLDS = feature_list_thresh  # @param {type:\"string\"}\n",
    "DRIFT_DEFAULT_THRESHOLDS = feature_list  # @param {type:\"string\"}\n",
    "DRIFT_CUSTOM_THRESHOLDS = feature_list_thresh  # @param {type:\"string\"}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mjVSViZR-dP2"
   },
   "source": [
    "### Create your monitoring job\n",
    "\n",
    "The following code uses the Google Python client library to translate your configuration settings into a programmatic request to start a model monitoring job. To do this successfully, you need to specify your alerting thresholds (for both skew and drift), your training data source, and apply those settings to all deployed models on your new endpoint (of which there should only be one at this point).\n",
    "\n",
    "Instantiating a monitoring job can take some time. If everything looks good with your request, you'll get a successful API response. Then, you'll need to check your email to receive a notification that the job is running.\n",
    "\n",
    "Drift Detection\n",
    "\n",
    "You need to specify the data drift threshold for the features you want to monitoring. The whole idea behind the alerting is to see if a feature's data distribution distance is above the threshold you set. If it is, we will send email alerts to the $USER_EMAIL you specified above.\n",
    "\n",
    "How do we calculate the feature distribution distance? We use [L-infinity distance](https://en.wikipedia.org/wiki/Chebyshev_distance) for categorical features and [Jensen-Shannon divergence](https://en.wikipedia.org/wiki/Jensen%E2%80%93Shannon_divergence) for numerical features. More details are [here](https://www.tensorflow.org/tfx/guide/tfdv#drift_detection).\n",
    "\n",
    "Below you just need to specify the features that you want use default threshold(0.001) and the customized threshold features. If you don't want to monitor a feature, feel free to skip filling into any of these fields.\n",
    "\n",
    "Note: if you want to enable the feature attributions score\n",
    "(based on Sampled Sharpley method, more details are [here](https://cloud.google.com/ai-platform-unified/docs/explainable-ai)) monitoring, just change the \"enable_feature_attributes\" to True in monitoring_objective_config_template. Make sure your model is configed with explanations [requirement](https://cloud.google.com/ai-platform-unified/docs/explainable-ai/configuring-explanations). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "-62TYm2iYv3K"
   },
   "outputs": [],
   "source": [
    "# Set thresholds specifying alerting criteria for training/serving skew and create config object.\n",
    "skew_thresholds = get_thresholds(SKEW_DEFAULT_THRESHOLDS, SKEW_CUSTOM_THRESHOLDS)\n",
    "skew_config = ModelMonitoringObjectiveConfig.TrainingPredictionSkewDetectionConfig(\n",
    "    skew_thresholds=skew_thresholds\n",
    ")\n",
    "\n",
    "# Set thresholds specifying alerting criteria for serving drift and create config object.\n",
    "drift_thresholds = get_thresholds(DRIFT_DEFAULT_THRESHOLDS, DRIFT_CUSTOM_THRESHOLDS)\n",
    "drift_config = ModelMonitoringObjectiveConfig.PredictionDriftDetectionConfig(\n",
    "    drift_thresholds=drift_thresholds)\n",
    "#explanation_config = ModelMonitoringObjectiveConfig.ExplanationConfig(\n",
    "#    enable_feature_attributes = False)\n",
    "\n",
    "# Specify training dataset source location (used for schema generation).\n",
    "# training_dataset = ModelMonitoringObjectiveConfig.TrainingDataset(target_field=TARGET)\n",
    "# training_dataset.bigquery_source = BigQuerySource(input_uri=DATASET_BQ_URI)\n",
    "training_dataset = ModelMonitoringObjectiveConfig.TrainingDataset(target_field=TARGET)\n",
    "training_dataset.data_format = \"csv\"\n",
    "training_dataset.gcs_source = GcsSource(uris=[DATASET_GCS_URI])\n",
    "\n",
    "# Aggregate the above settings into a ModelMonitoringObjectiveConfig object and use\n",
    "# that object to adjust the ModelDeploymentMonitoringObjectiveConfig object.\n",
    "objective_config = ModelMonitoringObjectiveConfig(\n",
    "    training_dataset=training_dataset,\n",
    "    training_prediction_skew_detection_config=skew_config,\n",
    "    prediction_drift_detection_config=drift_config,\n",
    "    #explanation_config = explanation_config,\n",
    ")\n",
    "objective_template = ModelDeploymentMonitoringObjectiveConfig(\n",
    "    objective_config=objective_config\n",
    ")\n",
    "\n",
    "# Find all deployed model ids on the created endpoint and set objectives for each.\n",
    "model_ids = get_deployed_model_ids(ENDPOINT_ID)\n",
    "objective_configs = set_objectives(model_ids, objective_template)\n",
    "\n",
    "# Create the monitoring job for all deployed models on this endpoint.\n",
    "monitoring_job = create_monitoring_job(objective_configs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "id": "OiwOVR4D_xhl"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0th Prediction Succeeded\n",
      "1th Prediction Succeeded\n",
      "2th Prediction Succeeded\n",
      "3th Prediction Succeeded\n",
      "4th Prediction Succeeded\n",
      "5th Prediction Succeeded\n",
      "6th Prediction Succeeded\n",
      "7th Prediction Succeeded\n",
      "8th Prediction Succeeded\n",
      "9th Prediction Succeeded\n",
      "10th Prediction Succeeded\n",
      "11th Prediction Succeeded\n",
      "12th Prediction Succeeded\n",
      "13th Prediction Succeeded\n",
      "14th Prediction Succeeded\n",
      "15th Prediction Succeeded\n",
      "16th Prediction Succeeded\n",
      "17th Prediction Succeeded\n",
      "18th Prediction Succeeded\n",
      "19th Prediction Succeeded\n",
      "20th Prediction Succeeded\n",
      "21th Prediction Succeeded\n",
      "22th Prediction Succeeded\n",
      "23th Prediction Succeeded\n",
      "24th Prediction Succeeded\n",
      "25th Prediction Succeeded\n",
      "26th Prediction Succeeded\n",
      "27th Prediction Succeeded\n",
      "28th Prediction Succeeded\n",
      "29th Prediction Succeeded\n",
      "30th Prediction Succeeded\n",
      "31th Prediction Succeeded\n",
      "32th Prediction Succeeded\n",
      "33th Prediction Succeeded\n",
      "34th Prediction Succeeded\n",
      "35th Prediction Succeeded\n",
      "36th Prediction Succeeded\n",
      "37th Prediction Succeeded\n",
      "38th Prediction Succeeded\n",
      "39th Prediction Succeeded\n",
      "40th Prediction Succeeded\n",
      "41th Prediction Succeeded\n",
      "42th Prediction Succeeded\n",
      "43th Prediction Succeeded\n",
      "44th Prediction Succeeded\n",
      "45th Prediction Succeeded\n",
      "46th Prediction Succeeded\n",
      "47th Prediction Succeeded\n",
      "48th Prediction Succeeded\n",
      "49th Prediction Succeeded\n",
      "50th Prediction Succeeded\n",
      "51th Prediction Succeeded\n",
      "52th Prediction Succeeded\n",
      "53th Prediction Succeeded\n",
      "54th Prediction Succeeded\n",
      "55th Prediction Succeeded\n",
      "56th Prediction Succeeded\n",
      "57th Prediction Succeeded\n",
      "58th Prediction Succeeded\n",
      "59th Prediction Succeeded\n",
      "60th Prediction Succeeded\n",
      "61th Prediction Succeeded\n",
      "62th Prediction Succeeded\n",
      "63th Prediction Succeeded\n",
      "64th Prediction Succeeded\n",
      "65th Prediction Succeeded\n",
      "66th Prediction Succeeded\n",
      "67th Prediction Succeeded\n",
      "68th Prediction Succeeded\n",
      "69th Prediction Succeeded\n",
      "70th Prediction Succeeded\n",
      "71th Prediction Succeeded\n",
      "72th Prediction Succeeded\n",
      "73th Prediction Succeeded\n",
      "74th Prediction Succeeded\n",
      "75th Prediction Succeeded\n",
      "76th Prediction Succeeded\n",
      "77th Prediction Succeeded\n",
      "78th Prediction Succeeded\n",
      "79th Prediction Succeeded\n",
      "80th Prediction Succeeded\n",
      "81th Prediction Succeeded\n",
      "82th Prediction Succeeded\n",
      "83th Prediction Succeeded\n",
      "84th Prediction Succeeded\n",
      "85th Prediction Succeeded\n",
      "86th Prediction Succeeded\n",
      "87th Prediction Succeeded\n",
      "88th Prediction Succeeded\n",
      "89th Prediction Succeeded\n",
      "90th Prediction Succeeded\n",
      "91th Prediction Succeeded\n",
      "92th Prediction Succeeded\n",
      "93th Prediction Succeeded\n",
      "94th Prediction Succeeded\n",
      "95th Prediction Succeeded\n",
      "96th Prediction Succeeded\n",
      "97th Prediction Succeeded\n",
      "98th Prediction Succeeded\n",
      "99th Prediction Succeeded\n"
     ]
    }
   ],
   "source": [
    "#  Predict with AutoML Table model\n",
    "\n",
    "instances_dict = x_test.iloc[0:100].astype(str).to_dict(orient=\"index\")\n",
    "instances_list = []\n",
    "for i in range(len(instances_dict)):\n",
    "    instances_list.append(instances_dict[i])\n",
    "\n",
    "# # Run a prediction request to generate schema, if necessary.\n",
    "for i in range(100):\n",
    "    try:\n",
    "        _ = send_predict_request(ENDPOINT, instances_list)\n",
    "        print(\"{}th Prediction Succeeded\".format(i))\n",
    "    except Exception:\n",
    "        print(\"{}th Prediction Failed\".format(i))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SaXYVFFslRru"
   },
   "source": [
    "After a minute or two, you should receive email at the address you configured above for USER_EMAIL. This email confirms successful deployment of your monitoring job. Here's a sample of what this email might look like:\n",
    "<br>\n",
    "<br>\n",
    "<img src=\"https://storage.googleapis.com/mco-general/img/mm6.png\" />\n",
    "<br>\n",
    "As your monitoring job collects data, measurements are stored in Google Cloud Storage and you are free to examine your data at any time. The circled path in the image above specifies the location of your measurements in Google Cloud Storage. Run the following cell to take a look at your measurements in Cloud Storage.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "id": "XV-vru2Pm1oX"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gs://cloud-ai-platform-e3baca16-39fe-4ad9-9cd1-957fa54da12d/instance_schemas/job-3248190444915392512/:\n",
      "gs://cloud-ai-platform-e3baca16-39fe-4ad9-9cd1-957fa54da12d/instance_schemas/job-3248190444915392512/analysis\n",
      "\n",
      "gs://cloud-ai-platform-e3baca16-39fe-4ad9-9cd1-957fa54da12d/instance_schemas/job-3602285965617397760/:\n",
      "gs://cloud-ai-platform-e3baca16-39fe-4ad9-9cd1-957fa54da12d/instance_schemas/job-3602285965617397760/analysis\n",
      "\n",
      "gs://cloud-ai-platform-e3baca16-39fe-4ad9-9cd1-957fa54da12d/instance_schemas/job-3851672794983038976/:\n",
      "gs://cloud-ai-platform-e3baca16-39fe-4ad9-9cd1-957fa54da12d/instance_schemas/job-3851672794983038976/analysis\n",
      "\n",
      "gs://cloud-ai-platform-e3baca16-39fe-4ad9-9cd1-957fa54da12d/instance_schemas/job-4401111949522239488/:\n",
      "gs://cloud-ai-platform-e3baca16-39fe-4ad9-9cd1-957fa54da12d/instance_schemas/job-4401111949522239488/analysis\n",
      "\n",
      "gs://cloud-ai-platform-e3baca16-39fe-4ad9-9cd1-957fa54da12d/instance_schemas/job-4461470739840630784/:\n",
      "gs://cloud-ai-platform-e3baca16-39fe-4ad9-9cd1-957fa54da12d/instance_schemas/job-4461470739840630784/analysis\n",
      "\n",
      "gs://cloud-ai-platform-e3baca16-39fe-4ad9-9cd1-957fa54da12d/instance_schemas/job-5367046108662988800/:\n",
      "gs://cloud-ai-platform-e3baca16-39fe-4ad9-9cd1-957fa54da12d/instance_schemas/job-5367046108662988800/analysis\n",
      "\n",
      "gs://cloud-ai-platform-e3baca16-39fe-4ad9-9cd1-957fa54da12d/instance_schemas/job-6297074616087085056/:\n",
      "gs://cloud-ai-platform-e3baca16-39fe-4ad9-9cd1-957fa54da12d/instance_schemas/job-6297074616087085056/analysis\n",
      "\n",
      "gs://cloud-ai-platform-e3baca16-39fe-4ad9-9cd1-957fa54da12d/instance_schemas/job-7343774501357748224/:\n",
      "gs://cloud-ai-platform-e3baca16-39fe-4ad9-9cd1-957fa54da12d/instance_schemas/job-7343774501357748224/analysis\n",
      "\n",
      "gs://cloud-ai-platform-e3baca16-39fe-4ad9-9cd1-957fa54da12d/instance_schemas/job-804583026787876864/:\n",
      "gs://cloud-ai-platform-e3baca16-39fe-4ad9-9cd1-957fa54da12d/instance_schemas/job-804583026787876864/analysis\n",
      "\n",
      "gs://cloud-ai-platform-e3baca16-39fe-4ad9-9cd1-957fa54da12d/instance_schemas/job-8496696005964595200/:\n",
      "gs://cloud-ai-platform-e3baca16-39fe-4ad9-9cd1-957fa54da12d/instance_schemas/job-8496696005964595200/analysis\n",
      "\n",
      "gs://cloud-ai-platform-e3baca16-39fe-4ad9-9cd1-957fa54da12d/instance_schemas/job-8793652106394337280/:\n",
      "gs://cloud-ai-platform-e3baca16-39fe-4ad9-9cd1-957fa54da12d/instance_schemas/job-8793652106394337280/analysis\n",
      "\n",
      "gs://cloud-ai-platform-e3baca16-39fe-4ad9-9cd1-957fa54da12d/instance_schemas/job-9012797967949627392/:\n",
      "gs://cloud-ai-platform-e3baca16-39fe-4ad9-9cd1-957fa54da12d/instance_schemas/job-9012797967949627392/analysis\n",
      "gs://cloud-ai-platform-e3baca16-39fe-4ad9-9cd1-957fa54da12d/model_monitoring/job-3602285965617397760/:\n",
      "gs://cloud-ai-platform-e3baca16-39fe-4ad9-9cd1-957fa54da12d/model_monitoring/job-3602285965617397760/serving/\n",
      "gs://cloud-ai-platform-e3baca16-39fe-4ad9-9cd1-957fa54da12d/model_monitoring/job-3602285965617397760/training/\n",
      "\n",
      "gs://cloud-ai-platform-e3baca16-39fe-4ad9-9cd1-957fa54da12d/model_monitoring/job-4461470739840630784/:\n",
      "gs://cloud-ai-platform-e3baca16-39fe-4ad9-9cd1-957fa54da12d/model_monitoring/job-4461470739840630784/serving/\n",
      "gs://cloud-ai-platform-e3baca16-39fe-4ad9-9cd1-957fa54da12d/model_monitoring/job-4461470739840630784/training/\n",
      "\n",
      "gs://cloud-ai-platform-e3baca16-39fe-4ad9-9cd1-957fa54da12d/model_monitoring/job-5367046108662988800/:\n",
      "gs://cloud-ai-platform-e3baca16-39fe-4ad9-9cd1-957fa54da12d/model_monitoring/job-5367046108662988800/serving/\n",
      "gs://cloud-ai-platform-e3baca16-39fe-4ad9-9cd1-957fa54da12d/model_monitoring/job-5367046108662988800/training/\n",
      "\n",
      "gs://cloud-ai-platform-e3baca16-39fe-4ad9-9cd1-957fa54da12d/model_monitoring/job-6297074616087085056/:\n",
      "gs://cloud-ai-platform-e3baca16-39fe-4ad9-9cd1-957fa54da12d/model_monitoring/job-6297074616087085056/serving/\n",
      "gs://cloud-ai-platform-e3baca16-39fe-4ad9-9cd1-957fa54da12d/model_monitoring/job-6297074616087085056/training/\n",
      "\n",
      "gs://cloud-ai-platform-e3baca16-39fe-4ad9-9cd1-957fa54da12d/model_monitoring/job-7343774501357748224/:\n",
      "gs://cloud-ai-platform-e3baca16-39fe-4ad9-9cd1-957fa54da12d/model_monitoring/job-7343774501357748224/training/\n",
      "\n",
      "gs://cloud-ai-platform-e3baca16-39fe-4ad9-9cd1-957fa54da12d/model_monitoring/job-8496696005964595200/:\n",
      "gs://cloud-ai-platform-e3baca16-39fe-4ad9-9cd1-957fa54da12d/model_monitoring/job-8496696005964595200/training/\n",
      "\n",
      "gs://cloud-ai-platform-e3baca16-39fe-4ad9-9cd1-957fa54da12d/model_monitoring/job-8793652106394337280/:\n",
      "gs://cloud-ai-platform-e3baca16-39fe-4ad9-9cd1-957fa54da12d/model_monitoring/job-8793652106394337280/serving/\n",
      "gs://cloud-ai-platform-e3baca16-39fe-4ad9-9cd1-957fa54da12d/model_monitoring/job-8793652106394337280/training/\n"
     ]
    }
   ],
   "source": [
    "!gsutil ls gs://cloud-ai-platform-e3baca16-39fe-4ad9-9cd1-957fa54da12d/instance_schemas/*\n",
    "!gsutil ls gs://cloud-ai-platform-e3baca16-39fe-4ad9-9cd1-957fa54da12d/model_monitoring/*\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XgUwU0sDpUUD"
   },
   "source": [
    "You will notice the following components in these Cloud Storage paths:\n",
    "\n",
    "- **cloud-ai-platform-..** - This is a bucket created for you and assigned to capture your service's prediction data. Each monitoring job you create will trigger creation of a new folder in this bucket.\n",
    "- **[model_monitoring|instance_schemas]/job-..** - This is your unique monitoring job number, which you can see above in both the response to your job creation requesst and the email notification. \n",
    "- **instance_schemas/job-../analysis** - This is the monitoring jobs understanding and encoding of your training data's schema (field names, types, etc.).\n",
    "- **instance_schemas/job-../predict** - This is the first prediction made to your model after the current monitoring job was enabled.\n",
    "- **model_monitoring/job-../serving** - This folder is used to record data relevant to drift calculations. It contains measurement summaries for every hour your model serves traffic.\n",
    "- **model_monitoring/job-../training** - This folder is used to record data relevant to training-serving skew calculations. It contains an ongoing summary of prediction data relative to training data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'name': 'projects/901951554789/locations/us-central1/modelDeploymentMonitoringJobs/804583026787876864',\n",
       "  'state': 'JOB_STATE_PENDING'}]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# List all monitoring jobs\n",
    "job_list = list_monitoring_jobs()\n",
    "job_list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8V2zo7-MMd7G"
   },
   "source": [
    "### You can create monitoring jobs with other user interfaces\n",
    "\n",
    "In the previous cells, you created a monitoring job using the Python client library. You can also use the *gcloud* command line tool to create a model monitoring job and, in the near future, you will be able to use the Cloud Console, as well for this function. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bQohDTJgLQlW"
   },
   "source": [
    "## Interpret your results\n",
    "\n",
    "While waiting for your results, which, as noted, may take up to an hour, you can read ahead to get sense of the alerting experience."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "uGPI92qbOFUR"
   },
   "source": [
    "### Here's what a sample email alert looks like...\n",
    "\n",
    "<img src=\"https://storage.googleapis.com/mco-general/img/mm7.png\" />\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HoaqsxpaRs1m"
   },
   "source": [
    "This email is warning you that the *cnt_user_engagement*, *country* and *language* feature values seen in production have skewed above your threshold between training and serving your model. It's also telling you that the *cnt_user_engagement* feature value is drifting significantly over time, again, as per your threshold specification."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "w4jVIVq4VzB_"
   },
   "source": [
    "### Monitoring results in the Cloud Console\n",
    "\n",
    "You can examine your model monitoring data from the Cloud Console. Below is a screenshot of those capabilities."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2OdIMVBAPZi_"
   },
   "source": [
    "#### Monitoring Status\n",
    "\n",
    "<img src=\"https://storage.googleapis.com/mco-general/img/mm1.png\" />\n",
    "\n",
    "#### Monitoring Alerts\n",
    "\n",
    "<img src=\"https://storage.googleapis.com/mco-general/img/mm2.png\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TpV-iwP9qw9c"
   },
   "source": [
    "## Clean up\n",
    "\n",
    "To clean up all Google Cloud resources used in this project, you can [delete the Google Cloud\n",
    "project](https://cloud.google.com/resource-manager/docs/creating-managing-projects#shutting_down_projects) you used for the tutorial.\n",
    "\n",
    "Otherwise, you can delete the individual resources you created in this tutorial:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "projects/901951554789/locations/us-central1/modelDeploymentMonitoringJobs/804583026787876864\n",
      "JOB_STATE_PENDING\n"
     ]
    }
   ],
   "source": [
    "# Print current job names and status\n",
    "jobs_list = list_monitoring_jobs()\n",
    "njobs = len(jobs_list)\n",
    "if (njobs == 0):\n",
    "    print(\"No monitoring jobs\")\n",
    "else:\n",
    "    for i in range(len(jobs_list)):\n",
    "        print(jobs_list[i]['name'])\n",
    "        print(jobs_list[i]['state'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "projects/901951554789/locations/us-central1/modelDeploymentMonitoringJobs/804583026787876864 pending cannot be stopped\n"
     ]
    }
   ],
   "source": [
    "# Delete all monitoring jobs\n",
    "jobs_list = list_monitoring_jobs()\n",
    "if (len(jobs_list) == 0):\n",
    "    print(\"No jobs to delete.\")\n",
    "for i in range(len(jobs_list)):\n",
    "    if (jobs_list[i]['state'] == \"JOB_STATE_RUNNING\"):\n",
    "        pause_monitoring_job(jobs_list[i]['name'])\n",
    "    if (jobs_list[i]['state'] == \"JOB_STATE_PENDING\"):\n",
    "        print(\"{} pending cannot be stopped\".format(jobs_list[i]['name']))\n",
    "    else:\n",
    "        delete_monitoring_job(jobs_list[i]['name'])\n",
    "        print(\"Deleted job\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "TPP_ImwJDFJf"
   },
   "outputs": [],
   "source": [
    "# out = !gcloud ai endpoints undeploy-model $ENDPOINT_ID --deployed-model-id $DEPLOYED_MODEL_ID\n",
    "# if _exit_code == 0:\n",
    "#     print(\"Model undeployed.\")\n",
    "# else:\n",
    "#     print(\"Error undeploying model:\", out)\n",
    "\n",
    "# out = !gcloud ai endpoints delete $ENDPOINT_ID --quiet\n",
    "# if _exit_code == 0:\n",
    "#     print(\"Endpoint deleted.\")\n",
    "# else:\n",
    "#     print(\"Error deleting endpoint:\", out)\n",
    "\n",
    "# out = !gcloud ai models delete $MODEL_ID --quiet\n",
    "# if _exit_code == 0:\n",
    "#     print(\"Model deleted.\")\n",
    "# else:\n",
    "#     print(\"Error deleting model:\", out)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "j3Dh15h3-NoO"
   },
   "source": [
    "## Learn more about model monitoring\n",
    "\n",
    "**Congratulations!** You've now learned what model monitoring is, how to configure and enable it, and how to find and interpret the results. Check out the following resources to learn more about model monitoring and ML Ops.\n",
    "\n",
    "- [TensorFlow Data Validation](https://www.tensorflow.org/tfx/guide/tfdv)\n",
    "- [Data Understanding, Validation, and Monitoring At Scale](https://blog.tensorflow.org/2018/09/introducing-tensorflow-data-validation.html)\n",
    "- [Vertex Product Documentation](https://cloud.google.com/vertex-ai/docs)\n",
    "- [Model Monitoring Reference Docs](https://cloud.google.com/vertex-ai/docs/model-monitoring)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [
    "lA32H1oKGgpf"
   ],
   "name": "model_monitoring.ipynb",
   "toc_visible": true
  },
  "environment": {
   "name": "tf2-gpu.2-1.m71",
   "type": "gcloud",
   "uri": "gcr.io/deeplearning-platform-release/tf2-gpu.2-1:m71"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
