{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==============================================================================\n",
    "# Copyright 2021 Google LLC. This software is provided as-is, without warranty\n",
    "# or representation for any use or purpose. Your use of it is subject to your\n",
    "# agreement with Google.\n",
    "# ==============================================================================\n",
    "#\n",
    "# Author: Chanchal Chatterjee\n",
    "# Email: cchatterjee@google.com\n",
    "#\n",
    "# Filename image_tf.ipynb\n",
    "# Classify numbers 0-9\n",
    "#"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Import packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!cd /home/jupyter/vapit/ai-platform-tf/Vertex\n",
    "!python3 -m pip install -r ./requirements.txt -U -q --user\n",
    "!python3 -m pip install -U -q google-cloud-aiplatform\n",
    "!python3 -m pip install -U -q google-cloud-storage==1.32\n",
    "!gcloud components update --quiet\n",
    "!python3 -m pip install -U -q build\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'status': 'ok', 'restart': True}"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Automatically restart kernel after installs\n",
    "import IPython\n",
    "app = IPython.Application.instance()\n",
    "app.kernel.do_shutdown(True)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import packages\n",
    "\n",
    "import json\n",
    "import logging\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from datetime import datetime\n",
    "from pytz import timezone\n",
    "from googleapiclient import discovery\n",
    "from google.cloud import aiplatform"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Configure Global Variables"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "List your current GCP project name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['cchatterjee-sandbox']"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "project_id = !gcloud config list --format 'value(core.project)' 2>/dev/null\n",
    "project_id"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Configure your system variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "JOB_DIR   =  gs://vapit_data/tf_models/jobdir\n",
      "MODEL_DIR =  gs://vapit_data/tf_models/models\n"
     ]
    }
   ],
   "source": [
    "# Configure your global variables\n",
    "PROJECT = project_id[0]  # Replace with your project ID\n",
    "USER = 'cchatterjee'             # Replace with your user name\n",
    "BUCKET_NAME = 'vapit_data'       # Replace with your gcs bucket name - gloablly unique\n",
    "\n",
    "FOLDER_NAME = 'tf_models'\n",
    "TIMEZONE = 'US/Pacific'\n",
    "REGION = 'us-central1'\n",
    "PACKAGE_URIS = f\"gs://{BUCKET_NAME}/trainer/tensorflow/trainer-0.1.tar.gz\" \n",
    "\n",
    "JOB_DIR = 'gs://{}/{}/jobdir'.format(\n",
    "    BUCKET_NAME,\n",
    "    FOLDER_NAME\n",
    "    )\n",
    "MODEL_DIR = 'gs://{}/{}/models'.format(\n",
    "    BUCKET_NAME,\n",
    "    FOLDER_NAME\n",
    "    )\n",
    "print(\"JOB_DIR   = \", JOB_DIR)\n",
    "print(\"MODEL_DIR = \", MODEL_DIR)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Authenticate your GCP account\n",
    "\n",
    "This is required if you run the notebook in Colab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "  from google.colab import auth\n",
    "  auth.authenticate_user()\n",
    "  print(\"Colab user is authenticated.\")\n",
    "except: pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create your bucket"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating gs://vapit_data/...\n",
      "ServiceException: 409 A Cloud Storage bucket named 'vapit_data' already exists. Try another name. Bucket names must be globally unique across all Google Cloud projects, including those outside of your organization.\n"
     ]
    }
   ],
   "source": [
    "!gsutil mb -l $REGION gs://$BUCKET_NAME "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CommandException: 1 files/objects could not be removed.\n"
     ]
    }
   ],
   "source": [
    "# Clean old job logs, job packages and models\n",
    "!gsutil -m -q rm $JOB_DIR/packages/**\n",
    "!gsutil -m -q rm $MODEL_DIR/model**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-----------\n",
    "### Special functions\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "#------\n",
    "def find_best_model_dir(model_dir, offset=1, maxFlag=1):\n",
    "    # Get a list of model directories\n",
    "    all_models = ! gsutil ls $model_dir\n",
    "    print(\"\")\n",
    "    print(\"All Models = \")\n",
    "    print(*all_models, sep='\\n')\n",
    "\n",
    "    # Check if model dirs exist\n",
    "    if ((\"CommandException\" in all_models[0]) or (len(all_models) <= 1)):\n",
    "        print(\"Create the models first.\")\n",
    "        return \"\"\n",
    "\n",
    "    # Find the best model from checkpoints\n",
    "    import re\n",
    "    best_acc = -np.Inf\n",
    "    if (maxFlag != 1):\n",
    "        best_acc = np.Inf\n",
    "    best_model_dir = \"\"\n",
    "    tup_list = []\n",
    "    for i in range(1,len(all_models)):\n",
    "        all_floats = re.findall(r\"[-+]?\\d*\\.\\d+|\\d+\", all_models[i]) #Find the floats in the string\n",
    "        cur_acc = -float(all_floats[-offset]) #which item is the model optimization metric\n",
    "        tup_list.append([all_models[i],cur_acc])\n",
    "        if (maxFlag*(cur_acc > best_acc) or (1-maxFlag)*(cur_acc < best_acc)):\n",
    "            best_acc = cur_acc\n",
    "            best_model_dir = all_models[i]\n",
    "    if maxFlag:\n",
    "        tup_list.sort(key=lambda tup: tup[1], reverse=False)\n",
    "    else:\n",
    "        tup_list.sort(key=lambda tup: tup[1], reverse=True)\n",
    "    #for i in range(len(tup_list)):\n",
    "    #    print(tup_list[i][0])\n",
    "    print(\"Best Accuracy  from Checkpoints = \", best_acc)\n",
    "    print(\"Best Model Dir from Checkpoints = \", best_model_dir)\n",
    "    \n",
    "    return best_model_dir\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot original images\n",
    "# image_array is np.array(num_images, x_size, y_size)\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def plot_images(image_array):\n",
    "    fig, axarr = plt.subplots(4,4, figsize=(8, 8))\n",
    "    for i in range(4):\n",
    "        for j in range(4):\n",
    "            #axarr[i,j].set_title('Image-'+str(4*i+j))\n",
    "            axarr[i,j].imshow(image_array[4*i+j,:,:], cmap='gray')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build python package and upload to your bucket"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing ./setup.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile ./setup.py\n",
    "\n",
    "# python3\n",
    "\n",
    "# ==============================================================================\n",
    "# Copyright 2020 Google LLC. This software is provided as-is, without warranty\n",
    "# or representation for any use or purpose. Your use of it is subject to your\n",
    "# agreement with Google.\n",
    "# ==============================================================================\n",
    "\n",
    "# https://cloud.google.com/ai-platform/training/docs/runtime-version-list\n",
    "\n",
    "from setuptools import find_packages\n",
    "from setuptools import setup\n",
    "\n",
    "#Runtime 2.1\n",
    "REQUIRED_PACKAGES = ['tensorflow==2.1.0',\n",
    "                     'numpy==1.18.0',\n",
    "                     'pandas==0.25.3',\n",
    "                     'scikit-learn==0.22',\n",
    "                     'google-cloud-storage==1.23.0',\n",
    "                     'gcsfs==0.6.1',\n",
    "                     'cloudml-hypertune',\n",
    "                    ]\n",
    "setup(\n",
    "    name='trainer',\n",
    "    version='0.1',\n",
    "    install_requires=REQUIRED_PACKAGES,\n",
    "    packages=find_packages(),\n",
    "    include_package_data=True,\n",
    "    description='Trainer package for Tensorflow Task'\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the tf_trainer directory and load the trainer files in it\n",
    "!mkdir -p trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing ./trainer/__init__.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile ./trainer/__init__.py\n",
    "# python3\n",
    "\n",
    "# ==============================================================================\n",
    "# Copyright 2020 Google LLC. This software is provided as-is, without warranty\n",
    "# or representation for any use or purpose. Your use of it is subject to your\n",
    "# agreement with Google.\n",
    "# ==============================================================================\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing ./trainer/inputs.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile ./trainer/inputs.py\n",
    "\n",
    "# Create the train and label lists\n",
    "import math\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "#------\n",
    "def load_data():\n",
    "\n",
    "    \"\"\"Creates train and test data set\"\"\"\n",
    "    mnist = tf.keras.datasets.mnist\n",
    "    (X_train, y_train),(X_test, y_test) = mnist.load_data()\n",
    "    X_train, X_test = X_train / 255.0, X_test / 255.0\n",
    "    \n",
    "    # Check the shape\n",
    "    print(\"X_train shape = \", X_train.shape)\n",
    "    print(\"X_test  shape = \", X_test.shape)\n",
    "    \n",
    "    print(\"y_train shape = \", y_train.shape)\n",
    "    print(\"y_test  shape = \", y_test.shape)\n",
    "\n",
    "    return [X_train, X_test, y_train, y_test]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing ./trainer/model.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile ./trainer/model.py\n",
    "\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "from tensorflow.keras import backend as K\n",
    "\n",
    "def tf_model(input_shape, model_depth: int = 1, dropout_rate: float = 0.02):\n",
    "    \"\"\"Creates the keras model used by task to train the model.\"\"\"\n",
    "    from tensorflow.keras.models import Sequential\n",
    "    from tensorflow.keras.layers import Dense, Flatten, Dropout\n",
    "    x_dim = input_shape[0]\n",
    "    y_dim = input_shape[1]\n",
    "\n",
    "    model = Sequential()\n",
    "    model.add(Flatten(input_shape=input_shape))\n",
    "    for i in range(0,model_depth):\n",
    "        nUnits = x_dim*y_dim-(i+1)*((x_dim*y_dim-128)//model_depth)\n",
    "        model.add(Dense(nUnits, activation='relu'))\n",
    "        model.add(Dropout(dropout_rate))\n",
    "    model.add(Dense(10, activation='softmax'))\n",
    "\n",
    "    print(model.summary())\n",
    "\n",
    "    return model\n",
    "\n",
    "def custom_loss(y_true, y_pred):\n",
    "    custom_loss = tf.keras.losses.sparse_categorical_crossentropy(y_true, y_pred)\n",
    "    return custom_loss\n",
    "\n",
    "def custom_metric(y_true, y_pred):\n",
    "    y_true = K.flatten(y_true)\n",
    "    y_pred = K.flatten(y_pred)\n",
    "    custom_metric = np.mean(tf.math.squared_difference(y_true, y_pred))\n",
    "    custom_metric = tf.reduce_sum(custom_metric)\n",
    "    return custom_metric\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing ./trainer/train.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile ./trainer/train.py\n",
    "\n",
    "# python3\n",
    "# ==============================================================================\n",
    "# Copyright 2020 Google LLC. This software is provided as-is, without warranty\n",
    "# or representation for any use or purpose. Your use of it is subject to your\n",
    "# agreement with Google.\n",
    "# ==============================================================================\n",
    "\n",
    "import os\n",
    "import json\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import datetime as datetime\n",
    "from pytz import timezone\n",
    "import hypertune\n",
    "import argparse\n",
    "from trainer import model\n",
    "from trainer import inputs\n",
    "\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "import os\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3' \n",
    "#0 = all messages are logged (default behavior)\n",
    "#1 = INFO messages are not printed\n",
    "#2 = INFO and WARNING messages are not printed\n",
    "#3 = INFO, WARNING, and ERROR messages are not printed\n",
    "\n",
    "def parse_arguments():\n",
    "    \"\"\"Argument parser.\n",
    "    Returns:\n",
    "      Dictionary of arguments.\n",
    "    \"\"\"\n",
    "    parser = argparse.ArgumentParser()\n",
    "    parser.add_argument('--model_depth', default=1, type=int,\n",
    "                        help='Hyperparameter: depth of the model')\n",
    "    parser.add_argument('--dropout_rate', default=0.02, type=float, \n",
    "                        help='Hyperparameter: Drop out rate')\n",
    "    parser.add_argument('--learning_rate', default=0.0001, type=float, \n",
    "                        help='Hyperparameter: initial learning rate')\n",
    "    parser.add_argument('--epochs', default=2, type=int,\n",
    "                        help='Hyperparameter: epoch.')\n",
    "    parser.add_argument('--batch_size', default=4, type=int, \n",
    "                        help='batch size of the deep network')\n",
    "    parser.add_argument('--model_dir', default=\"\",\n",
    "                        help='Directory to store model checkpoints and logs.')\n",
    "    parser.add_argument('--verbosity',choices=['DEBUG', 'ERROR', 'FATAL', 'INFO', 'WARN'],\n",
    "                        default='FATAL')\n",
    "    args, _ = parser.parse_known_args()\n",
    "    return args\n",
    "\n",
    "def get_callbacks(args, early_stop_patience: int = 3):\n",
    "    \"\"\"Creates Keras callbacks for model training.\"\"\"\n",
    "\n",
    "    # Get trialId\n",
    "    trialId = json.loads(os.environ.get(\"TF_CONFIG\", \"{}\")).get(\"task\", {}).get(\"trial\", \"\")\n",
    "    if trialId == '':\n",
    "        trialId = '0'\n",
    "    print(\"trialId=\", trialId)\n",
    "\n",
    "    curTime = datetime.datetime.now(timezone('US/Pacific')).strftime('%H%M%S')\n",
    "    \n",
    "    # Modify model_dir paths to include trialId\n",
    "    model_dir = args.model_dir + \"/checkpoints/cp-\"+curTime+\"-\"+trialId+\"-{val_accuracy:.4f}\"\n",
    "    log_dir   = args.model_dir + \"/log_dir\"\n",
    "\n",
    "    tensorboard_cb = tf.keras.callbacks.TensorBoard(log_dir, histogram_freq=1)\n",
    "    checkpoint_cb  = tf.keras.callbacks.ModelCheckpoint(model_dir, monitor='val_accuracy', mode='max', \n",
    "                                                        verbose=0, save_best_only=True,\n",
    "                                                        save_weights_only=False)\n",
    "    earlystop_cb   = tf.keras.callbacks.EarlyStopping(monitor='val_accuracy', patience=3)\n",
    "\n",
    "    return [checkpoint_cb, tensorboard_cb, earlystop_cb]\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "\n",
    "    # ---------------------------------------\n",
    "    # Parse Arguments\n",
    "    # ---------------------------------------\n",
    "    args = parse_arguments()\n",
    "    #args.model_dir = MODEL_DIR + datetime.datetime.now(timezone('US/Pacific')).strftime('/model_%m%d%Y_%H%M')\n",
    "    print(args)\n",
    "\n",
    "    # ---------------------------------------\n",
    "    # Input Data & Preprocessing\n",
    "    # ---------------------------------------\n",
    "    print(\"Input and pre-process data ...\")\n",
    "    # Extract train_seismic, train_label\n",
    "    train_test_data = inputs.load_data()\n",
    "    X_train = train_test_data[0]\n",
    "    X_test  = train_test_data[1]\n",
    "    y_train = train_test_data[2]\n",
    "    y_test  = train_test_data[3]\n",
    "\n",
    "    # ---------------------------------------\n",
    "    # Train model\n",
    "    # ---------------------------------------\n",
    "    print(\"Creating model ...\")\n",
    "    input_shape = (X_train.shape)[1:]\n",
    "    tf_model = model.tf_model(input_shape, model_depth=args.model_depth,\n",
    "                              dropout_rate=args.dropout_rate)\n",
    "    \n",
    "    optimizer = tf.keras.optimizers.Adam(learning_rate=args.learning_rate)\n",
    "    tf_model.compile(optimizer=optimizer,\n",
    "                     loss='sparse_categorical_crossentropy',\n",
    "                     metrics=['accuracy'])\n",
    "    \n",
    "    print(\"Fitting model ...\")\n",
    "    callbacks = get_callbacks(args, 3)\n",
    "    histy = tf_model.fit(x=X_train, y=y_train,\n",
    "                         epochs=args.epochs,\n",
    "                         batch_size=args.batch_size,\n",
    "                         validation_data=(X_test, y_test),\n",
    "                         callbacks=callbacks)\n",
    "    # TBD save history for visualization\n",
    "\n",
    "    final_epoch_accuracy = histy.history['accuracy'][-1]\n",
    "    final_epoch_count = len(histy.history['accuracy'])\n",
    "\n",
    "    print('final_epoch_accuracy = %.6f' % final_epoch_accuracy)\n",
    "    print('final_epoch_count = %2d' % final_epoch_count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the tf directory and load the trainer files in it\n",
    "!cp ./trainer/train.py ./trainer/train_hpt.py\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Appending to ./trainer/train_hpt.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile -a ./trainer/train_hpt.py\n",
    "\n",
    "    \"\"\"This method updates a CAIP HPTuning Job with a final metric for the job.\n",
    "    In TF2.X the user must either use hypertune or a custom callback with\n",
    "    tf.summary.scalar to update CAIP HP Tuning jobs. This function uses\n",
    "    hypertune, which appears to be the preferred solution. Hypertune also works\n",
    "    with containers, without code change.\n",
    "    Args:\n",
    "        metric_tag: The metric being optimized.  This MUST MATCH the\n",
    "          hyperparameterMetricTag specificed in the hyperparameter tuning yaml.\n",
    "        metric_value: The value to report at the end of model training.\n",
    "        global_step: An int value to specify the number of trainin steps completed\n",
    "          at the time the metric was reported.\n",
    "    \"\"\"\n",
    "\n",
    "    hpt = hypertune.HyperTune()\n",
    "    hpt.report_hyperparameter_tuning_metric(\n",
    "        hyperparameter_metric_tag='accuracy',\n",
    "        metric_value=final_epoch_accuracy,\n",
    "        global_step=final_epoch_count\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found existing installation: setuptools 47.1.0\n",
      "Uninstalling setuptools-47.1.0:\n",
      "  Successfully uninstalled setuptools-47.1.0\n",
      "Collecting wheel\n",
      "  Using cached wheel-0.36.2-py2.py3-none-any.whl (35 kB)\n",
      "Collecting setuptools>=40.8.0\n",
      "  Using cached setuptools-57.2.0-py3-none-any.whl (818 kB)\n",
      "Installing collected packages: wheel, setuptools\n",
      "Successfully installed setuptools-57.2.0 wheel-0.36.2\n",
      "\u001b[33mWARNING: You are using pip version 20.1.1; however, version 21.1.3 is available.\n",
      "You should consider upgrading via the '/tmp/build-env-c6s4n9_c/bin/python -m pip install --upgrade pip' command.\u001b[0m\n",
      "running egg_info\n",
      "creating trainer.egg-info\n",
      "writing trainer.egg-info/PKG-INFO\n",
      "writing dependency_links to trainer.egg-info/dependency_links.txt\n",
      "writing requirements to trainer.egg-info/requires.txt\n",
      "writing top-level names to trainer.egg-info/top_level.txt\n",
      "writing manifest file 'trainer.egg-info/SOURCES.txt'\n",
      "reading manifest file 'trainer.egg-info/SOURCES.txt'\n",
      "writing manifest file 'trainer.egg-info/SOURCES.txt'\n",
      "running sdist\n",
      "running egg_info\n",
      "writing trainer.egg-info/PKG-INFO\n",
      "writing dependency_links to trainer.egg-info/dependency_links.txt\n",
      "writing requirements to trainer.egg-info/requires.txt\n",
      "writing top-level names to trainer.egg-info/top_level.txt\n",
      "reading manifest file 'trainer.egg-info/SOURCES.txt'\n",
      "writing manifest file 'trainer.egg-info/SOURCES.txt'\n",
      "warning: sdist: standard file not found: should have one of README, README.rst, README.txt, README.md\n",
      "\n",
      "running check\n",
      "warning: check: missing required meta-data: url\n",
      "\n",
      "warning: check: missing meta-data: either (author and author_email) or (maintainer and maintainer_email) must be supplied\n",
      "\n",
      "creating trainer-0.1\n",
      "creating trainer-0.1/trainer\n",
      "creating trainer-0.1/trainer.egg-info\n",
      "copying files to trainer-0.1...\n",
      "copying setup.py -> trainer-0.1\n",
      "copying trainer/__init__.py -> trainer-0.1/trainer\n",
      "copying trainer/inputs.py -> trainer-0.1/trainer\n",
      "copying trainer/model.py -> trainer-0.1/trainer\n",
      "copying trainer/train.py -> trainer-0.1/trainer\n",
      "copying trainer/train_hpt.py -> trainer-0.1/trainer\n",
      "copying trainer.egg-info/PKG-INFO -> trainer-0.1/trainer.egg-info\n",
      "copying trainer.egg-info/SOURCES.txt -> trainer-0.1/trainer.egg-info\n",
      "copying trainer.egg-info/dependency_links.txt -> trainer-0.1/trainer.egg-info\n",
      "copying trainer.egg-info/requires.txt -> trainer-0.1/trainer.egg-info\n",
      "copying trainer.egg-info/top_level.txt -> trainer-0.1/trainer.egg-info\n",
      "Writing trainer-0.1/setup.cfg\n",
      "Creating tar archive\n",
      "removing 'trainer-0.1' (and everything under it)\n",
      "Found existing installation: setuptools 47.1.0\n",
      "Uninstalling setuptools-47.1.0:\n",
      "  Successfully uninstalled setuptools-47.1.0\n",
      "Collecting wheel\n",
      "  Using cached wheel-0.36.2-py2.py3-none-any.whl (35 kB)\n",
      "Collecting setuptools>=40.8.0\n",
      "  Using cached setuptools-57.2.0-py3-none-any.whl (818 kB)\n",
      "Installing collected packages: wheel, setuptools\n",
      "Successfully installed setuptools-57.2.0 wheel-0.36.2\n",
      "\u001b[33mWARNING: You are using pip version 20.1.1; however, version 21.1.3 is available.\n",
      "You should consider upgrading via the '/tmp/build-env-4xulby0p/bin/python -m pip install --upgrade pip' command.\u001b[0m\n",
      "running egg_info\n",
      "writing trainer.egg-info/PKG-INFO\n",
      "writing dependency_links to trainer.egg-info/dependency_links.txt\n",
      "writing requirements to trainer.egg-info/requires.txt\n",
      "writing top-level names to trainer.egg-info/top_level.txt\n",
      "reading manifest file 'trainer.egg-info/SOURCES.txt'\n",
      "writing manifest file 'trainer.egg-info/SOURCES.txt'\n",
      "Requirement already satisfied: wheel in /tmp/build-env-4xulby0p/lib/python3.7/site-packages (from -r /tmp/build-reqs-isflxubc.txt (line 1)) (0.36.2)\n",
      "\u001b[33mWARNING: You are using pip version 20.1.1; however, version 21.1.3 is available.\n",
      "You should consider upgrading via the '/tmp/build-env-4xulby0p/bin/python -m pip install --upgrade pip' command.\u001b[0m\n",
      "running bdist_wheel\n",
      "running build\n",
      "running build_py\n",
      "creating build\n",
      "creating build/lib\n",
      "creating build/lib/trainer\n",
      "copying trainer/__init__.py -> build/lib/trainer\n",
      "copying trainer/inputs.py -> build/lib/trainer\n",
      "copying trainer/train_hpt.py -> build/lib/trainer\n",
      "copying trainer/model.py -> build/lib/trainer\n",
      "copying trainer/train.py -> build/lib/trainer\n",
      "running egg_info\n",
      "writing trainer.egg-info/PKG-INFO\n",
      "writing dependency_links to trainer.egg-info/dependency_links.txt\n",
      "writing requirements to trainer.egg-info/requires.txt\n",
      "writing top-level names to trainer.egg-info/top_level.txt\n",
      "reading manifest file 'trainer.egg-info/SOURCES.txt'\n",
      "writing manifest file 'trainer.egg-info/SOURCES.txt'\n",
      "installing to build/bdist.linux-x86_64/wheel\n",
      "running install\n",
      "running install_lib\n",
      "creating build/bdist.linux-x86_64\n",
      "creating build/bdist.linux-x86_64/wheel\n",
      "creating build/bdist.linux-x86_64/wheel/trainer\n",
      "copying build/lib/trainer/__init__.py -> build/bdist.linux-x86_64/wheel/trainer\n",
      "copying build/lib/trainer/inputs.py -> build/bdist.linux-x86_64/wheel/trainer\n",
      "copying build/lib/trainer/train_hpt.py -> build/bdist.linux-x86_64/wheel/trainer\n",
      "copying build/lib/trainer/model.py -> build/bdist.linux-x86_64/wheel/trainer\n",
      "copying build/lib/trainer/train.py -> build/bdist.linux-x86_64/wheel/trainer\n",
      "running install_egg_info\n",
      "Copying trainer.egg-info to build/bdist.linux-x86_64/wheel/trainer-0.1-py3.7.egg-info\n",
      "running install_scripts\n",
      "creating build/bdist.linux-x86_64/wheel/trainer-0.1.dist-info/WHEEL\n",
      "creating '/home/jupyter/vapit/ai-platform-tf/Vertex/dist/tmp66987nwd/trainer-0.1-py3-none-any.whl' and adding 'build/bdist.linux-x86_64/wheel' to it\n",
      "adding 'trainer/__init__.py'\n",
      "adding 'trainer/inputs.py'\n",
      "adding 'trainer/model.py'\n",
      "adding 'trainer/train.py'\n",
      "adding 'trainer/train_hpt.py'\n",
      "adding 'trainer-0.1.dist-info/METADATA'\n",
      "adding 'trainer-0.1.dist-info/WHEEL'\n",
      "adding 'trainer-0.1.dist-info/top_level.txt'\n",
      "adding 'trainer-0.1.dist-info/RECORD'\n",
      "removing build/bdist.linux-x86_64/wheel\n",
      "\u001b[0mCopying file://./dist/trainer-0.1.tar.gz [Content-Type=application/x-tar]...\n",
      "/ [1 files][  3.7 KiB/  3.7 KiB]                                                \n",
      "Operation completed over 1 objects/3.7 KiB.                                      \n"
     ]
    }
   ],
   "source": [
    "!cd /home/jupyter/vapit/ai-platform-tf/Vertex\n",
    "!python3 -m build\n",
    "!gsutil cp ./dist/trainer-0.1.tar.gz $PACKAGE_URIS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "------\n",
    "### Training with Google Vertex AI "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the full article, please visit: https://cloud.google.com/vertex-ai/docs\n",
    "\n",
    "Where Vertex AI fits in the ML workflow \\\n",
    "The diagram below gives a high-level overview of the stages in an ML workflow. The blue-filled boxes indicate where Vertex AI provides managed services and APIs:\n",
    "\n",
    "<img src=\"img/ml-workflow.svg\" alt=\"Drawing\">\n",
    "\n",
    "As the diagram indicates, you can use Vertex AI to manage the following stages in the ML workflow:\n",
    "\n",
    "- Train an ML model on your data:\n",
    " - Train model\n",
    " - Evaluate model accuracy\n",
    " - Tune hyperparameters\n",
    " \n",
    " \n",
    "- Deploy your trained model.\n",
    "\n",
    "- Send prediction requests to your model:\n",
    " - Online prediction\n",
    " - Batch prediction (for TensorFlow only)\n",
    " \n",
    " \n",
    "- Monitor the predictions on an ongoing basis.\n",
    "\n",
    "- Manage your models and model versions.\n",
    "\n",
    "- For the latest list, see \n",
    "  - Pre-built containers for training: https://cloud.google.com/vertex-ai/docs/training/pre-built-containers\n",
    "    and \n",
    "  - Pre-built containers for prediction: https://cloud.google.com/vertex-ai/docs/predictions/pre-built-containers\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Train at local\n",
    "\n",
    "Before submitting training jobs to Cloud AI Platform, you can test your train.py code in the local environment. You can test by running your python script in command line, but another and maybe better choice is to use `gcloud ai-platform local train` command. The latter method could make sure your your entire python package are ready to be submitted to the remote VMs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train shape =  (60000, 28, 28)\n",
      "X_test  shape =  (10000, 28, 28)\n",
      "y_train shape =  (60000,)\n",
      "y_test  shape =  (10000,)\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 576x576 with 16 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "from trainer import inputs\n",
    "train_test_data = inputs.load_data()\n",
    "X_test  = train_test_data[1]\n",
    "plot_images(X_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MODEL_DEPTH =  2\n",
      "DROPOUT_RATE = 0.0100\n",
      "LEARNING_RATE = 0.000050\n",
      "EPOCHS =  1\n",
      "BATCH_SIZE = 32\n",
      "MODEL_DIR = gs://vapit_data/tf_models/models/model_07152021_2018\n",
      "2021-07-16 03:18:42.989000: W tensorflow/stream_executor/platform/default/dso_loader.cc:55] Could not load dynamic library 'libnvinfer.so.6'; dlerror: libnvinfer.so.6: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/local/cuda/lib64:/usr/local/nccl2/lib:/usr/local/cuda/extras/CUPTI/lib64\n",
      "2021-07-16 03:18:42.989108: W tensorflow/stream_executor/platform/default/dso_loader.cc:55] Could not load dynamic library 'libnvinfer_plugin.so.6'; dlerror: libnvinfer_plugin.so.6: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/local/cuda/lib64:/usr/local/nccl2/lib:/usr/local/cuda/extras/CUPTI/lib64\n",
      "2021-07-16 03:18:42.989127: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:30] Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.\n",
      "Namespace(batch_size=32, dropout_rate=0.01, epochs=1, learning_rate=5e-05, model_depth=2, model_dir='gs://vapit_data/tf_models/models/model_07152021_2018', verbosity='FATAL')\n",
      "Input and pre-process data ...\n",
      "X_train shape =  (60000, 28, 28)\n",
      "X_test  shape =  (10000, 28, 28)\n",
      "y_train shape =  (60000,)\n",
      "y_test  shape =  (10000,)\n",
      "Creating model ...\n",
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "flatten (Flatten)            (None, 784)               0         \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 456)               357960    \n",
      "_________________________________________________________________\n",
      "dropout (Dropout)            (None, 456)               0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 128)               58496     \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 10)                1290      \n",
      "=================================================================\n",
      "Total params: 417,746\n",
      "Trainable params: 417,746\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "Fitting model ...\n",
      "trialId= 0\n",
      "Train on 60000 samples, validate on 10000 samples\n",
      "   32/60000 [..............................] - ETA: 9:45 - loss: 2.3373 - accuracy: 0.1562WARNING:tensorflow:Method (on_train_batch_end) is slow compared to the batch update (0.829102). Check your callbacks.\n",
      "59904/60000 [============================>.] - ETA: 0s - loss: 0.5412 - accuracy: 0.8657WARNING:tensorflow:From /home/jupyter/.local/lib/python3.7/site-packages/tensorflow_core/python/ops/resource_variable_ops.py:1786: calling BaseResourceVariable.__init__ (from tensorflow.python.ops.resource_variable_ops) with constraint is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "If using Keras pass *_constraint arguments to layers.\n",
      "60000/60000 [==============================] - 13s 224us/sample - loss: 0.5408 - accuracy: 0.8657 - val_loss: 0.2515 - val_accuracy: 0.9292\n",
      "final_epoch_accuracy = 0.865717\n",
      "final_epoch_count =  1\n",
      "CPU times: user 462 ms, sys: 139 ms, total: 601 ms\n",
      "Wall time: 18.6 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# Run the training manually\n",
    "# Training parameters\n",
    "from datetime import datetime\n",
    "from pytz import timezone\n",
    "\n",
    "MODEL_DEPTH = 2\n",
    "DROPOUT_RATE = 0.01\n",
    "LEARNING_RATE = 0.00005\n",
    "EPOCHS = 1\n",
    "BATCH_SIZE = 32\n",
    "\n",
    "MODEL_DIR_PYTH = 'gs://{}/{}/models/{}'.format(\n",
    "    BUCKET_NAME,\n",
    "    FOLDER_NAME,\n",
    "    datetime.now(timezone('US/Pacific')).strftime('model_%m%d%Y_%H%M')\n",
    "    )\n",
    "\n",
    "print('MODEL_DEPTH = %2d' % MODEL_DEPTH)\n",
    "print('DROPOUT_RATE = %.4f' % DROPOUT_RATE)\n",
    "print('LEARNING_RATE = %.6f' % LEARNING_RATE)\n",
    "print('EPOCHS = %2d' % EPOCHS)\n",
    "print('BATCH_SIZE = %2d' % BATCH_SIZE)\n",
    "print(\"MODEL_DIR =\", MODEL_DIR_PYTH)\n",
    "\n",
    "# Run training\n",
    "! python3 -m trainer.train \\\n",
    "    --model_depth=$MODEL_DEPTH \\\n",
    "    --dropout_rate=$DROPOUT_RATE \\\n",
    "    --learning_rate=$LEARNING_RATE \\\n",
    "    --epochs=$EPOCHS \\\n",
    "    --batch_size=$BATCH_SIZE \\\n",
    "    --model_dir=$MODEL_DIR_PYTH \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "------\n",
    "### Hyperparameter Tuning\n",
    "\n",
    "To use hyperparameter tuning in your training job you must perform the following steps:\n",
    "\n",
    "- Specify the hyperparameter tuning configuration for your training job by including a HyperparameterSpec in your TrainingInput object.\n",
    "\n",
    "- Include the following code in your training application:\n",
    "\n",
    " - Parse the command-line arguments representing the hyperparameters you want to tune, and use the values to set the hyperparameters for your training trial.\n",
    " - Add your hyperparameter metric to the summary for your graph.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "JOB_NAME_HPT =  tensorflow_train_cchatterjee_071521_2019_hpt\n",
      "JOB_DIR =  gs://vapit_data/tf_models/jobdir\n",
      "MODEL_DIR_HPT =  gs://vapit_data/tf_models/models/model_07152021_2019\n"
     ]
    }
   ],
   "source": [
    "# Google Vertex AI requires each job to have unique name, \n",
    "# Therefore, we use prefix + timestamp to form job names.\n",
    "JOBNAME_HPT = 'tensorflow_train_{}_{}_hpt'.format(\n",
    "    USER,\n",
    "    datetime.now(timezone(TIMEZONE)).strftime(\"%m%d%y_%H%M\")\n",
    "    ) # define unique job name\n",
    "\n",
    "# We use the job names as folder names to store outputs.\n",
    "MODEL_DIR_HPT = 'gs://{}/{}/models/{}'.format(\n",
    "    BUCKET_NAME,\n",
    "    FOLDER_NAME,\n",
    "    datetime.now(timezone('US/Pacific')).strftime('model_%m%d%Y_%H%M')\n",
    "    )\n",
    "JOB_DIR = 'gs://{}/{}/jobdir'.format(\n",
    "    BUCKET_NAME,\n",
    "    FOLDER_NAME\n",
    "    )\n",
    "\n",
    "print(\"JOB_NAME_HPT = \", JOBNAME_HPT)\n",
    "print(\"JOB_DIR = \", JOB_DIR)\n",
    "print(\"MODEL_DIR_HPT = \", MODEL_DIR_HPT)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Submit the hyperparameter job to vertex AI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "response: name: \"projects/901951554789/locations/us-central1/hyperparameterTuningJobs/9142434786910928896\"\n",
      "display_name: \"tensorflow_train_cchatterjee_071521_2019_hpt\"\n",
      "study_spec {\n",
      "  metrics {\n",
      "    metric_id: \"accuracy\"\n",
      "    goal: MAXIMIZE\n",
      "  }\n",
      "  parameters {\n",
      "    parameter_id: \"model_depth\"\n",
      "    integer_value_spec {\n",
      "      min_value: 1\n",
      "      max_value: 5\n",
      "    }\n",
      "    scale_type: UNIT_LINEAR_SCALE\n",
      "  }\n",
      "  parameters {\n",
      "    parameter_id: \"epochs\"\n",
      "    integer_value_spec {\n",
      "      min_value: 1\n",
      "      max_value: 4\n",
      "    }\n",
      "    scale_type: UNIT_LINEAR_SCALE\n",
      "  }\n",
      "}\n",
      "max_trial_count: 4\n",
      "parallel_trial_count: 2\n",
      "trial_job_spec {\n",
      "  worker_pool_specs {\n",
      "    machine_spec {\n",
      "      machine_type: \"n1-standard-4\"\n",
      "    }\n",
      "    replica_count: 1\n",
      "    disk_spec {\n",
      "      boot_disk_type: \"pd-ssd\"\n",
      "      boot_disk_size_gb: 100\n",
      "    }\n",
      "    python_package_spec {\n",
      "      executor_image_uri: \"us-docker.pkg.dev/vertex-ai/training/tf-cpu.2-2:latest\"\n",
      "      package_uris: \"gs://vapit_data/trainer/tensorflow/trainer-0.1.tar.gz\"\n",
      "      python_module: \"trainer.train_hpt\"\n",
      "      args: \"--job-dir\"\n",
      "      args: \"gs://vapit_data/tf_models/jobdir\"\n",
      "      args: \"--model_dir\"\n",
      "      args: \"gs://vapit_data/tf_models/models/model_07152021_2019\"\n",
      "      args: \"--dropout_rate\"\n",
      "      args: \"0.01\"\n",
      "      args: \"--learning_rate\"\n",
      "      args: \"5e-05\"\n",
      "      args: \"--batch_size\"\n",
      "      args: \"32\"\n",
      "    }\n",
      "  }\n",
      "}\n",
      "state: JOB_STATE_PENDING\n",
      "create_time {\n",
      "  seconds: 1626405552\n",
      "  nanos: 302214000\n",
      "}\n",
      "update_time {\n",
      "  seconds: 1626405552\n",
      "  nanos: 302214000\n",
      "}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "executor_image_uri = 'us-docker.pkg.dev/vertex-ai/training/tf-cpu.2-2:latest'\n",
    "python_module =  \"trainer.train_hpt\"\n",
    "api_endpoint = \"{}-aiplatform.googleapis.com\".format(REGION)\n",
    "machine_type = \"n1-standard-4\"\n",
    "\n",
    "# The AI Platform services require regional API endpoints.\n",
    "client_options = {\"api_endpoint\": api_endpoint}\n",
    "# Initialize client that will be used to create and send requests.\n",
    "# This client only needs to be created once, and can be reused for multiple requests.\n",
    "client = aiplatform.gapic.JobServiceClient(client_options=client_options)\n",
    "\n",
    "# study_spec\n",
    "metric = {\n",
    "    \"metric_id\": \"accuracy\",\n",
    "    \"goal\": aiplatform.gapic.StudySpec.MetricSpec.GoalType.MAXIMIZE,\n",
    "}\n",
    "\n",
    "depth = {\n",
    "        \"parameter_id\": \"model_depth\",\n",
    "        \"integer_value_spec\": {\"min_value\": 1, \"max_value\": 5},\n",
    "        \"scale_type\": aiplatform.gapic.StudySpec.ParameterSpec.ScaleType.UNIT_LINEAR_SCALE,\n",
    "}\n",
    "# dropout_rate = {\n",
    "#         \"parameter_id\": \"dropout_rate\",\n",
    "#         \"double_value_spec\": {\"min_value\": 0.001, \"max_value\": 0.1},\n",
    "#         \"scale_type\": aiplatform.gapic.StudySpec.ParameterSpec.ScaleType.UNIT_LOG_SCALE,\n",
    "# }\n",
    "# learning_rate = {\n",
    "#         \"parameter_id\": \"learning_rate\",\n",
    "#         \"double_value_spec\": {\"min_value\": 0.00001, \"max_value\": 0.01},\n",
    "#         \"scale_type\": aiplatform.gapic.StudySpec.ParameterSpec.ScaleType.UNIT_LOG_SCALE,\n",
    "# }\n",
    "# batch_size = {\n",
    "#         \"parameter_id\": \"batch_size\",\n",
    "#         \"integer_value_spec\": {\"min_value\": 1, \"max_value\": 16},\n",
    "#         \"scale_type\": aiplatform.gapic.StudySpec.ParameterSpec.ScaleType.UNIT_LINEAR_SCALE,\n",
    "# }\n",
    "epochs = {\n",
    "        \"parameter_id\": \"epochs\",\n",
    "        \"integer_value_spec\": {\"min_value\": 1, \"max_value\": 4},\n",
    "        \"scale_type\": aiplatform.gapic.StudySpec.ParameterSpec.ScaleType.UNIT_LINEAR_SCALE,\n",
    "}\n",
    "\n",
    "# trial_job_spec\n",
    "machine_spec = {\n",
    "    \"machine_type\": machine_type,\n",
    "}\n",
    "worker_pool_spec = {\n",
    "    \"machine_spec\": machine_spec,\n",
    "    \"replica_count\": 1,\n",
    "    \"python_package_spec\": {\n",
    "        \"executor_image_uri\": executor_image_uri,\n",
    "        \"package_uris\": [PACKAGE_URIS],\n",
    "        \"python_module\": python_module,\n",
    "        \"args\": [\n",
    "            '--job-dir',\n",
    "            JOB_DIR,\n",
    "            '--model_dir',\n",
    "            MODEL_DIR_HPT,\n",
    "            '--dropout_rate',\n",
    "            str(DROPOUT_RATE),\n",
    "            '--learning_rate',\n",
    "            str(LEARNING_RATE),\n",
    "            '--batch_size',\n",
    "            str(BATCH_SIZE),\n",
    "        ],\n",
    "    },\n",
    "}\n",
    "\n",
    "# hyperparameter_tuning_job\n",
    "hyperparameter_tuning_job = {\n",
    "    \"display_name\": JOBNAME_HPT,\n",
    "    \"max_trial_count\": 4,\n",
    "    \"parallel_trial_count\": 2,\n",
    "    \"study_spec\": {\n",
    "        \"metrics\": [metric],\n",
    "        \"parameters\": [depth, epochs],\n",
    "#         \"algorithm\": aiplatform.gapic.StudySpec.Algorithm.RANDOM_SEARCH,\n",
    "    },\n",
    "    \"trial_job_spec\": {\"worker_pool_specs\": [worker_pool_spec]},\n",
    "}\n",
    "parent = f\"projects/{PROJECT}/locations/{REGION}\"\n",
    "response = client.create_hyperparameter_tuning_job(\n",
    "    parent=parent, hyperparameter_tuning_job=hyperparameter_tuning_job\n",
    ")\n",
    "print(\"response:\", response)\n",
    "job_name_hpt = response.name.split('/')[-1]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Check the status of Long Running Operation (LRO) with Google API Client\n",
    "\n",
    "Send an API request to Vertex AI to get the detailed information. The most interesting piece of information is the hyperparameter values in the trial with best performance metric."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Job status =  JobState.JOB_STATE_SUCCEEDED\n",
      "response: name: \"projects/901951554789/locations/us-central1/hyperparameterTuningJobs/9142434786910928896\"\n",
      "display_name: \"tensorflow_train_cchatterjee_071521_2019_hpt\"\n",
      "study_spec {\n",
      "  metrics {\n",
      "    metric_id: \"accuracy\"\n",
      "    goal: MAXIMIZE\n",
      "  }\n",
      "  parameters {\n",
      "    parameter_id: \"model_depth\"\n",
      "    integer_value_spec {\n",
      "      min_value: 1\n",
      "      max_value: 5\n",
      "    }\n",
      "    scale_type: UNIT_LINEAR_SCALE\n",
      "  }\n",
      "  parameters {\n",
      "    parameter_id: \"epochs\"\n",
      "    integer_value_spec {\n",
      "      min_value: 1\n",
      "      max_value: 4\n",
      "    }\n",
      "    scale_type: UNIT_LINEAR_SCALE\n",
      "  }\n",
      "}\n",
      "max_trial_count: 4\n",
      "parallel_trial_count: 2\n",
      "trial_job_spec {\n",
      "  worker_pool_specs {\n",
      "    machine_spec {\n",
      "      machine_type: \"n1-standard-4\"\n",
      "    }\n",
      "    replica_count: 1\n",
      "    disk_spec {\n",
      "      boot_disk_type: \"pd-ssd\"\n",
      "      boot_disk_size_gb: 100\n",
      "    }\n",
      "    python_package_spec {\n",
      "      executor_image_uri: \"us-docker.pkg.dev/vertex-ai/training/tf-cpu.2-2:latest\"\n",
      "      package_uris: \"gs://vapit_data/trainer/tensorflow/trainer-0.1.tar.gz\"\n",
      "      python_module: \"trainer.train_hpt\"\n",
      "      args: \"--job-dir\"\n",
      "      args: \"gs://vapit_data/tf_models/jobdir\"\n",
      "      args: \"--model_dir\"\n",
      "      args: \"gs://vapit_data/tf_models/models/model_07152021_2019\"\n",
      "      args: \"--dropout_rate\"\n",
      "      args: \"0.01\"\n",
      "      args: \"--learning_rate\"\n",
      "      args: \"5e-05\"\n",
      "      args: \"--batch_size\"\n",
      "      args: \"32\"\n",
      "    }\n",
      "  }\n",
      "}\n",
      "trials {\n",
      "  id: \"1\"\n",
      "  state: SUCCEEDED\n",
      "  parameters {\n",
      "    parameter_id: \"epochs\"\n",
      "    value {\n",
      "      number_value: 3.0\n",
      "    }\n",
      "  }\n",
      "  parameters {\n",
      "    parameter_id: \"model_depth\"\n",
      "    value {\n",
      "      number_value: 3.0\n",
      "    }\n",
      "  }\n",
      "  final_measurement {\n",
      "    step_count: 3\n",
      "    metrics {\n",
      "      metric_id: \"accuracy\"\n",
      "      value: 0.9619166851043701\n",
      "    }\n",
      "  }\n",
      "  start_time {\n",
      "    seconds: 1626405559\n",
      "    nanos: 275281643\n",
      "  }\n",
      "  end_time {\n",
      "    seconds: 1626406322\n",
      "  }\n",
      "}\n",
      "trials {\n",
      "  id: \"2\"\n",
      "  state: SUCCEEDED\n",
      "  parameters {\n",
      "    parameter_id: \"epochs\"\n",
      "    value {\n",
      "      number_value: 4.0\n",
      "    }\n",
      "  }\n",
      "  parameters {\n",
      "    parameter_id: \"model_depth\"\n",
      "    value {\n",
      "      number_value: 3.0\n",
      "    }\n",
      "  }\n",
      "  final_measurement {\n",
      "    step_count: 4\n",
      "    metrics {\n",
      "      metric_id: \"accuracy\"\n",
      "      value: 0.9707333445549011\n",
      "    }\n",
      "  }\n",
      "  start_time {\n",
      "    seconds: 1626405559\n",
      "    nanos: 275399252\n",
      "  }\n",
      "  end_time {\n",
      "    seconds: 1626406312\n",
      "  }\n",
      "}\n",
      "trials {\n",
      "  id: \"3\"\n",
      "  state: SUCCEEDED\n",
      "  parameters {\n",
      "    parameter_id: \"epochs\"\n",
      "    value {\n",
      "      number_value: 4.0\n",
      "    }\n",
      "  }\n",
      "  parameters {\n",
      "    parameter_id: \"model_depth\"\n",
      "    value {\n",
      "      number_value: 2.0\n",
      "    }\n",
      "  }\n",
      "  final_measurement {\n",
      "    step_count: 4\n",
      "    metrics {\n",
      "      metric_id: \"accuracy\"\n",
      "      value: 0.9585833549499512\n",
      "    }\n",
      "  }\n",
      "  start_time {\n",
      "    seconds: 1626406393\n",
      "    nanos: 677467245\n",
      "  }\n",
      "  end_time {\n",
      "    seconds: 1626407171\n",
      "  }\n",
      "}\n",
      "trials {\n",
      "  id: \"4\"\n",
      "  state: SUCCEEDED\n",
      "  parameters {\n",
      "    parameter_id: \"epochs\"\n",
      "    value {\n",
      "      number_value: 4.0\n",
      "    }\n",
      "  }\n",
      "  parameters {\n",
      "    parameter_id: \"model_depth\"\n",
      "    value {\n",
      "      number_value: 5.0\n",
      "    }\n",
      "  }\n",
      "  final_measurement {\n",
      "    step_count: 4\n",
      "    metrics {\n",
      "      metric_id: \"accuracy\"\n",
      "      value: 0.9766499996185303\n",
      "    }\n",
      "  }\n",
      "  start_time {\n",
      "    seconds: 1626406396\n",
      "    nanos: 359737758\n",
      "  }\n",
      "  end_time {\n",
      "    seconds: 1626406607\n",
      "  }\n",
      "}\n",
      "state: JOB_STATE_SUCCEEDED\n",
      "create_time {\n",
      "  seconds: 1626405552\n",
      "  nanos: 302214000\n",
      "}\n",
      "start_time {\n",
      "  seconds: 1626405554\n",
      "}\n",
      "end_time {\n",
      "  seconds: 1626407252\n",
      "}\n",
      "update_time {\n",
      "  seconds: 1626407272\n",
      "  nanos: 774634000\n",
      "}\n",
      "\n",
      "Job state succeeded.\n"
     ]
    }
   ],
   "source": [
    "client_options = {\"api_endpoint\": api_endpoint}\n",
    "client = aiplatform.gapic.JobServiceClient(client_options=client_options)\n",
    "name = client.hyperparameter_tuning_job_path(\n",
    "    project=PROJECT,\n",
    "    location=REGION,\n",
    "    hyperparameter_tuning_job=job_name_hpt,\n",
    ")\n",
    "response = client.get_hyperparameter_tuning_job(name=name)\n",
    "print(\"Job status = \", response.state)\n",
    "print(\"response:\", response)\n",
    "# print(\"response state: \", str(response.state))\n",
    "if \"JobState.JOB_STATE_SUCCEEDED\" == str(response.state):\n",
    "    print(\"Job state succeeded.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Get the hyperparameters associated with the best metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Metrics Value (larger is better): 0.9619166851043701\n",
      "Metrics Value (larger is better): 0.9707333445549011\n",
      "Metrics Value (larger is better): 0.9585833549499512\n",
      "Metrics Value (larger is better): 0.9766499996185303\n",
      "{'epochs': 4.0, 'model_depth': 5.0}\n"
     ]
    }
   ],
   "source": [
    "max_ind = 0\n",
    "max_val = 0\n",
    "for ind, trials in enumerate(response.trials):\n",
    "    value = trials.final_measurement.metrics[0].value\n",
    "    print(\"Metrics Value (larger is better):\", value)\n",
    "    if value > max_val:\n",
    "        max_val = value\n",
    "        max_ind = ind\n",
    "        \n",
    "param_dict = {}\n",
    "for params in response.trials[max_ind].parameters:\n",
    "    param_dict[params.parameter_id] = params.value\n",
    "\n",
    "print(param_dict)\n",
    "\n",
    "depth=str(int(param_dict['model_depth']))\n",
    "# dropout_rate=str(param_dict['dropout_rate'])\n",
    "# learning_rate=str(param_dict['learning_rate'])\n",
    "# batch_size=str(int(param_dict['batch_size']))\n",
    "epochs=str(int(param_dict['epochs']))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Get the best model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "All Models = \n",
      "gs://vapit_data/tf_models/models/model_07152021_2019/checkpoints/\n",
      "gs://vapit_data/tf_models/models/model_07152021_2019/checkpoints/cp-203037-0-0.9441/\n",
      "gs://vapit_data/tf_models/models/model_07152021_2019/checkpoints/cp-203037-0-0.9582/\n",
      "gs://vapit_data/tf_models/models/model_07152021_2019/checkpoints/cp-203037-0-0.9653/\n",
      "gs://vapit_data/tf_models/models/model_07152021_2019/checkpoints/cp-203037-0-0.9692/\n",
      "gs://vapit_data/tf_models/models/model_07152021_2019/checkpoints/cp-203055-0-0.9408/\n",
      "gs://vapit_data/tf_models/models/model_07152021_2019/checkpoints/cp-203055-0-0.9546/\n",
      "gs://vapit_data/tf_models/models/model_07152021_2019/checkpoints/cp-203055-0-0.9625/\n",
      "gs://vapit_data/tf_models/models/model_07152021_2019/checkpoints/cp-203503-0-0.9470/\n",
      "gs://vapit_data/tf_models/models/model_07152021_2019/checkpoints/cp-203503-0-0.9643/\n",
      "gs://vapit_data/tf_models/models/model_07152021_2019/checkpoints/cp-203503-0-0.9660/\n",
      "gs://vapit_data/tf_models/models/model_07152021_2019/checkpoints/cp-203503-0-0.9740/\n",
      "gs://vapit_data/tf_models/models/model_07152021_2019/checkpoints/cp-204459-0-0.9276/\n",
      "gs://vapit_data/tf_models/models/model_07152021_2019/checkpoints/cp-204459-0-0.9455/\n",
      "gs://vapit_data/tf_models/models/model_07152021_2019/checkpoints/cp-204459-0-0.9542/\n",
      "gs://vapit_data/tf_models/models/model_07152021_2019/checkpoints/cp-204459-0-0.9609/\n",
      "Best Accuracy  from Checkpoints =  0.974\n",
      "Best Model Dir from Checkpoints =  gs://vapit_data/tf_models/models/model_07152021_2019/checkpoints/cp-203503-0-0.9740/\n"
     ]
    }
   ],
   "source": [
    "best_model_dir_hpt = find_best_model_dir(MODEL_DIR_HPT+'/checkpoints', offset=1, maxFlag=1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "------\n",
    "### Training with Tuned Parameters\n",
    "\n",
    "Once your hyperparameter training jobs are done. You can use the optimized combination of hyperparameters from your trials and start a single training job on Cloud AI Platform to train your final model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "JOB_NAME_TRN =  tensorflow_train_cchatterjee_071521_1938\n",
      "JOB_DIR =  gs://vapit_data/tf_models/jobdir\n",
      "MODEL_DIR_TRN =  gs://vapit_data/tf_models/models/model_07152021_1938\n"
     ]
    }
   ],
   "source": [
    "# Google Cloud AI Platform requires each job to have unique name, \n",
    "# Therefore, we use prefix + timestamp to form job names.\n",
    "JOBNAME_TRN = 'tensorflow_train_{}_{}'.format(\n",
    "    USER,\n",
    "    datetime.now(timezone(TIMEZONE)).strftime(\"%m%d%y_%H%M\")\n",
    "    )\n",
    "# We use the job names as folder names to store outputs.\n",
    "MODEL_DIR_TRN = 'gs://{}/{}/models/{}'.format(\n",
    "    BUCKET_NAME,\n",
    "    FOLDER_NAME,\n",
    "    datetime.now(timezone('US/Pacific')).strftime('model_%m%d%Y_%H%M')\n",
    "    )\n",
    "JOB_DIR = 'gs://{}/{}/jobdir'.format(\n",
    "    BUCKET_NAME,\n",
    "    FOLDER_NAME\n",
    "    )\n",
    "\n",
    "# Training parameters\n",
    "MODEL_DEPTH = 3\n",
    "DROPOUT_RATE = 0.02\n",
    "LEARNING_RATE = 0.0001\n",
    "BATCH_SIZE = 32\n",
    "EPOCHS = 2\n",
    "\n",
    "print(\"JOB_NAME_TRN = \", JOBNAME_TRN)\n",
    "print(\"JOB_DIR = \", JOB_DIR)\n",
    "print(\"MODEL_DIR_TRN = \", MODEL_DIR_TRN)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "response: name: \"projects/901951554789/locations/us-central1/customJobs/1913031485074440192\"\n",
      "display_name: \"tensorflow_train_cchatterjee_071521_1938\"\n",
      "job_spec {\n",
      "  worker_pool_specs {\n",
      "    machine_spec {\n",
      "      machine_type: \"n1-standard-4\"\n",
      "    }\n",
      "    replica_count: 1\n",
      "    disk_spec {\n",
      "      boot_disk_type: \"pd-ssd\"\n",
      "      boot_disk_size_gb: 100\n",
      "    }\n",
      "    python_package_spec {\n",
      "      executor_image_uri: \"us-docker.pkg.dev/vertex-ai/training/tf-cpu.2-2:latest\"\n",
      "      package_uris: \"gs://vapit_data/trainer/tensorflow/trainer-0.1.tar.gz\"\n",
      "      python_module: \"trainer.train\"\n",
      "      args: \"--model_depth\"\n",
      "      args: \"3\"\n",
      "      args: \"--dropout_rate\"\n",
      "      args: \"0.02\"\n",
      "      args: \"--learning_rate\"\n",
      "      args: \"0.0001\"\n",
      "      args: \"--batch_size\"\n",
      "      args: \"32\"\n",
      "      args: \"--epochs\"\n",
      "      args: \"2\"\n",
      "      args: \"--job-dir\"\n",
      "      args: \"gs://vapit_data/tf_models/jobdir\"\n",
      "      args: \"--model_dir\"\n",
      "      args: \"gs://vapit_data/tf_models/models/model_07152021_1938\"\n",
      "    }\n",
      "  }\n",
      "}\n",
      "state: JOB_STATE_PENDING\n",
      "create_time {\n",
      "  seconds: 1626403572\n",
      "  nanos: 371331000\n",
      "}\n",
      "update_time {\n",
      "  seconds: 1626403572\n",
      "  nanos: 371331000\n",
      "}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "executor_image_uri = 'us-docker.pkg.dev/vertex-ai/training/tf-cpu.2-2:latest'\n",
    "python_module = \"trainer.train\"\n",
    "api_endpoint = \"{}-aiplatform.googleapis.com\".format(REGION)\n",
    "machine_type = \"n1-standard-4\"\n",
    "        \n",
    "# The AI Platform services require regional API endpoints.\n",
    "client_options = {\"api_endpoint\": api_endpoint}\n",
    "# Initialize client that will be used to create and send requests.\n",
    "# This client only needs to be created once, and can be reused for multiple requests.\n",
    "client = aiplatform.gapic.JobServiceClient(client_options=client_options)\n",
    "custom_job = {\n",
    "    \"display_name\": JOBNAME_TRN,\n",
    "    \"job_spec\": {\n",
    "        \"worker_pool_specs\": [\n",
    "            {\n",
    "                \"machine_spec\": {\n",
    "                    \"machine_type\": machine_type,\n",
    "                },\n",
    "                \"replica_count\": 1,\n",
    "                \"python_package_spec\": {\n",
    "                    \"executor_image_uri\": executor_image_uri,\n",
    "                    \"package_uris\": [PACKAGE_URIS],\n",
    "                    \"python_module\": python_module,\n",
    "                    \"args\": [\n",
    "                         '--job-dir',\n",
    "                        JOB_DIR,\n",
    "                        '--model_dir',\n",
    "                        MODEL_DIR_TRN,\n",
    "                        '--model_depth',\n",
    "                        str(MODEL_DEPTH),\n",
    "                        '--dropout_rate',\n",
    "                        str(DROPOUT_RATE),\n",
    "                        '--learning_rate',\n",
    "                        str(LEARNING_RATE),\n",
    "                        '--batch_size',\n",
    "                        str(BATCH_SIZE),\n",
    "                        '--epochs',\n",
    "                        str(EPOCHS),\n",
    "                   ],\n",
    "                },\n",
    "            }\n",
    "        ]\n",
    "    },\n",
    "}\n",
    "\n",
    "parent = f\"projects/{PROJECT}/locations/{REGION}\"\n",
    "response = client.create_custom_job(parent=parent, custom_job=custom_job)\n",
    "print(\"response:\", response)\n",
    "job_id_trn = response.name.split('/')[-1]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check the training job status"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "JobState.JOB_STATE_SUCCEEDED\n"
     ]
    }
   ],
   "source": [
    "# check the training job status\n",
    "client_options = {\"api_endpoint\": api_endpoint}\n",
    "client = aiplatform.gapic.JobServiceClient(client_options=client_options)\n",
    "name = client.custom_job_path(\n",
    "    project=PROJECT,\n",
    "    location=REGION,\n",
    "    custom_job=job_id_trn,\n",
    ")\n",
    "response = client.get_custom_job(name=name)\n",
    "print(response.state)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Get the best model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "All Models = \n",
      "gs://vapit_data/tf_models/models/model_07152021_1938/checkpoints/\n",
      "gs://vapit_data/tf_models/models/model_07152021_1938/checkpoints/cp-195726-0-0.9523/\n",
      "gs://vapit_data/tf_models/models/model_07152021_1938/checkpoints/cp-195726-0-0.9694/\n",
      "Best Accuracy  from Checkpoints =  0.9694\n",
      "Best Model Dir from Checkpoints =  gs://vapit_data/tf_models/models/model_07152021_1938/checkpoints/cp-195726-0-0.9694/\n"
     ]
    }
   ],
   "source": [
    "best_model_dir_trn = find_best_model_dir(MODEL_DIR_TRN+'/checkpoints', offset=1, maxFlag=1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "--------\n",
    "### Deploy the Model\n",
    "\n",
    "Vertex AI provides tools to upload your trained ML model to the cloud, so that you can send prediction requests to the model.\n",
    "\n",
    "In order to deploy your trained model on Vertex AI, you must save your trained model using the tools provided by your machine learning framework. This involves serializing the information that represents your trained model into a file which you can deploy for prediction in the cloud.\n",
    "\n",
    "Then you upload the saved model to a Cloud Storage bucket, and create a model resource on Vertex AI, specifying the Cloud Storage path to your saved model.\n",
    "\n",
    "When you deploy your model, you can also provide custom code (beta) to customize how it handles prediction requests.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Import model artifacts to Vertex AI \n",
    "\n",
    "When you import a model, you associate it with a container for Vertex AI to run prediction requests. You can use pre-built containers provided by Vertex AI, or use your own custom containers that you build and push to Container Registry or Artifact Registry.\n",
    "\n",
    "You can use a pre-built container if your model meets the following requirements:\n",
    "\n",
    "- Trained in Python 3.7 or later\n",
    "- Trained using TensorFlow, scikit-learn, or XGBoost\n",
    "- Exported to meet framework-specific requirements for one of the pre-built prediction containers\n",
    "\n",
    "The link to the list of pre-built predict container images:\n",
    "\n",
    "https://cloud.google.com/vertex-ai/docs/predictions/pre-built-containers?_ga=2.125143370.-1302053296.1620920844&_gac=1.221340266.1622086653.CjwKCAjw47eFBhA9EiwAy8kzNOkCqVAmokRvQaxBDOoa8AhGOpzzW69x64rRzfgWxogIn3m6moQoBRoCuOsQAvD_BwE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:google.cloud.aiplatform.models:Creating Model\n",
      "INFO:google.cloud.aiplatform.models:Create Model backing LRO: projects/901951554789/locations/us-central1/models/4920050651506933760/operations/5088907325108912128\n",
      "INFO:google.cloud.aiplatform.models:Model created. Resource name: projects/901951554789/locations/us-central1/models/4920050651506933760\n",
      "INFO:google.cloud.aiplatform.models:To use this Model in another session:\n",
      "INFO:google.cloud.aiplatform.models:model = aiplatform.Model('projects/901951554789/locations/us-central1/models/4920050651506933760')\n",
      "model_id =  4920050651506933760\n"
     ]
    }
   ],
   "source": [
    "MODEL_NAME = \"image_tensorflow_model\"\n",
    "\n",
    "response = aiplatform.Model.upload(\n",
    "    display_name = MODEL_NAME,\n",
    "    serving_container_image_uri = 'us-docker.pkg.dev/vertex-ai/prediction/tf2-cpu.2-2:latest',\n",
    "    artifact_uri = best_model_dir_hpt, #best_model_dir_trn,\n",
    ")\n",
    "\n",
    "model_id = response.name.split('/')[-1]\n",
    "print(\"model_id = \", model_id)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create Endpoint\n",
    "\n",
    "You need the endpoint ID to deploy the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:google.cloud.aiplatform.models:Creating Endpoint\n",
      "INFO:google.cloud.aiplatform.models:Create Endpoint backing LRO: projects/901951554789/locations/us-central1/endpoints/5334144322713419776/operations/5818490464742932480\n",
      "INFO:google.cloud.aiplatform.models:Endpoint created. Resource name: projects/901951554789/locations/us-central1/endpoints/5334144322713419776\n",
      "INFO:google.cloud.aiplatform.models:To use this Endpoint in another session:\n",
      "INFO:google.cloud.aiplatform.models:endpoint = aiplatform.Endpoint('projects/901951554789/locations/us-central1/endpoints/5334144322713419776')\n",
      "endpoint.display_name  =  image_tensorflow_model_endpoint\n",
      "endpoint.resource_name =  projects/901951554789/locations/us-central1/endpoints/5334144322713419776\n",
      "endpoint_id =  5334144322713419776\n"
     ]
    }
   ],
   "source": [
    "MODEL_ENDPOINT_DISPLAY_NAME = \"image_tensorflow_model_endpoint\"\n",
    "\n",
    "aiplatform.init(project=PROJECT, location=REGION)\n",
    "endpoint = aiplatform.Endpoint.create(\n",
    "    display_name=MODEL_ENDPOINT_DISPLAY_NAME, project=PROJECT, location=REGION,\n",
    ")\n",
    "\n",
    "endpoint_id = endpoint.resource_name.split('/')[-1]\n",
    "\n",
    "print(\"endpoint.display_name  = \", endpoint.display_name)\n",
    "print(\"endpoint.resource_name = \", endpoint.resource_name)\n",
    "#print(endpoint.uri)\n",
    "print(\"endpoint_id = \", endpoint_id)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Deploy Model to the endpoint\n",
    "\n",
    "You must deploy a model to an endpoint before that model can be used to serve online predictions; deploying a model associates physical resources with the model so it can serve online predictions with low latency. An undeployed model can serve batch predictions, which do not have the same low latency requirements."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL_NAME = \"image_tensorflow_model\"\n",
    "DEPLOYED_MODEL_DISPLAY_NAME = \"image_tensorflow_model_deployed\"\n",
    "\n",
    "aiplatform.init(project=PROJECT, location=REGION)\n",
    "model = aiplatform.Model(model_name=model_id)\n",
    "\n",
    "# The explanation_metadata and explanation_parameters should only be\n",
    "# provided for a custom trained model and not an AutoML model.\n",
    "model.deploy(\n",
    "    endpoint=endpoint,\n",
    "    deployed_model_display_name=DEPLOYED_MODEL_DISPLAY_NAME,\n",
    "    machine_type = \"n1-standard-4\",\n",
    "    sync=True\n",
    ")\n",
    "\n",
    "print(model.display_name)\n",
    "print(model.resource_name)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Explore models and endpoints"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'name': 'projects/901951554789/locations/us-central1/models/4920050651506933760',\n",
       "  'display_name': 'image_tensorflow_model',\n",
       "  'create_time': DatetimeWithNanoseconds(2021, 7, 16, 3, 51, 13, 308910, tzinfo=datetime.timezone.utc),\n",
       "  'container': 'us-docker.pkg.dev/vertex-ai/prediction/tf2-cpu.2-2:latest',\n",
       "  'artifact_uri': 'gs://vapit_data/tf_models/models/model_07152021_2019/checkpoints/cp-203503-0-0.9740/'},\n",
       " {'name': 'projects/901951554789/locations/us-central1/models/6353180495429238784',\n",
       "  'display_name': 'freddiemacdata_20216247028',\n",
       "  'create_time': DatetimeWithNanoseconds(2021, 6, 24, 7, 1, 20, 984405, tzinfo=datetime.timezone.utc),\n",
       "  'container': '',\n",
       "  'artifact_uri': ''},\n",
       " {'name': 'projects/901951554789/locations/us-central1/models/7870893569853095936',\n",
       "  'display_name': 'my_first_tensorflow_model',\n",
       "  'create_time': DatetimeWithNanoseconds(2021, 6, 23, 5, 7, 45, 691387, tzinfo=datetime.timezone.utc),\n",
       "  'container': 'us-docker.pkg.dev/cloud-aiplatform/prediction/tf2-cpu.2-4:latest',\n",
       "  'artifact_uri': 'gs://vapit_data/tf_models/jobdir/model_062121_0825/checkpoints/cp-084311-0-0.9555/'},\n",
       " {'name': 'projects/901951554789/locations/us-central1/models/2825173137337876480',\n",
       "  'display_name': 'my_first_tensorflow_model',\n",
       "  'create_time': DatetimeWithNanoseconds(2021, 6, 21, 16, 20, 45, 560741, tzinfo=datetime.timezone.utc),\n",
       "  'container': 'us-docker.pkg.dev/vertex-ai/prediction/tf2-cpu.2-2:latest',\n",
       "  'artifact_uri': 'gs://vapit_data/tf_models/jobdir/model_062121_0825/checkpoints/cp-084311-0-0.9555/'}]"
      ]
     },
     "execution_count": 112,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# List all deployed models\n",
    "from google.cloud.aiplatform import gapic as aip\n",
    "def list_models():\n",
    "    PARENT = \"projects/\" + PROJECT + \"/locations/\" + REGION\n",
    "    API_ENDPOINT = \"{}-aiplatform.googleapis.com\".format(REGION)\n",
    "    client_options = {\"api_endpoint\": API_ENDPOINT}\n",
    "    client = aip.ModelServiceClient(client_options=client_options)\n",
    "    response = client.list_models(parent=PARENT)\n",
    "    model_list = []\n",
    "    for model in response:\n",
    "        model_list.append(\n",
    "            {\n",
    "                \"name\": model.name,\n",
    "                \"display_name\": model.display_name,\n",
    "                \"create_time\": model.create_time,\n",
    "                \"container\":  model.container_spec.image_uri,\n",
    "                \"artifact_uri\": model.artifact_uri\n",
    "            }\n",
    "        )\n",
    "    return(model_list)\n",
    "\n",
    "model_list = list_models()\n",
    "model_list\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'name': 'projects/901951554789/locations/us-central1/endpoints/5334144322713419776',\n",
       "  'display_name': 'image_tensorflow_model_endpoint',\n",
       "  'create_time': DatetimeWithNanoseconds(2021, 7, 16, 3, 52, 30, 92560, tzinfo=datetime.timezone.utc),\n",
       "  'deployed_models': ''},\n",
       " {'name': 'projects/901951554789/locations/us-central1/endpoints/5417038703354707968',\n",
       "  'display_name': 'freddimac_deployed',\n",
       "  'create_time': DatetimeWithNanoseconds(2021, 6, 24, 15, 26, 21, 375987, tzinfo=datetime.timezone.utc),\n",
       "  'deployed_models': 'projects/901951554789/locations/us-central1/models/6353180495429238784'},\n",
       " {'name': 'projects/901951554789/locations/us-central1/endpoints/5035921584888479744',\n",
       "  'display_name': 'my_first_tensorflow_model_endpoint',\n",
       "  'create_time': DatetimeWithNanoseconds(2021, 6, 23, 5, 14, 35, 882989, tzinfo=datetime.timezone.utc),\n",
       "  'deployed_models': 'projects/901951554789/locations/us-central1/models/7870893569853095936'},\n",
       " {'name': 'projects/901951554789/locations/us-central1/endpoints/7537108227939368960',\n",
       "  'display_name': 'my_first_tensorflow_model_endpoint',\n",
       "  'create_time': DatetimeWithNanoseconds(2021, 6, 21, 16, 20, 50, 482709, tzinfo=datetime.timezone.utc),\n",
       "  'deployed_models': 'projects/901951554789/locations/us-central1/models/2825173137337876480'}]"
      ]
     },
     "execution_count": 114,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# List all Endpoints\n",
    "from google.cloud.aiplatform import gapic as aip\n",
    "def list_endpoints():\n",
    "    PARENT = \"projects/\" + PROJECT + \"/locations/\" + REGION\n",
    "    API_ENDPOINT = \"{}-aiplatform.googleapis.com\".format(REGION)\n",
    "    client_options = {\"api_endpoint\": API_ENDPOINT}\n",
    "    client = aip.EndpointServiceClient(client_options=client_options)\n",
    "    response = client.list_endpoints(parent=PARENT)\n",
    "    endpoint_list = []\n",
    "    for endpoint in response:\n",
    "        model_name = ''\n",
    "        if (len(endpoint.deployed_models) > 0):\n",
    "            model_name = endpoint.deployed_models[0].model\n",
    "        endpoint_list.append(\n",
    "            {\n",
    "                \"name\": endpoint.name,\n",
    "                \"display_name\": endpoint.display_name,\n",
    "                \"create_time\": endpoint.create_time,\n",
    "                \"deployed_models\": model_name\n",
    "            }\n",
    "        )\n",
    "    return(endpoint_list)\n",
    "\n",
    "endpoint_list = list_endpoints()\n",
    "endpoint_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# deployed_model_id = endpoint.list_models()[0].id\n",
    "# print(deployed_model_id)\n",
    "# endpoint.undeploy(deployed_model_id=deployed_model_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(endpoint.list_models())\n",
    "# print(endpoint.resource_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "------\n",
    "### Send inference requests to your model\n",
    "\n",
    "Vertex AI provides the services you need to request predictions from your model in the cloud.\n",
    "\n",
    "There are two ways to get predictions from trained models: online prediction (sometimes called HTTP prediction) and batch prediction. In both cases, you pass input data to a cloud-hosted machine-learning model and get inferences for each data instance.\n",
    "\n",
    "Vertex AI online prediction is a service optimized to run your data through hosted models with as little latency as possible. You send small batches of data to the service and it returns your predictions in the response."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Call Google API for online inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train shape =  (60000, 28, 28)\n",
      "X_test  shape =  (10000, 28, 28)\n",
      "y_train shape =  (60000,)\n",
      "y_test  shape =  (10000,)\n",
      "batch_size= 16\n",
      "n_samples= 160\n",
      "Processing samples 0 16\n",
      "Processing samples 16 32\n",
      "Processing samples 32 48\n",
      "Processing samples 48 64\n",
      "Processing samples 64 80\n",
      "Processing samples 80 96\n",
      "Processing samples 96 112\n",
      "Processing samples 112 128\n",
      "Processing samples 128 144\n",
      "Processing samples 144 160\n"
     ]
    }
   ],
   "source": [
    "from googleapiclient import errors\n",
    "\n",
    "from trainer import inputs\n",
    "train_test_data = inputs.load_data()\n",
    "x_test  = train_test_data[1]\n",
    "#y_test  = train_test_data[3]\n",
    "\n",
    "pprobas = []\n",
    "batch_size = 16\n",
    "n_samples = min(160,x_test.shape[0])\n",
    "print(\"batch_size=\", batch_size)\n",
    "print(\"n_samples=\", n_samples)\n",
    "\n",
    "aiplatform.init(project=PROJECT, location=REGION)\n",
    "\n",
    "for i in range(0, n_samples, batch_size):\n",
    "    j = min(i+batch_size, n_samples)\n",
    "    print(\"Processing samples\", i, j)\n",
    "    response = aiplatform.Endpoint(endpoint_id).predict(instances=x_test[i:j].tolist())\n",
    "    try:\n",
    "        for prediction_ in response.predictions:\n",
    "            pprobas.append(prediction_)\n",
    "    except errors.HttpError as err:\n",
    "        # Something went wrong, print out some information.\n",
    "        tf.compat.v1.logging.error('There was an error getting the job info, Check the details:')\n",
    "        tf.compat.v1.logging.error(err._get_reason())\n",
    "        break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[7.29929661e-06, 1.15038265e-05, 1.23135484e-04, ...,\n",
       "        9.99724925e-01, 7.08431855e-07, 3.36139638e-05],\n",
       "       [2.14078182e-06, 8.08617217e-04, 9.99043882e-01, ...,\n",
       "        1.34824347e-07, 2.29412199e-05, 5.11516651e-09],\n",
       "       [2.89322543e-05, 9.93603647e-01, 1.02722691e-03, ...,\n",
       "        1.02126366e-03, 2.83336500e-04, 2.99024050e-05],\n",
       "       ...,\n",
       "       [9.97872710e-01, 8.58354360e-06, 1.70927568e-04, ...,\n",
       "        2.86914496e-04, 3.42474159e-05, 6.43192965e-04],\n",
       "       [3.95108473e-05, 1.18261110e-03, 4.56119664e-02, ...,\n",
       "        4.01133875e-04, 1.87761816e-05, 3.67112480e-05],\n",
       "       [1.97613572e-05, 7.69881764e-03, 2.87157920e-04, ...,\n",
       "        9.92664252e-04, 2.21624869e-05, 4.64928162e-04]])"
      ]
     },
     "execution_count": 119,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.array(pprobas)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Call Google GCLOUD API for online inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train shape =  (60000, 28, 28)\n",
      "X_test  shape =  (10000, 28, 28)\n",
      "y_train shape =  (60000,)\n",
      "y_test  shape =  (10000,)\n"
     ]
    }
   ],
   "source": [
    "from trainer import inputs\n",
    "train_test_data = inputs.load_data()\n",
    "x_test  = train_test_data[1]\n",
    "\n",
    "# Create a temporary json file to contain data to be predicted\n",
    "JSON_TEMP = 'tf_test_data.json' # temp json file name to hold the inference data\n",
    "batch_size = 100                # data batch size\n",
    "start = 0\n",
    "end = min(ind+batch_size, len(x_test))\n",
    "body={'instances': x_test[start:end].tolist()}\n",
    "# body = json.dumps(body).encode().decode()\n",
    "with open(JSON_TEMP, 'w') as fp:\n",
    "    fp.write(json.dumps(body))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!gcloud beta ai endpoints predict $endpoint_id \\\n",
    "  --region=$REGION \\\n",
    "  --json-request=$JSON_TEMP\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Call Google API for batch inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "n_samples = 1000\n",
      "nFiles = 10\n",
      "nRecsPerFile = 100\n",
      "Copying file://./batch_data/unkeyed_batch_7.json [Content-Type=application/json]...\n",
      "Copying file://./batch_data/unkeyed_batch_9.json [Content-Type=application/json]...\n",
      "Copying file://./batch_data/unkeyed_batch_5.json [Content-Type=application/json]...\n",
      "Copying file://./batch_data/unkeyed_batch_2.json [Content-Type=application/json]...\n",
      "Copying file://./batch_data/unkeyed_batch_8.json [Content-Type=application/json]...\n",
      "Copying file://./batch_data/unkeyed_batch_0.json [Content-Type=application/json]...\n",
      "Copying file://./batch_data/unkeyed_batch_1.json [Content-Type=application/json]...\n",
      "Copying file://./batch_data/unkeyed_batch_6.json [Content-Type=application/json]...\n",
      "Copying file://./batch_data/unkeyed_batch_3.json [Content-Type=application/json]...\n",
      "Copying file://./batch_data/unkeyed_batch_4.json [Content-Type=application/json]...\n",
      "/ [10/10 files][  5.8 MiB/  5.8 MiB] 100% Done                                  \n",
      "Operation completed over 10 objects/5.8 MiB.                                     \n",
      "CommandException: 1 files/objects could not be removed.\n"
     ]
    }
   ],
   "source": [
    "# Write batch data to file in GCS\n",
    "\n",
    "import shutil\n",
    "import os\n",
    "\n",
    "# Clean current directory\n",
    "DATA_DIR = './batch_data'\n",
    "shutil.rmtree(DATA_DIR, ignore_errors=True)\n",
    "os.makedirs(DATA_DIR)\n",
    "\n",
    "n_samples = min(1000,x_test.shape[0])\n",
    "nFiles = 10\n",
    "nRecsPerFile = min(1000,n_samples//nFiles)\n",
    "print(\"n_samples =\", n_samples)\n",
    "print(\"nFiles =\", nFiles)\n",
    "print(\"nRecsPerFile =\", nRecsPerFile)\n",
    "\n",
    "# Create nFiles files with nImagesPerFile images each\n",
    "for i in range(nFiles):\n",
    "    with open(f'{DATA_DIR}/unkeyed_batch_{i}.json', \"w\") as file:\n",
    "        for z in range(nRecsPerFile):\n",
    "            print(f'{{\"dense_input\": {np.array(x_test)[i*nRecsPerFile+z].tolist()}}}', file=file)\n",
    "            #print(f'{{\"{model_layers[0]}\": {np.array(x_test)[i*nRecsPerFile+z].tolist()}}}', file=file)\n",
    "            #key = f'key_{i}_{z}'\n",
    "            #print(f'{{\"image\": {x_test_images[z].tolist()}, \"key\": \"{key}\"}}', file=file)\n",
    "\n",
    "# Write batch data to gcs file\n",
    "!gsutil -m cp -r ./batch_data gs://$BUCKET_NAME/$FOLDER_NAME/\n",
    "    \n",
    "# Remove old batch prediction results\n",
    "!gsutil -m rm -r gs://$BUCKET_NAME/$FOLDER_NAME/batch_predictions\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "JOB_NAME_BATCH =  tensorflow_batch_cchatterjee_071521_2111\n",
      "JOB_DIR_BATCH =  gs://vapit_data/tf_models/tensorflow_batch_cchatterjee_071521_2111\n"
     ]
    }
   ],
   "source": [
    "JOBNAME_BATCH = 'tensorflow_batch_{}_{}'.format(\n",
    "    USER,\n",
    "    datetime.now(timezone(TIMEZONE)).strftime(\"%m%d%y_%H%M\")\n",
    "    )\n",
    "# We use the job names as folder names to store outputs.\n",
    "JOB_DIR_BATCH = 'gs://{}/{}/{}'.format(\n",
    "    BUCKET_NAME,\n",
    "    FOLDER_NAME,\n",
    "    JOBNAME_BATCH,\n",
    "    )\n",
    "\n",
    "INPUT_PATH='gs://' + BUCKET_NAME + '/' + FOLDER_NAME + '/batch_data/*'\n",
    "OUTPUT_PATH='gs://' + BUCKET_NAME + '/' + FOLDER_NAME + '/batch_predictions'\n",
    "\n",
    "print(\"JOB_NAME_BATCH = \", JOBNAME_BATCH)\n",
    "print(\"JOB_DIR_BATCH = \", JOB_DIR_BATCH)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "aiplatform.init(project=PROJECT, location=REGION)\n",
    "\n",
    "my_model = aiplatform.Model(model_name=model_id)\n",
    "\n",
    "\n",
    "# Make SDK batch_predict method call\n",
    "batch_prediction_job = my_model.batch_predict(\n",
    "    instances_format=\"jsonl\",\n",
    "    predictions_format=\"jsonl\",\n",
    "    job_display_name=JOBNAME_BATCH,\n",
    "    gcs_source=INPUT_PATH,\n",
    "    gcs_destination_prefix=OUTPUT_PATH,\n",
    "    model_parameters=None,\n",
    "    machine_type=\"n1-standard-4\",\n",
    "    starting_replica_count=1,\n",
    "    max_replica_count=1,\n",
    "    sync=True,\n",
    ")\n",
    "print(batch_prediction_job.display_name)\n",
    "print(batch_prediction_job.resource_name)\n",
    "print(batch_prediction_job.state)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"errors\")\n",
    "!gsutil cat $OUTPUT_PATH/prediction.errors_stats-00000-of-00001\n",
    "print(\"batch prediction results\")\n",
    "!gsutil cat $OUTPUT_PATH/prediction.results-00000-of-00010\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "environment": {
   "name": "tf2-gpu.2-1.m71",
   "type": "gcloud",
   "uri": "gcr.io/deeplearning-platform-release/tf2-gpu.2-1:m71"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
