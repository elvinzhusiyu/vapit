{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ============================================================================== \\\n",
    " Copyright 2020 Google LLC. This software is provided as-is, without warranty \\\n",
    " or representation for any use or purpose. Your use of it is subject to your \\\n",
    " agreement with Google. \\\n",
    " ============================================================================== \n",
    " \n",
    " Author: Elvin Zhu, Chanchal Chatterjee \\\n",
    " Email: elvinzhu@google.com \\\n",
    "<img src=\"img/google-cloud-icon.jpg\" alt=\"Drawing\" style=\"width: 200px;\"/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !python3 -m pip install kfp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import kfp\n",
    "import yaml\n",
    "import kfp.components as comp\n",
    "import kfp.dsl as dsl\n",
    "from typing import NamedTuple\n",
    "from kfp.compiler import compiler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_preprocess(\n",
    "    bucket_name: str,\n",
    "    input_file: str,\n",
    "    target_column: str,\n",
    "    ) -> NamedTuple('PreprocessOutput', \n",
    "              [\n",
    "                  ('x_train_name', str),\n",
    "                  ('x_test_name', str),\n",
    "                  ('y_train_name', str),\n",
    "                  ('y_test_name', str),\n",
    "                  ('n_classes', int),\n",
    "              ]):\n",
    "    \n",
    "    from collections import namedtuple\n",
    "    from sklearn.model_selection import train_test_split\n",
    "    import pandas as pd\n",
    "    import os\n",
    "    import logging\n",
    "\n",
    "\n",
    "    logging.info(\"Preprocessing raw data:\")\n",
    "    logging.info(\" => Drop id column:\")\n",
    "    logging.info(\" => One hot encoding categorical features\")\n",
    "    logging.info(\" => Count number of classes\")\n",
    "    logging.info(\" => Perform train/test split\")\n",
    "\n",
    "    logging.info(\"Reading raw data file: {}\".format(input_file))\n",
    "    dataset = pd.read_csv(input_file)\n",
    "    # Drop unique id column which is not useful for ML\n",
    "    logging.info(\"Drop unique id column which is not an useful feature for ML: {}\".format('LOAN_SEQUENCE_NUMBER'))\n",
    "    dataset.drop(['LOAN_SEQUENCE_NUMBER'], axis=1, inplace=True)\n",
    "\n",
    "    # Convert categorical columns into one-hot encodings\n",
    "    logging.info(\"Convert categorical columns into one-hot encodings\")\n",
    "    [logging.info(\"categorical feature: {}\".format(col)) for col in dataset.columns if dataset[col].dtype == 'object']\n",
    "    str_cols = [col for col in dataset.columns if dataset[col].dtype == 'object']\n",
    "    dataset = pd.get_dummies(dataset, columns=str_cols)\n",
    "    \n",
    "    # Count number of unique classes\n",
    "    logging.info(\"Count number of unique classes ...\")\n",
    "    n_classes = dataset[target_column].nunique()\n",
    "    logging.info(\"No. of Classes: {}\".format(n_classes))\n",
    "\n",
    "    # Split with a small test size so as to allow our model to train on more data\n",
    "    logging.info(\"Perform train/test split ...\")\n",
    "    x_train, x_test, y_train, y_test = train_test_split(\n",
    "        dataset.drop(target_column, axis=1), \n",
    "        dataset[target_column], \n",
    "        test_size=0.1,\n",
    "        random_state=1,\n",
    "        shuffle=True, \n",
    "        stratify=dataset[target_column], \n",
    "        )\n",
    "\n",
    "    # Fill Nan value with zeros\n",
    "    x_train = x_train.fillna(0)\n",
    "    x_test = x_test.fillna(0)\n",
    "    \n",
    "    logging.info(\"Get feature/label shapes ...\")\n",
    "    logging.info(\"x_train shape = {}\".format(x_train.shape))\n",
    "    logging.info(\"x_test shape = {}\".format(x_test.shape))\n",
    "    logging.info(\"y_train shape = {}\".format(y_train.shape))\n",
    "    logging.info(\"y_test shape = {}\".format(y_test.shape))\n",
    "    \n",
    "    # Build data split file name from input file name\n",
    "    base_file_name = os.path.basename(input_file)\n",
    "    base_name, ext_name = os.path.splitext(base_file_name)\n",
    "    x_train_name = \"{}_x_train{}\".format(base_name, ext_name)\n",
    "    x_test_name = \"{}_x_test{}\".format(base_name, ext_name)\n",
    "    y_train_name = \"{}_y_train{}\".format(base_name, ext_name)\n",
    "    y_test_name = \"{}_y_test{}\".format(base_name, ext_name)\n",
    "    \n",
    "    x_train_name = os.path.join(\"gs://\", bucket_name, \"data_split\", x_train_name)\n",
    "    x_test_name = os.path.join(\"gs://\", bucket_name, \"data_split\", x_test_name)\n",
    "    y_train_name = os.path.join(\"gs://\", bucket_name, \"data_split\", y_train_name)\n",
    "    y_test_name = os.path.join(\"gs://\", bucket_name, \"data_split\", y_test_name)\n",
    "\n",
    "    # Save split data to gcs\n",
    "    x_train.to_csv(x_train_name, index=False)\n",
    "    x_test.to_csv(x_test_name, index=False)\n",
    "\n",
    "    # The preprocessing for label column is different \n",
    "    # between tensorflow and XGBoost models\n",
    "    pd.get_dummies(y_train).to_csv(y_train_name, index=False, header=None)\n",
    "    pd.get_dummies(y_test).to_csv(y_test_name, index=False, header=None)\n",
    "    \n",
    "    # Saving data\n",
    "    logging.info(\"Saving data ...\")\n",
    "    logging.info(\"x_train saved to {}\".format(x_train_name))\n",
    "    logging.info(\"y_train saved to {}\".format(y_train_name))\n",
    "    logging.info(\"x_test saved to {}\".format(x_test_name))\n",
    "    logging.info(\"y_test saved to {}\".format(y_test_name))\n",
    "    logging.info(\"finished\")\n",
    "    \n",
    "    PreprocessOutput = namedtuple('PreprocessOutput', \n",
    "        ['x_train_name', 'x_test_name', 'y_train_name', 'y_test_name', 'n_classes'])\n",
    "    return PreprocessOutput(\n",
    "        x_train_name=x_train_name,\n",
    "        x_test_name=x_test_name,\n",
    "        y_train_name=y_train_name,\n",
    "        y_test_name=y_test_name,\n",
    "        n_classes=n_classes,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !gsutil cp ./config/config_hpt.yaml gs://tuti_asset/config/config_hpt.yaml\n",
    "# !gsutil cp ./config/config_hpt.yaml gs://tuti_asset/config/config.yaml"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def hypertune(\n",
    "        job_name: str,\n",
    "        bucket_name: str,\n",
    "        job_folder_name: str,\n",
    "        region: str,\n",
    "        train_feature_path: str,\n",
    "        train_label_path: str,\n",
    "        val_feature_path: str,\n",
    "        val_label_path: str,\n",
    "        epochs: int,\n",
    "        config_yaml: str = None,\n",
    "    ) -> NamedTuple('TrainOutput', \n",
    "              [('response', str), ('job_name', str)]):\n",
    "    from collections import namedtuple\n",
    "    import subprocess\n",
    "    import logging\n",
    "\n",
    "    job_dir = 'gs://{}/{}/{}'.format(\n",
    "        bucket_name,\n",
    "        job_folder_name,\n",
    "        job_name,\n",
    "        )\n",
    "    job_name = job_name + \"_hpt\"\n",
    "    package_path = \"/pipelines/component/trainer\"\n",
    "    module_name = \"trainer.train_hpt\"\n",
    "    \n",
    "    job_config = \"/pipelines/component/config/config_hpt.yaml\"\n",
    "    # if user input config yaml, then replace the default\n",
    "    if config_yaml is not None:\n",
    "        with open(job_config, 'w') as fout:\n",
    "            fout.write(config_yaml)\n",
    "\n",
    "    logging.info(\"JOB_NAME = {} \".format(job_name))\n",
    "    logging.info(\"JOB_DIR = {} \".format(job_dir))\n",
    "    logging.info(\"JOB_CONFIG = {} \".format(job_config))\n",
    "        \n",
    "    response = subprocess.run([\n",
    "        \"gcloud\", \"ai-platform\", \"jobs\", \"submit\", \"training\",\n",
    "        job_name,\n",
    "        \"--package-path\", package_path,\n",
    "        \"--module-name\", module_name,\n",
    "        \"--python-version\", \"3.7\",\n",
    "        \"--runtime-version\", \"2.2\",\n",
    "        \"--job-dir\", job_dir,\n",
    "        \"--region\", region,\n",
    "        \"--config\", job_config,\n",
    "        \"--\",\n",
    "        \"--train_feature_name\", train_feature_path,\n",
    "        \"--train_label_name\", train_label_path,\n",
    "        \"--test_feature_name\", val_feature_path,\n",
    "        \"--test_label_name\", val_label_path,\n",
    "        \"--epochs\", str(epochs),\n",
    "        ], stdout=subprocess.PIPE)   \n",
    "    \n",
    "    response = subprocess.run([\n",
    "        \"gcloud\", \"ai-platform\", \"jobs\", \"describe\", job_name,\n",
    "        ], stdout=subprocess.PIPE)\n",
    "    \n",
    "    TrainOutput = namedtuple('TrainOutput',['response', 'job_name'])\n",
    "        \n",
    "    return TrainOutput(response=response.stdout.decode(), job_name=job_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_job_status(\n",
    "        response: str,\n",
    "        job_name: str,\n",
    "        time_out: int = 9000, # timeout after 2.5 hours by default\n",
    "        time_sleep: int = 60, # check status every minute by default\n",
    "    ) -> NamedTuple('LRO_Output', \n",
    "              [('response', str), ('status', bool)]):\n",
    "    from collections import namedtuple\n",
    "    import subprocess\n",
    "    import time\n",
    "    import yaml\n",
    "    import logging\n",
    "\n",
    "    time0 = time.time()\n",
    "    status = False\n",
    "    while time.time() - time0 < time_out:\n",
    "        response = subprocess.run([\n",
    "            \"gcloud\", \"ai-platform\", \"jobs\", \"describe\", job_name,\n",
    "            ], stdout=subprocess.PIPE)\n",
    "        response = response.stdout.decode()\n",
    "        response_dict = yaml.safe_load(response)\n",
    "        if 'state' in response_dict and response_dict.get('state') == 'SUCCEEDED':\n",
    "            status = True\n",
    "            break\n",
    "        else:\n",
    "            logging.info(\"Checking status ...\")\n",
    "            logging.info(response)\n",
    "            time.sleep(time_sleep)\n",
    "    if not status:\n",
    "        raise TimeoutError(\"No successful job found. Timeout after {} seconds\".format(time_out))\n",
    "        \n",
    "    LRO_Output = namedtuple('LRO_Output',['response', 'status'])\n",
    "    return LRO_Output(response=response, status=status)        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_hyperparameter(\n",
    "        project_id: str,\n",
    "        job_name: str, \n",
    "        status: bool,\n",
    "    ) -> NamedTuple('Ghp_Output', \n",
    "              [('model_depth', int), ('dropout_rate', float), ('learning_rate', float), ('batch_size', int)]):\n",
    "    from googleapiclient import discovery\n",
    "    import json\n",
    "    import logging\n",
    "    import pandas as pd\n",
    "    from collections import namedtuple\n",
    "    \n",
    "    # Define the project id and the job id and format it for the api request\n",
    "    job_id = 'projects/{}/jobs/{}'.format(project_id, job_name)\n",
    "\n",
    "    # Build the service\n",
    "    ml = discovery.build('ml', 'v1', cache_discovery=False)\n",
    "    # Execute the request and pass in the job id\n",
    "    request = ml.projects().jobs().get(name=job_id).execute()\n",
    "    logging.info(json.dumps(request, indent=4))\n",
    "    # Print response\n",
    "    logging.info(json.dumps(request, indent=4))\n",
    "    trials = request['trainingOutput']['trials']\n",
    "    trials = pd.DataFrame(trials)\n",
    "    trials['hyperparameters.model_depth'] = trials['hyperparameters'].apply(lambda x: x['model_depth'])\n",
    "    trials['hyperparameters.dropout_rate'] = trials['hyperparameters'].apply(lambda x: x['dropout_rate'])\n",
    "    trials['hyperparameters.learning_rate'] = trials['hyperparameters'].apply(lambda x: x['learning_rate'])\n",
    "    trials['hyperparameters.batch_size'] = trials['hyperparameters'].apply(lambda x: x['batch_size'])\n",
    "    trials['finalMetric.trainingStep'] = trials['finalMetric'].apply(lambda x: x['trainingStep'])\n",
    "    trials['finalMetric.objectiveValue'] = trials['finalMetric'].apply(lambda x: x['objectiveValue'])\n",
    "    trials = trials.sort_values(['finalMetric.objectiveValue'], ascending=False)\n",
    "    \n",
    "    model_depth=trials['hyperparameters'][0]['model_depth']\n",
    "    dropout_rate=trials['hyperparameters'][0]['dropout_rate']\n",
    "    learning_rate=trials['hyperparameters'][0]['learning_rate']\n",
    "    batch_size=trials['hyperparameters'][0]['batch_size']\n",
    "\n",
    "    Ghp_Output = namedtuple('Ghp_Output',['model_depth', 'dropout_rate', 'learning_rate', 'batch_size'])\n",
    "    return Ghp_Output(model_depth=model_depth, dropout_rate=dropout_rate, learning_rate=learning_rate, batch_size=batch_size )  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(\n",
    "        job_name: str,\n",
    "        bucket_name: str,\n",
    "        job_folder_name: str,\n",
    "        region: str,\n",
    "        train_feature_path: str,\n",
    "        train_label_path: str,\n",
    "        val_feature_path: str,\n",
    "        val_label_path: str,\n",
    "        model_depth: int,\n",
    "        dropout_rate: float,\n",
    "        learning_rate: float,\n",
    "        batch_size: int,\n",
    "        epochs: int,\n",
    "        config_yaml: str = None,\n",
    "    ) -> NamedTuple('TrainOutput', \n",
    "              [('response', str), ('job_name', str)]):\n",
    "    from collections import namedtuple\n",
    "    import subprocess\n",
    "    import logging\n",
    "\n",
    "    job_dir = 'gs://{}/{}/{}'.format(\n",
    "        bucket_name,\n",
    "        job_folder_name,\n",
    "        job_name,\n",
    "        )\n",
    "    package_path = \"/pipelines/component/trainer\"\n",
    "    module_name = \"trainer.train\"\n",
    "    job_config = \"/pipelines/component/config/config.yaml\"\n",
    "    logging.info(\"JOB_NAME = {} \".format(job_name))\n",
    "    logging.info(\"JOB_DIR = {} \".format(job_dir))\n",
    "    logging.info(\"JOB_CONFIG = {} \".format(job_config))\n",
    "    # if user input config yaml, then replace the default\n",
    "    if config_yaml is not None:\n",
    "        with open(job_config, 'w') as fout:\n",
    "            fout.write(config_yaml)\n",
    "            \n",
    "    response = subprocess.run([\n",
    "        \"gcloud\", \"ai-platform\", \"jobs\", \"submit\", \"training\",\n",
    "        job_name,\n",
    "        \"--job-dir\", job_dir,\n",
    "        \"--package-path\", package_path,\n",
    "        \"--module-name\", module_name,\n",
    "        \"--region\", region,\n",
    "        \"--python-version\", \"3.7\",\n",
    "        \"--runtime-version\", \"2.2\",\n",
    "        \"--config\", job_config,\n",
    "        \"--\",\n",
    "        \"--train_feature_name\", train_feature_path,\n",
    "        \"--train_label_name\", train_label_path,\n",
    "        \"--test_feature_name\", val_feature_path,\n",
    "        \"--test_label_name\", val_label_path,\n",
    "        \"--model_depth\", str(model_depth),\n",
    "        \"--dropout_rate\", str(dropout_rate),\n",
    "        \"--learning_rate\", str(learning_rate),\n",
    "        \"--batch_size\", str(batch_size),\n",
    "        \"--epochs\", str(epochs),\n",
    "    ], stdout=subprocess.PIPE)\n",
    "    \n",
    "    response = subprocess.run([\n",
    "        \"gcloud\", \"ai-platform\", \"jobs\", \"describe\", job_name,\n",
    "    ], stdout=subprocess.PIPE)\n",
    "    \n",
    "    TrainOutput = namedtuple('TrainOutput',['response', 'job_name'])\n",
    "        \n",
    "    return TrainOutput(response=response.stdout.decode(), job_name=job_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def deploy(\n",
    "    bucket_name: str,\n",
    "    job_folder_name: str,\n",
    "    job_name: str,\n",
    "    model_name: str,\n",
    "    model_version: str,\n",
    "    region:str,\n",
    "    model_framework:str,\n",
    "    model_description: str,\n",
    "    status: bool,\n",
    "    ):\n",
    "    from collections import namedtuple\n",
    "    import subprocess\n",
    "    import logging\n",
    "    import re\n",
    "       \n",
    "    latest_model_dir = \"gs://{}/{}/{}\".format(bucket_name, job_folder_name, job_name)\n",
    "    \n",
    "    # Check if model exists:\n",
    "    response = subprocess.run([\n",
    "            \"gcloud\", \"ai-platform\", \"models\", \"list\",\n",
    "            \"--region\", \"global\",\n",
    "        ], stdout=subprocess.PIPE)\n",
    "    response = response.stdout.decode().split(\"\\n\")[1:]\n",
    "    list_of_models = [re.sub(\" +\", \" \", x).split(\" \")[0] for x in response]\n",
    "\n",
    "    # create model if not exists\n",
    "    if not model_name in list_of_models:\n",
    "        # create model\n",
    "        response = subprocess.run([\n",
    "            \"gcloud\", \"ai-platform\", \"models\", \"create\",\n",
    "            model_name,\n",
    "            \"--region\", region,\n",
    "            \"--enable-logging\",\n",
    "        ], stdout=subprocess.PIPE)\n",
    "    \n",
    "    # create model version\n",
    "    response = subprocess.run([\n",
    "        \"gcloud\",\"beta\", \"ai-platform\", \"versions\", \"create\",\n",
    "        model_version,\n",
    "        \"--model\", model_name,\n",
    "        \"--origin\", latest_model_dir,\n",
    "        \"--region\", \"global\",\n",
    "        \"--python-version\", \"3.7\",\n",
    "        \"--runtime-version\", \"2.2\",\n",
    "        \"--framework\", model_framework,\n",
    "        \"--description\", model_description,\n",
    "    ], stdout=subprocess.PIPE)\n",
    "    \n",
    "    DeployOutput = namedtuple('DeployOutput',['response'])\n",
    "        \n",
    "    return DeployOutput(response=response.stdout.decode())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compile python functions to components"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "component_dir = \"./components\"\n",
    "\n",
    "base_image = \"gcr.io/deeplearning-platform-release/tf2-gpu.2-1\"\n",
    "yaml_name = '{}/preprocess.yaml'.format(component_dir)\n",
    "\n",
    "preprocess_op = comp.func_to_container_op(\n",
    "    data_preprocess, \n",
    "    output_component_file=yaml_name,\n",
    "    base_image=base_image)\n",
    "\n",
    "\n",
    "base_image = \"gcr.io/img-seg-3d/trainer-tf:v1\"\n",
    "yaml_name = '{}/train_hpt.yaml'.format(component_dir)\n",
    "\n",
    "hypertune_op = comp.func_to_container_op(\n",
    "    hypertune, \n",
    "    output_component_file=yaml_name,\n",
    "    base_image=base_image)\n",
    "\n",
    "\n",
    "base_image = \"gcr.io/deeplearning-platform-release/tf2-gpu.2-1\"\n",
    "yaml_name = '{}/lro.yaml'.format(component_dir)\n",
    "\n",
    "lro_op = comp.func_to_container_op(\n",
    "    get_job_status, \n",
    "    output_component_file=yaml_name,\n",
    "    base_image=base_image)\n",
    "\n",
    "base_image = \"gcr.io/deeplearning-platform-release/tf2-gpu.2-1\"\n",
    "yaml_name = '{}/ghp.yaml'.format(component_dir)\n",
    "\n",
    "ghp_op = comp.func_to_container_op(\n",
    "    get_hyperparameter, \n",
    "    output_component_file=yaml_name,\n",
    "    base_image=base_image)\n",
    "\n",
    "base_image = \"gcr.io/img-seg-3d/trainer-tf@sha256:9565ad78f43f05b69fb6afe86b43f61627b76407f998b7de00ada6b886cb05d8\"\n",
    "yaml_name = '{}/train.yaml'.format(component_dir)\n",
    "\n",
    "train_op = comp.func_to_container_op(\n",
    "    train, \n",
    "    output_component_file=yaml_name,\n",
    "    base_image=base_image)\n",
    "\n",
    "\n",
    "base_image = \"gcr.io/deeplearning-platform-release/tf2-gpu.2-1\"\n",
    "yaml_name = '{}/deploy.yaml'.format(component_dir)\n",
    "\n",
    "deploy_op = comp.func_to_container_op(\n",
    "    deploy, \n",
    "    output_component_file=yaml_name,\n",
    "    base_image=base_image)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compile KFP pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "@dsl.pipeline(\n",
    "   name='generic prediction pipeline',\n",
    "   description='A pipeline that performs generic seismic image segmentation.'\n",
    ")\n",
    "def train_pipeline(\n",
    "    job_name: str,\n",
    "    project_id: str,\n",
    "    region: str,\n",
    "    user_name: str,\n",
    "    bucket_name: str,\n",
    "    input_file: str,\n",
    "    job_folder_name: str,\n",
    "    target_column: str,\n",
    "    deployed_model_name: str,\n",
    "    deployed_model_version: str,\n",
    "    deployed_model_description: str,\n",
    "    config_yaml_hpt: str,\n",
    "    config_yaml: str,\n",
    "    epochs_hpt: int,\n",
    "    epochs_final: int,\n",
    "    ):\n",
    "    preprocess_task = preprocess_op(\n",
    "        bucket_name = bucket_name,\n",
    "        input_file = input_file,\n",
    "        target_column = target_column,\n",
    "    )\n",
    "    \n",
    "    hpt_task = hypertune_op(\n",
    "        job_name = job_name,\n",
    "        bucket_name = bucket_name,\n",
    "        job_folder_name = job_folder_name,\n",
    "        region = region,\n",
    "        train_feature_path = preprocess_task.outputs['x_train_name'],\n",
    "        train_label_path = preprocess_task.outputs['y_train_name'],\n",
    "        val_feature_path = preprocess_task.outputs['x_test_name'],\n",
    "        val_label_path = preprocess_task.outputs['y_test_name'],\n",
    "        epochs = epochs_hpt,\n",
    "        config_yaml = config_yaml_hpt,\n",
    "    )\n",
    "           \n",
    "    lro_task = lro_op(\n",
    "        response = hpt_task.outputs['response'],\n",
    "        job_name = hpt_task.outputs['job_name'],\n",
    "    )\n",
    "    \n",
    "    ghp_task = ghp_op(\n",
    "        project_id = project_id,\n",
    "        job_name = hpt_task.outputs['job_name'],\n",
    "        status = lro_task.outputs['status'],\n",
    "    )\n",
    "    \n",
    "    train_task = train_op(\n",
    "        job_name = job_name,\n",
    "        bucket_name = bucket_name,\n",
    "        job_folder_name = job_folder_name,\n",
    "        region = region,\n",
    "        train_feature_path = preprocess_task.outputs['x_train_name'],\n",
    "        train_label_path = preprocess_task.outputs['y_train_name'],\n",
    "        val_feature_path = preprocess_task.outputs['x_test_name'],\n",
    "        val_label_path = preprocess_task.outputs['y_test_name'],\n",
    "        model_depth = ghp_task.outputs['model_depth'],\n",
    "        dropout_rate = ghp_task.outputs['dropout_rate'],\n",
    "        learning_rate = ghp_task.outputs['learning_rate'],\n",
    "        batch_size = ghp_task.outputs['batch_size'],\n",
    "        epochs = epochs_final,\n",
    "        config_yaml = config_yaml,\n",
    "    )\n",
    "    \n",
    "    lro_task_2 = lro_op(\n",
    "        response = train_task.outputs['response'],\n",
    "        job_name = train_task.outputs['job_name'],\n",
    "    )\n",
    "    \n",
    "    deploy_task = deploy_op(\n",
    "        status = lro_task_2.outputs['status'],\n",
    "        bucket_name = bucket_name,\n",
    "        job_folder_name = job_folder_name,\n",
    "        job_name = train_task.outputs['job_name'],\n",
    "        region = 'global',\n",
    "        model_framework = 'tensorflow',\n",
    "        model_name = deployed_model_name,\n",
    "        model_version = deployed_model_version,\n",
    "        model_description = deployed_model_description,\n",
    "    )\n",
    "    \n",
    "pipeline_pkg_path=\"./train_pipeline.tar.gz\"\n",
    "\n",
    "compiler.Compiler().compile(train_pipeline, package_path=pipeline_pkg_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run KFP pipeline on AI Platform hosted Kubernetes cluster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<a href=\"http://6ff530db99970db2-dot-us-central2.pipelines.googleusercontent.com/#/experiments/details/c8a8a752-36e7-486e-b447-68bd05946c69\" target=\"_blank\" >Experiment details</a>."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<a href=\"http://6ff530db99970db2-dot-us-central2.pipelines.googleusercontent.com/#/runs/details/b10086cc-d740-4ce0-8306-fb8853eea71c\" target=\"_blank\" >Run details</a>."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# ============== Uncomment to run the pipeline ==============\n",
    "\n",
    "# from datetime import datetime\n",
    "# from pytz import timezone\n",
    "# my_timezone = 'US/Pacific'\n",
    "\n",
    "# # Coping config file to worker VM\n",
    "# config_hpt = \"./config/config_hpt.yaml\"\n",
    "# with open(config_hpt, 'r') as fin:\n",
    "#     config_yaml_hpt = fin.read()\n",
    "    \n",
    "# config = \"./config/config.yaml\"\n",
    "# with open(config, 'r') as fin:\n",
    "#     config_yaml = fin.read()\n",
    "    \n",
    "# # Define pipeline input\n",
    "# params = {\n",
    "#     \"job_name\": 'tf_train_elvinzhu_{}'.format(\n",
    "#         datetime.now(timezone(my_timezone)).strftime(\"%m%d%y_%H%M\")\n",
    "#         ),\n",
    "#     \"project_id\": 'img-seg-3d',\n",
    "#     \"region\": 'us-central1',\n",
    "#     \"user_name\": 'elvinzhu',\n",
    "#     \"job_folder_name\": 'tf_train_job',\n",
    "#     \"bucket_name\": 'tuti_job',\n",
    "#     \"input_file\": 'gs://tuti_asset/datasets/mortgage_structured.csv',\n",
    "#     \"target_column\": 'TARGET',\n",
    "#     \"deployed_model_name\": \"kfp_tf_model\",\n",
    "#     \"deployed_model_version\": \"kfp_tf_bst_v0_2\",\n",
    "#     \"deployed_model_description\": \"best_tensorflow_hpt\",\n",
    "#     \"config_yaml_hpt\": config_yaml_hpt,\n",
    "#     \"config_yaml\": config_yaml,\n",
    "#     \"epochs_hpt\": 5, # No. of epochs for training in hypertune\n",
    "#     \"epochs_final\": 10, # No. of epochs for final training\n",
    "# }\n",
    "# kfp_host_name = '6ff530db99970db2-dot-us-central2.pipelines.googleusercontent.com'\n",
    "# kfp_exp_name = 'tensorflow_ai_platform'\n",
    "# kfp_run_name = 'demo_tensorflow'\n",
    "\n",
    "# client = kfp.Client(host=kfp_host_name) \n",
    "# # Create Experiment GROUP\n",
    "# exp = client.create_experiment(name = kfp_exp_name)\n",
    "# # Create Experiment RUN\n",
    "# run = client.run_pipeline(exp.id, kfp_run_name, pipeline_pkg_path, params=params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "environment": {
   "name": "tf2-gpu.2-1.m61",
   "type": "gcloud",
   "uri": "gcr.io/deeplearning-platform-release/tf2-gpu.2-1:m61"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
