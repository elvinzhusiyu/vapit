{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ============================================================================== \\\n",
    " Copyright 2020 Google LLC. This software is provided as-is, without warranty \\\n",
    " or representation for any use or purpose. Your use of it is subject to your \\\n",
    " agreement with Google. \\\n",
    " ============================================================================== \n",
    " \n",
    " Author: Elvin Zhu, Chanchal Chatterjee \\\n",
    " Email: elvinzhu@google.com \\\n",
    "<img src=\"img/google-cloud-icon.jpg\" alt=\"Drawing\" style=\"width: 200px;\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### List your current GCP project name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "img-seg-3d\n"
     ]
    }
   ],
   "source": [
    "!gcloud config list --format 'value(core.project)' 2>/dev/null"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import packages\n",
    "\n",
    "import json\n",
    "import logging\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from datetime import datetime\n",
    "from pytz import timezone\n",
    "import googleapiclient\n",
    "from googleapiclient import discovery"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-----------\n",
    "### Dataset preprocessing\n",
    "\n",
    "Preprocess input data by\n",
    "\n",
    "    1. Dropping unique ID column;\n",
    "    2. Convert categorical into one-hot encodings;\n",
    "    3. Count number of unique classes;\n",
    "    4. Split train/test\n",
    "    5. None value removal\n",
    "    6. Save process data into gcs\n",
    "    \n",
    "What is the difference from XGBoost preprocessing?\n",
    "1. None value removal added (Automatically handled by XGBoost model);\n",
    "2. Labels are one-hot encoded (XGBoost uses integers);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:root:Preprocessing raw data:\n",
      "INFO:root: => Drop id column:\n",
      "INFO:root: => One hot encoding categorical features\n",
      "INFO:root: => Count number of classes\n",
      "INFO:root: => Perform train/test split\n",
      "INFO:root:Reading raw data file: gs://tuti_asset/datasets/mortgage_structured.csv\n",
      "INFO:root:Drop unique id column which is not an useful feature for ML: LOAN_SEQUENCE_NUMBER\n",
      "INFO:root:Convert categorical columns into one-hot encodings\n",
      "INFO:root:categorical feature: first_time_home_buyer_flag\n",
      "INFO:root:categorical feature: occupancy_status\n",
      "INFO:root:categorical feature: channel\n",
      "INFO:root:categorical feature: property_state\n",
      "INFO:root:categorical feature: property_type\n",
      "INFO:root:categorical feature: loan_purpose\n",
      "INFO:root:categorical feature: seller_name\n",
      "INFO:root:categorical feature: service_name\n",
      "INFO:root:Count number of unique classes ...\n",
      "INFO:root:No. of Classes: 4\n",
      "INFO:root:Perform train/test split ...\n",
      "INFO:root:Get feature/label shapes ...\n",
      "INFO:root:x_train shape = (93639, 149)\n",
      "INFO:root:x_test shape = (10405, 149)\n",
      "INFO:root:y_train shape = (93639,)\n",
      "INFO:root:y_test shape = (10405,)\n",
      "INFO:root:Saving data ...\n",
      "INFO:root:x_train saved to gs://tuti_job/data_split/mortgage_structured_x_train.csv\n",
      "INFO:root:y_train saved to gs://tuti_job/data_split/mortgage_structured_y_train.csv\n",
      "INFO:root:x_test saved to gs://tuti_job/data_split/mortgage_structured_x_test.csv\n",
      "INFO:root:y_test saved to gs://tuti_job/data_split/mortgage_structured_y_test.csv\n",
      "INFO:root:finished\n"
     ]
    }
   ],
   "source": [
    "INPUT_DATA = \"gs://tuti_asset/datasets/mortgage_structured.csv\"\n",
    "TARGET_COLUMN = \"TARGET\"\n",
    "\n",
    "# TODO: Update gcs path before proceeding\n",
    "TRAIN_FEATURE_PATH = \"gs://<YOUR-BUCKET>/data_split/mortgage_structured_x_train.csv\"\n",
    "TRAIN_LABEL_PATH = \"gs://<YOUR-BUCKET>/data_split/mortgage_structured_y_train.csv\"\n",
    "TEST_FEATURE_PATH = \"gs://<YOUR-BUCKET>/data_split/mortgage_structured_x_test.csv\"\n",
    "TEST_LABEL_PATH = \"gs://<YOUR-BUCKET>/data_split/mortgage_structured_y_test.csv\"\n",
    "\n",
    "!python3 preprocessing.py \\\n",
    "    --input_file $INPUT_DATA \\\n",
    "    --x_train_name $TRAIN_FEATURE_PATH \\\n",
    "    --y_train_name $TRAIN_LABEL_PATH \\\n",
    "    --x_test_name $TEST_FEATURE_PATH \\\n",
    "    --y_test_name $TEST_LABEL_PATH \\\n",
    "    --target_column $TARGET_COLUMN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "------\n",
    "### Training with Google AI Platform"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the full article, please visit: https://cloud.google.com/ai-platform/docs/technical-overview\n",
    "\n",
    "Where AI Platform fits in the ML workflow \\\n",
    "The diagram below gives a high-level overview of the stages in an ML workflow. The blue-filled boxes indicate where AI Platform provides managed services and APIs:\n",
    "\n",
    "<img src=\"img/ml-workflow.svg\" alt=\"Drawing\">\n",
    "\n",
    "As the diagram indicates, you can use AI Platform to manage the following stages in the ML workflow:\n",
    "\n",
    "- Train an ML model on your data:\n",
    " - Train model\n",
    " - Evaluate model accuracy\n",
    " - Tune hyperparameters\n",
    " \n",
    " \n",
    "- Deploy your trained model.\n",
    "\n",
    "- Send prediction requests to your model:\n",
    " - Online prediction\n",
    " - Batch prediction (for TensorFlow only)\n",
    " \n",
    " \n",
    "- Monitor the predictions on an ongoing basis.\n",
    "\n",
    "\n",
    "- Manage your models and model versions.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "JOB_NAME =  tf_train_elvinzhu_051021_1232\n",
      "JOB_DIR =  gs://tuti_job/tf_train_job/tf_train_elvinzhu_051021_1232\n",
      "JOB_CONFIG =  ./config/config.yaml\n"
     ]
    }
   ],
   "source": [
    "PROJECT_ID = '<YOUR-PROJECT-ID>'     # Replace with your project ID\n",
    "USER = '<YOUR-USERNAME>'             # Replace with your user name\n",
    "BUCKET_NAME = '<YOUR-BUCKET>'    # Replace with your gcs bucket name\n",
    "FOLDER_NAME = 'tf_train_job' # Replace with your gcs folder name\n",
    "REGION = 'us-central1'        # Replace with your GCP region\n",
    "TIMEZONE = 'US/Pacific'       # Replace with your local timezone\n",
    "\n",
    "# Google Cloud AI Platform requires each job to have unique name, \n",
    "# Therefore, we use prefix + timestamp to form job names.\n",
    "JOBNAME = 'tf_train_{}_{}'.format(\n",
    "    USER,\n",
    "    datetime.now(timezone(TIMEZONE)).strftime(\"%m%d%y_%H%M\")\n",
    "    ) # Unique job name\n",
    "\n",
    "# We use the job names as folder names to store outputs.\n",
    "JOB_DIR = 'gs://{}/{}/{}'.format(\n",
    "    BUCKET_NAME,\n",
    "    FOLDER_NAME,\n",
    "    JOBNAME,\n",
    "    ) # gcs path to hold the outputs\n",
    "\n",
    "# This is the AI Platform configuration for training, created in the setup step\n",
    "JOB_CONFIG = \"./config/config.yaml\" # local path to training config file\n",
    "\n",
    "# Path to your input feature and labels\n",
    "TRAIN_FEATURE_PATH = 'gs://<YOUR-BUCKET>/data_split/mortgage_structured_x_train.csv'\n",
    "TRAIN_LABEL_PATH = 'gs://<YOUR-BUCKET>/data_split/mortgage_structured_y_train.csv'\n",
    "TEST_FEATURE_PATH = \"gs://<YOUR-BUCKET>/data_split/mortgage_structured_x_test.csv\"\n",
    "TEST_LABEL_PATH = \"gs://<YOUR-BUCKET>/data_split/mortgage_structured_y_test.csv\"\n",
    "\n",
    "# Get the initial set of hyperparameters\n",
    "N_CLASSES = 4 \n",
    "BOOSTER = 'gbtree' # Booster type\n",
    "MAX_DEPTH = 2      # Depth of trees\n",
    "N_ESTIMATORS = 10  # No of estimators\n",
    "\n",
    "print(\"JOB_NAME = \", JOBNAME)\n",
    "print(\"JOB_DIR = \", JOB_DIR)\n",
    "print(\"JOB_CONFIG = \", JOB_CONFIG)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Train at local\n",
    "\n",
    "Before submitting training jobs to Cloud AI Platform, you can test your train.py code in the local environment. You can test by running your python script in command line, but another and maybe better choice is to use `gcloud ai-platform local train` command. The latter method could make sure your your entire python package are ready to be submitted to the remote VMs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-05-10 19:32:46.840859: W tensorflow/stream_executor/platform/default/dso_loader.cc:55] Could not load dynamic library 'libnvinfer.so.6'; dlerror: libnvinfer.so.6: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/local/cuda/lib64:/usr/local/nccl2/lib:/usr/local/cuda/extras/CUPTI/lib64\n",
      "2021-05-10 19:32:46.840985: W tensorflow/stream_executor/platform/default/dso_loader.cc:55] Could not load dynamic library 'libnvinfer_plugin.so.6'; dlerror: libnvinfer_plugin.so.6: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/local/cuda/lib64:/usr/local/nccl2/lib:/usr/local/cuda/extras/CUPTI/lib64\n",
      "2021-05-10 19:32:46.840998: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:30] Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.\n",
      "Namespace(batch_size=4, depth=3, dropout_rate=0.02, epochs=1, job_dir='./models', learning_rate=0.0001, test_feature_name='gs://tuti_job/data_split/mortgage_structured_x_test.csv', test_label_name='gs://tuti_job/data_split/mortgage_structured_y_test.csv', train_feature_name='gs://tuti_job/data_split/mortgage_structured_x_train.csv', train_label_name='gs://tuti_job/data_split/mortgage_structured_y_train.csv', verbosity='FATAL')\n",
      "Input and pre-process data ...\n",
      "Shapes:\n",
      "(93639, 149)\n",
      "(93639, 4)\n",
      "Creating model ...\n",
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense (Dense)                (None, 128)               19200     \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 107)               13803     \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 65)                7020      \n",
      "_________________________________________________________________\n",
      "dropout (Dropout)            (None, 65)                0         \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 4)                 264       \n",
      "=================================================================\n",
      "Total params: 40,287\n",
      "Trainable params: 40,287\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "Fitting model ...\n",
      "trialId= 0\n",
      "Train on 93639 samples, validate on 10405 samples\n",
      "93584/93639 [============================>.] - ETA: 0s - loss: 0.2254 - accuracy: 0.9247WARNING:tensorflow:From /home/jupyter/.local/lib/python3.7/site-packages/tensorflow_core/python/ops/resource_variable_ops.py:1786: calling BaseResourceVariable.__init__ (from tensorflow.python.ops.resource_variable_ops) with constraint is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "If using Keras pass *_constraint arguments to layers.\n",
      "93639/93639 [==============================] - 60s 638us/sample - loss: 0.2253 - accuracy: 0.9247 - val_loss: 0.0397 - val_accuracy: 0.9289\n",
      "final_epoch_accuracy = 0.924689\n",
      "final_epoch_count =  1\n"
     ]
    }
   ],
   "source": [
    "# Train on local machine with python command\n",
    "!python3 trainer/train.py \\\n",
    "    --job-dir ./models \\\n",
    "    --train_feature_name $TRAIN_FEATURE_PATH \\\n",
    "    --train_label_name $TRAIN_LABEL_PATH \\\n",
    "    --test_feature_name $TEST_FEATURE_PATH \\\n",
    "    --test_label_name $TEST_LABEL_PATH\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-05-10 19:33:59.233378: W tensorflow/stream_executor/platform/default/dso_loader.cc:55] Could not load dynamic library 'libnvinfer.so.6'; dlerror: libnvinfer.so.6: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/local/cuda/lib64:/usr/local/nccl2/lib:/usr/local/cuda/extras/CUPTI/lib64\n",
      "2021-05-10 19:33:59.233502: W tensorflow/stream_executor/platform/default/dso_loader.cc:55] Could not load dynamic library 'libnvinfer_plugin.so.6'; dlerror: libnvinfer_plugin.so.6: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/local/cuda/lib64:/usr/local/nccl2/lib:/usr/local/cuda/extras/CUPTI/lib64\n",
      "2021-05-10 19:33:59.233533: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:30] Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.\n",
      "Namespace(batch_size=4, depth=3, dropout_rate=0.02, epochs=1, job_dir='./models', learning_rate=0.0001, test_feature_name='gs://tuti_job/data_split/mortgage_structured_x_test.csv', test_label_name='gs://tuti_job/data_split/mortgage_structured_y_test.csv', train_feature_name='gs://tuti_job/data_split/mortgage_structured_x_train.csv', train_label_name='gs://tuti_job/data_split/mortgage_structured_y_train.csv', verbosity='FATAL')\n",
      "Input and pre-process data ...\n",
      "Shapes:\n",
      "(93639, 149)\n",
      "(93639, 4)\n",
      "Creating model ...\n",
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense (Dense)                (None, 128)               19200     \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 107)               13803     \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 65)                7020      \n",
      "_________________________________________________________________\n",
      "dropout (Dropout)            (None, 65)                0         \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 4)                 264       \n",
      "=================================================================\n",
      "Total params: 40,287\n",
      "Trainable params: 40,287\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "Fitting model ...\n",
      "trialId= 0\n",
      "Train on 93639 samples, validate on 10405 samples\n",
      "93608/93639 [============================>.] - ETA: 0s - loss: 0.1956 - accuracy: 0.6983WARNING:tensorflow:From /home/jupyter/.local/lib/python3.7/site-packages/tensorflow_core/python/ops/resource_variable_ops.py:1786: calling BaseResourceVariable.__init__ (from tensorflow.python.ops.resource_variable_ops) with constraint is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "If using Keras pass *_constraint arguments to layers.\n",
      "93639/93639 [==============================] - 62s 662us/sample - loss: 0.1955 - accuracy: 0.6983 - val_loss: 0.0356 - val_accuracy: 0.9289\n",
      "final_epoch_accuracy = 0.698341\n",
      "final_epoch_count =  1\n"
     ]
    }
   ],
   "source": [
    "# Train on local machine with gcloud command\n",
    "!gcloud ai-platform local train \\\n",
    "    --job-dir ./models \\\n",
    "    --package-path $(pwd)/trainer \\\n",
    "    --module-name trainer.train \\\n",
    "    -- \\\n",
    "    --train_feature_name $TRAIN_FEATURE_PATH \\\n",
    "    --train_label_name $TRAIN_LABEL_PATH \\\n",
    "    --test_feature_name $TEST_FEATURE_PATH \\\n",
    "    --test_label_name $TEST_LABEL_PATH \\\n",
    "    --depth 3 \\\n",
    "    --dropout_rate 0.02 \\\n",
    "    --learning_rate 0.0001 \\\n",
    "    --batch_size 4 \\\n",
    "    --epochs 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Submit jobs to AI platform\n",
    "See link for a full list of arguments: \\\n",
    "https://cloud.google.com/sdk/gcloud/reference/ai-platform/jobs/submit/training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Job [tf_train_elvinzhu_051021_1232] submitted successfully.\n",
      "Your job is still active. You may view the status of your job with the command\n",
      "\n",
      "  $ gcloud ai-platform jobs describe tf_train_elvinzhu_051021_1232\n",
      "\n",
      "or continue streaming the logs with the command\n",
      "\n",
      "  $ gcloud ai-platform jobs stream-logs tf_train_elvinzhu_051021_1232\n",
      "jobId: tf_train_elvinzhu_051021_1232\n",
      "state: QUEUED\n"
     ]
    }
   ],
   "source": [
    "# submit the training job to AI Platform\n",
    "! gcloud ai-platform jobs submit training $JOBNAME \\\n",
    "    --job-dir $JOB_DIR \\\n",
    "    --package-path $(pwd)/trainer \\\n",
    "    --module-name trainer.train \\\n",
    "    --region $REGION \\\n",
    "    --python-version 3.7 \\\n",
    "    --runtime-version 2.2 \\\n",
    "    --config $JOB_CONFIG \\\n",
    "    -- \\\n",
    "    --train_feature_name $TRAIN_FEATURE_PATH \\\n",
    "    --train_label_name $TRAIN_LABEL_PATH \\\n",
    "    --test_feature_name $TEST_FEATURE_PATH \\\n",
    "    --test_label_name $TEST_LABEL_PATH \\\n",
    "    --depth 3 \\\n",
    "    --dropout_rate 0.02 \\\n",
    "    --learning_rate 0.0001 \\\n",
    "    --batch_size 4 \\\n",
    "    --epochs 5 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "createTime: '2021-05-10T19:35:09Z'\n",
      "etag: jW5sUEEG4Nc=\n",
      "jobId: tf_train_elvinzhu_051021_1232\n",
      "state: PREPARING\n",
      "trainingInput:\n",
      "  args:\n",
      "  - --train_feature_name\n",
      "  - gs://tuti_job/data_split/mortgage_structured_x_train.csv\n",
      "  - --train_label_name\n",
      "  - gs://tuti_job/data_split/mortgage_structured_y_train.csv\n",
      "  - --test_feature_name\n",
      "  - gs://tuti_job/data_split/mortgage_structured_x_test.csv\n",
      "  - --test_label_name\n",
      "  - gs://tuti_job/data_split/mortgage_structured_y_test.csv\n",
      "  - --depth\n",
      "  - '3'\n",
      "  - --dropout_rate\n",
      "  - '0.02'\n",
      "  - --learning_rate\n",
      "  - '0.0001'\n",
      "  - --batch_size\n",
      "  - '4'\n",
      "  - --epochs\n",
      "  - '5'\n",
      "  jobDir: gs://tuti_job/tf_train_job/tf_train_elvinzhu_051021_1232\n",
      "  packageUris:\n",
      "  - gs://tuti_job/tf_train_job/tf_train_elvinzhu_051021_1232/packages/c54b00d83480f177f19013665b0db22ba09f5137f103daa45e5a5ab8e62c55d1/trainer-0.1.tar.gz\n",
      "  pythonModule: trainer.train\n",
      "  pythonVersion: '3.7'\n",
      "  region: us-central1\n",
      "  runtimeVersion: '2.2'\n",
      "  scaleTier: STANDARD_1\n",
      "trainingOutput: {}\n",
      "\n",
      "View job in the Cloud Console at:\n",
      "https://console.cloud.google.com/mlengine/jobs/tf_train_elvinzhu_051021_1232?project=img-seg-3d\n",
      "\n",
      "View logs at:\n",
      "https://console.cloud.google.com/logs?resource=ml_job%2Fjob_id%2Ftf_train_elvinzhu_051021_1232&project=img-seg-3d\n"
     ]
    }
   ],
   "source": [
    "!gcloud ai-platform jobs describe $JOBNAME"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "------\n",
    "### Hyperparameter Tuning\n",
    "\n",
    "To use hyperparameter tuning in your training job you must perform the following steps:\n",
    "\n",
    "- Specify the hyperparameter tuning configuration for your training job by including a HyperparameterSpec in your TrainingInput object.\n",
    "\n",
    "- Include the following code in your training application:\n",
    "\n",
    " - Parse the command-line arguments representing the hyperparameters you want to tune, and use the values to set the hyperparameters for your training trial.\n",
    " - Add your hyperparameter metric to the summary for your graph.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "JOB_NAME =  tf_train_elvinzhu_051021_1235_hpt\n",
      "JOB_DIR =  gs://tuti_job/tf_train_job/jobdir\n",
      "JOB_CONFIG =  ./config/config_hpt.yaml\n"
     ]
    }
   ],
   "source": [
    "# Gcloud training config\n",
    "PROJECT_ID = '<YOUR-PROJECT-ID>'     # Replace with your project ID\n",
    "USER = '<YOUR-USERNAME>'             # Replace with your user name\n",
    "BUCKET_NAME = '<YOUR-BUCKET>'    # Replace with your gcs bucket name\n",
    "FOLDER_NAME = 'tf_train_job' # Replace with your gcs folder name\n",
    "REGION = 'us-central1'        # Replace with your GCP region\n",
    "TIMEZONE = 'US/Pacific'       # Replace with your local timezone\n",
    "\n",
    "# Google Cloud AI Platform requires each job to have unique name, \n",
    "# Therefore, we use prefix + timestamp to form job names.\n",
    "JOBNAME = 'tf_train_{}_{}_hpt'.format(\n",
    "    USER,\n",
    "    datetime.now(timezone(TIMEZONE)).strftime(\"%m%d%y_%H%M\")\n",
    "    ) # define unique job name\n",
    "\n",
    "# We use the job names as folder names to store outputs.\n",
    "JOB_DIR = 'gs://{}/{}/jobdir'.format(\n",
    "    BUCKET_NAME,\n",
    "    FOLDER_NAME,\n",
    "    ) # define unique job dir on gcs\n",
    "\n",
    "# This is the AI Platform configuration for hypertune, created in the setup step\n",
    "JOB_CONFIG = \"./config/config_hpt.yaml\" # local path to hypertune config file\n",
    "\n",
    "# Path to your input feature and labels (Train/validation)\n",
    "TRAIN_FEATURE_PATH = 'gs://<YOUR-BUCKET>/data_split/mortgage_structured_x_train.csv'\n",
    "TRAIN_LABEL_PATH = 'gs://<YOUR-BUCKET>/data_split/mortgage_structured_y_train.csv'\n",
    "TEST_FEATURE_PATH = \"gs://<YOUR-BUCKET>/data_split/mortgage_structured_x_test.csv\"\n",
    "TEST_LABEL_PATH = \"gs://<YOUR-BUCKET>/data_split/mortgage_structured_y_test.csv\"\n",
    "\n",
    "print(\"JOB_NAME = \", JOBNAME)\n",
    "print(\"JOB_DIR = \", JOB_DIR)\n",
    "print(\"JOB_CONFIG = \", JOB_CONFIG)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Job [tf_train_elvinzhu_051021_1235_hpt] submitted successfully.\n",
      "Your job is still active. You may view the status of your job with the command\n",
      "\n",
      "  $ gcloud ai-platform jobs describe tf_train_elvinzhu_051021_1235_hpt\n",
      "\n",
      "or continue streaming the logs with the command\n",
      "\n",
      "  $ gcloud ai-platform jobs stream-logs tf_train_elvinzhu_051021_1235_hpt\n",
      "jobId: tf_train_elvinzhu_051021_1235_hpt\n",
      "state: QUEUED\n"
     ]
    }
   ],
   "source": [
    "# submit the hyperparameter training job\n",
    "!gcloud ai-platform jobs submit training $JOBNAME \\\n",
    "    --package-path $(pwd)/trainer \\\n",
    "    --module-name trainer.train_hpt \\\n",
    "    --python-version 3.7 \\\n",
    "    --runtime-version 2.2 \\\n",
    "    --job-dir $JOB_DIR \\\n",
    "    --region $REGION \\\n",
    "    --config $JOB_CONFIG \\\n",
    "    -- \\\n",
    "    --train_feature_name $TRAIN_FEATURE_PATH \\\n",
    "    --train_label_name $TRAIN_LABEL_PATH \\\n",
    "    --test_feature_name $TEST_FEATURE_PATH \\\n",
    "    --test_label_name $TEST_LABEL_PATH \\\n",
    "    --epochs 5 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Check the status of Long Running Operation (LRO) a.k.a. jobs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "createTime: '2021-05-10T19:35:13Z'\n",
      "etag: AcULn0Ne9TE=\n",
      "jobId: tf_train_elvinzhu_051021_1235_hpt\n",
      "state: PREPARING\n",
      "trainingInput:\n",
      "  args:\n",
      "  - --train_feature_name\n",
      "  - gs://tuti_job/data_split/mortgage_structured_x_train.csv\n",
      "  - --train_label_name\n",
      "  - gs://tuti_job/data_split/mortgage_structured_y_train.csv\n",
      "  - --test_feature_name\n",
      "  - gs://tuti_job/data_split/mortgage_structured_x_test.csv\n",
      "  - --test_label_name\n",
      "  - gs://tuti_job/data_split/mortgage_structured_y_test.csv\n",
      "  - --epochs\n",
      "  - '5'\n",
      "  hyperparameters:\n",
      "    enableTrialEarlyStopping: true\n",
      "    goal: MAXIMIZE\n",
      "    hyperparameterMetricTag: accuracy\n",
      "    maxParallelTrials: 2\n",
      "    maxTrials: 5\n",
      "    params:\n",
      "    - maxValue: 10.0\n",
      "      minValue: 2.0\n",
      "      parameterName: model_depth\n",
      "      scaleType: UNIT_LINEAR_SCALE\n",
      "      type: INTEGER\n",
      "    - maxValue: 0.01\n",
      "      minValue: 0.001\n",
      "      parameterName: dropout_rate\n",
      "      scaleType: UNIT_LOG_SCALE\n",
      "      type: DOUBLE\n",
      "    - maxValue: 0.01\n",
      "      minValue: 1e-05\n",
      "      parameterName: learning_rate\n",
      "      scaleType: UNIT_LOG_SCALE\n",
      "      type: DOUBLE\n",
      "    - maxValue: 10.0\n",
      "      minValue: 1.0\n",
      "      parameterName: batch_size\n",
      "      scaleType: UNIT_LINEAR_SCALE\n",
      "      type: INTEGER\n",
      "  jobDir: gs://tuti_job/tf_train_job/jobdir\n",
      "  packageUris:\n",
      "  - gs://tuti_job/tf_train_job/jobdir/packages/a2c9d634a74b43779f83ddf1d8690ef726570c1ef05b7e35dbd54fc7018e272d/trainer-0.1.tar.gz\n",
      "  pythonModule: trainer.train_hpt\n",
      "  pythonVersion: '3.7'\n",
      "  region: us-central1\n",
      "  runtimeVersion: '2.2'\n",
      "  scaleTier: STANDARD_1\n",
      "trainingOutput:\n",
      "  isHyperparameterTuningJob: true\n",
      "\n",
      "View job in the Cloud Console at:\n",
      "https://console.cloud.google.com/mlengine/jobs/tf_train_elvinzhu_051021_1235_hpt?project=img-seg-3d\n",
      "\n",
      "View logs at:\n",
      "https://console.cloud.google.com/logs?resource=ml_job%2Fjob_id%2Ftf_train_elvinzhu_051021_1235_hpt&project=img-seg-3d\n"
     ]
    }
   ],
   "source": [
    "!gcloud ai-platform jobs describe $JOBNAME"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Check the status of Long Running Operation (LRO) with Google API Client\n",
    "\n",
    "Send an API request to Cloud AI Platform to get the detailed information. The most interesting piece of information is the hyperparameter values in the trial with best performance metric."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the project id and the job id and format it for the api request\n",
    "# We need to use project id and job name from last step\n",
    "job_id = 'projects/{}/jobs/{}'.format(PROJECT_ID, JOBNAME)\n",
    "# Build the service\n",
    "ml = discovery.build('ml', 'v1', cache_discovery=False)\n",
    "# Execute the request and pass in the job id\n",
    "request = ml.projects().jobs().get(name=job_id).execute()\n",
    "# Print response\n",
    "logging.info(json.dumps(request, indent=4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parse request response and sort experiments based on final metrics\n",
    "trials = request['trainingOutput']['trials']\n",
    "trials = pd.DataFrame(trials)\n",
    "trials['hyperparameters.model_depth'] = trials['hyperparameters'].apply(lambda x: x['model_depth'])\n",
    "trials['hyperparameters.dropout_rate'] = trials['hyperparameters'].apply(lambda x: x['dropout_rate'])\n",
    "trials['hyperparameters.learning_rate'] = trials['hyperparameters'].apply(lambda x: x['learning_rate'])\n",
    "trials['hyperparameters.batch_size'] = trials['hyperparameters'].apply(lambda x: x['batch_size'])\n",
    "trials['finalMetric.trainingStep'] = trials['finalMetric'].apply(lambda x: x['trainingStep'])\n",
    "trials['finalMetric.objectiveValue'] = trials['finalMetric'].apply(lambda x: x['objectiveValue'])\n",
    "trials = trials.sort_values(['finalMetric.objectiveValue'], ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trials"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "------\n",
    "### Training with Tuned Parameters\n",
    "\n",
    "Once your hyperparameter training jobs are done. You can use the optimized combination of hyperparameters from your trials and start a single training job on Cloud AI Platform to train your final model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "PROJECT_ID = '<YOUR-PROJECT-ID>' # Replace with your project ID\n",
    "USER = '<YOUR-USERNAME>' # Replace with your User name\n",
    "BUCKET_NAME = '<YOUR-BUCKET>' # Replace with your bucket name\n",
    "FOLDER_NAME = 'tf_train_job' # Replace with your Folder name\n",
    "REGION = 'us-central1' # Replace with your region\n",
    "TIMEZONE = 'US/Pacific'\n",
    "\n",
    "# Google Cloud AI Platform requires each job to have unique name, \n",
    "# Therefore, we use prefix + timestamp to form job names.\n",
    "JOBNAME = 'tf_train_{}_{}'.format(\n",
    "    USER,\n",
    "    datetime.now(timezone(TIMEZONE)).strftime(\"%m%d%y_%H%M\")\n",
    "    )\n",
    "# We use the job names as folder names to store outputs.\n",
    "JOB_DIR = 'gs://{}/{}/{}'.format(\n",
    "    BUCKET_NAME,\n",
    "    FOLDER_NAME,\n",
    "    JOBNAME,\n",
    "    )\n",
    "\n",
    "# This is the AI Platform configuration for training, created in the setup step\n",
    "JOB_CONFIG = \"./config/config.yaml\" # local path to train config file\n",
    "\n",
    "print(\"JOB_NAME = \", JOBNAME)\n",
    "print(\"JOB_DIR = \", JOB_DIR)\n",
    "print(\"JOB_CONFIG = \", JOB_CONFIG)\n",
    "\n",
    "# Path to your input feature and labels (Train/validation)\n",
    "TRAIN_FEATURE_PATH = 'gs://<YOUR-BUCKET>/data_split/mortgage_structured_x_train.csv'\n",
    "TRAIN_LABEL_PATH = 'gs://<YOUR-BUCKET>/data_split/mortgage_structured_y_train.csv'\n",
    "TEST_FEATURE_PATH = \"gs://<YOUR-BUCKET>/data_split/mortgage_structured_x_test.csv\"\n",
    "TEST_LABEL_PATH = \"gs://<YOUR-BUCKET>/data_split/mortgage_structured_y_test.csv\"\n",
    "\n",
    "# Getthe best hypertuned model parameters\n",
    "N_CLASSES = 4\n",
    "DEPTH=trials['hyperparameters'][0]['model_depth']\n",
    "DROPOUT_RATE=trials['hyperparameters'][0]['dropout_rate']\n",
    "LEARNING_RATE=trials['hyperparameters'][0]['learning_rate']\n",
    "BATCH_SIZE=trials['hyperparameters'][0]['batch_size']\n",
    "\n",
    "\n",
    "print(\"TRAIN_FEATURE_PATH = \", TRAIN_FEATURE_PATH)\n",
    "print(\"TRAIN_LABEL_PATH = \", TRAIN_LABEL_PATH)\n",
    "print(\"N_CLASSES = \", N_CLASSES)\n",
    "print(\"DEPTH = \", DEPTH)\n",
    "print(\"DROPOUT_RATE = \", DROPOUT_RATE)\n",
    "print(\"LEARNING_RATE = \", LEARNING_RATE)\n",
    "print(\"BATCH_SIZE = \", BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# submit the training job\n",
    "! gcloud ai-platform jobs submit training $JOBNAME \\\n",
    "    --job-dir $JOB_DIR \\\n",
    "    --package-path $(pwd)/trainer \\\n",
    "    --module-name trainer.train \\\n",
    "    --region $REGION \\\n",
    "    --python-version 3.7 \\\n",
    "    --runtime-version 2.2 \\\n",
    "    --config $JOB_CONFIG \\\n",
    "    -- \\\n",
    "    --train_feature_name $TRAIN_FEATURE_PATH \\\n",
    "    --train_label_name $TRAIN_LABEL_PATH \\\n",
    "    --test_feature_name $TEST_FEATURE_PATH \\\n",
    "    --test_label_name $TEST_LABEL_PATH \\\n",
    "    --depth $DEPTH \\\n",
    "    --dropout_rate $DROPOUT_RATE \\\n",
    "    --learning_rate $LEARNING_RATE \\\n",
    "    --batch_size $BATCH_SIZE \\\n",
    "    --epochs 5 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check the training job status\n",
    "! gcloud ai-platform jobs describe $JOBNAME"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "--------\n",
    "### Deploy the Model\n",
    "\n",
    "AI Platform provides tools to upload your trained ML model to the cloud, so that you can send prediction requests to the model.\n",
    "\n",
    "In order to deploy your trained model on AI Platform, you must save your trained model using the tools provided by your machine learning framework. This involves serializing the information that represents your trained model into a file which you can deploy for prediction in the cloud.\n",
    "\n",
    "Then you upload the saved model to a Cloud Storage bucket, and create a model resource on AI Platform, specifying the Cloud Storage path to your saved model.\n",
    "\n",
    "When you deploy your model, you can also provide custom code (beta) to customize how it handles prediction requests.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL_NAME = \"tensorflow_model\"                # Model name of your choice to deploy\n",
    "MODEL_VERSION = \"tensorflow_v0_1\" # Model version name of your choice to deploy\n",
    "REGION = \"global\"                       # The deployed model region\n",
    "MODEL_FRAMEWORK = \"tensorflow\"             # The deployed model framework (tensorflow, sklearn, xgboost)\n",
    "MODEL_DESCRIPTION = \"tensorflow_hpt_best\"      # The description of your model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create model if not exist\n",
    "!gcloud ai-platform models create $MODEL_NAME --region $\"global\" --enable-logging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# list model versions under model\n",
    "!gcloud ai-platform versions list --model $MODEL_NAME --region \"global\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The gcs path contains your latested trained model\n",
    "LATEST_MODEL_DIR = \"gs://{}/{}/{}\".format(BUCKET_NAME, FOLDER_NAME, JOBNAME)\n",
    "print(\"LATEST_MODEL_DIR: \", LATEST_MODEL_DIR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Deploy the model to endpoint\n",
    "! gcloud beta ai-platform versions create $MODEL_VERSION \\\n",
    "  --model=$MODEL_NAME \\\n",
    "  --origin=$LATEST_MODEL_DIR \\\n",
    "  --runtime-version=2.2 \\\n",
    "  --python-version=3.7 \\\n",
    "  --framework=$MODEL_FRAMEWORK \\\n",
    "  --description=$MODEL_DESCRIPTION \\\n",
    "  --region=$REGION \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# List all models\n",
    "!gcloud ai-platform models list --region $REGION\n",
    "# List all versions of the created model\n",
    "!gcloud ai-platform versions list --model $MODEL_NAME --region $REGION\n",
    "# Describe the Model\n",
    "!gcloud ai-platform models describe $MODEL_NAME --region $REGION"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "------\n",
    "### Send inference requests to your model\n",
    "\n",
    "AI Platform provides the services you need to request predictions from your model in the cloud.\n",
    "\n",
    "There are two ways to get predictions from trained models: online prediction (sometimes called HTTP prediction) and batch prediction. In both cases, you pass input data to a cloud-hosted machine-learning model and get inferences for each data instance.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load testing data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load test feature and labels\n",
    "test_feature_url = 'gs://<YOUR-BUCKET>/data_split/mortgage_structured_x_test.csv'\n",
    "test_label_url = 'gs://<YOUR-BUCKET>/data_split/mortgage_structured_y_test.csv'\n",
    "\n",
    "x_test = pd.read_csv(test_feature_url)\n",
    "y_test = pd.read_csv(test_label_url, header=None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Call Google API for online inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create google API client \n",
    "PROJECT_ID = \"<YOUR-PROJECT-ID>\" # Your project id\n",
    "MODEL_NAME = \"tensorflow_model\"  # The model name from previous step\n",
    "VERSION = \"tensorflow_v0_1\" # The model version from previous step\n",
    "batch_size = 1000\n",
    "\n",
    "# Create model inference with Google API Client \n",
    "# Model endpoint name\n",
    "model_name = 'projects/{}/models/{}/versions/{}'.format(\n",
    "    PROJECT_ID, \n",
    "    MODEL_NAME, \n",
    "    VERSION\n",
    "    )\n",
    "\n",
    "# Build the service\n",
    "service = discovery.build(\n",
    "    'ml', \n",
    "    'v1', \n",
    "    cache_discovery=False, \n",
    "    cache=False\n",
    "    )\n",
    "\n",
    "prediction_list = []\n",
    "\n",
    "for ind in range(0, len(x_test), batch_size):\n",
    "    start = ind\n",
    "    end = min(ind+batch_size, len(x_test))\n",
    "    response = service.projects().predict(\n",
    "        name=model_name,\n",
    "        body={'instances': x_test.iloc[start:end].values.tolist()}\n",
    "        ).execute()\n",
    "    response = response['predictions']\n",
    "    response = [x['dense_6'] for x in response]\n",
    "    prediction_list += response\n",
    "    \n",
    "prediction_list = np.array(prediction_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Other way to call Cloud AI Platform API using gcloud command for prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def post_process(predict, n_sample, n_class):  \n",
    "    \"\"\"Parse response of inference requests,\n",
    "    Args:\n",
    "        predict: List of strings, inference request response;\n",
    "        n_sample: No. of samples for inference;\n",
    "        n_class: No. of classes\n",
    "    Return:\n",
    "        List of inference labels\n",
    "    \"\"\"\n",
    "    predictions = np.empty([n_sample, n_class])\n",
    "    for entry in predict[1:]:\n",
    "        key, value = entry.split(\":\")\n",
    "        exec(\"{} = {}\".format(key, value))\n",
    "    predictions = np.argmax(predictions, axis=1)\n",
    "    return predictions.tolist()\n",
    "\n",
    "def accuracy_score(y_true, y_pred):\n",
    "    \"\"\" Compute accuracy score\n",
    "    Args:\n",
    "        y_ture: list of ground truth labels,\n",
    "        y_pred: list of predicted labels,\n",
    "    Return:\n",
    "        float, accuracy score\n",
    "    \"\"\"\n",
    "    from sklearn import metrics\n",
    "    return metrics.accuracy_score(y_true, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "PROJECT_ID = \"<YOUR-PROJECT-ID>\"        # Project ID\n",
    "MODEL_NAME = \"tensorflow_model\"         # Model name from previous step\n",
    "VERSION = \"tensorflow_v0_1\"     # Model version from previous step\n",
    "JSON_TEMP = 'test_data.json' # temp json file name to hold the inference data\n",
    "batch_size = 1000                # data batch size\n",
    "\n",
    "y_pred = []\n",
    "for ind in range(0, len(x_test), batch_size):\n",
    "    start = ind\n",
    "    end = min(ind+batch_size, len(x_test))\n",
    "    body={'instances': x_test.iloc[start:end].values.tolist()}\n",
    "    with open(JSON_TEMP, 'w') as fp:\n",
    "        json.dump(body, fp)\n",
    "    \n",
    "    predict = !gcloud ai-platform predict \\\n",
    "      --model=$MODEL_NAME \\\n",
    "      --version=$VERSION \\\n",
    "      --format='text' \\\n",
    "      --json-request=$JSON_TEMP \\\n",
    "      --region=$REGION\n",
    "    \n",
    "    y_pred += post_process(predict[1:], end-start, N_CLASSES)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy = accuracy_score([np.where(r==1)[0][0] for r in y_test.to_numpy()], y_pred)\n",
    "print(\"Accuracy: \", accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "environment": {
   "name": "tf2-gpu.2-1.m61",
   "type": "gcloud",
   "uri": "gcr.io/deeplearning-platform-release/tf2-gpu.2-1:m61"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
