{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==============================================================================\n",
    "# Copyright 2020 Google LLC. This software is provided as-is, without warranty\n",
    "# or representation for any use or purpose. Your use of it is subject to your\n",
    "# agreement with Google.\n",
    "# ==============================================================================\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found existing installation: tensorflow 2.1.0\n",
      "Uninstalling tensorflow-2.1.0:\n",
      "  Successfully uninstalled tensorflow-2.1.0\n",
      "Found existing installation: numpy 1.18.0\n",
      "Uninstalling numpy-1.18.0:\n",
      "  Successfully uninstalled numpy-1.18.0\n",
      "Found existing installation: pandas 1.2.1\n",
      "Uninstalling pandas-1.2.1:\n",
      "  Successfully uninstalled pandas-1.2.1\n",
      "Found existing installation: scipy 1.4.1\n",
      "Uninstalling scipy-1.4.1:\n",
      "  Successfully uninstalled scipy-1.4.1\n",
      "Found existing installation: scikit-learn 0.22\n",
      "Uninstalling scikit-learn-0.22:\n",
      "  Successfully uninstalled scikit-learn-0.22\n",
      "Found existing installation: google-api-python-client 1.7.11\n",
      "Uninstalling google-api-python-client-1.7.11:\n",
      "  Successfully uninstalled google-api-python-client-1.7.11\n",
      "Found existing installation: gcsfs 0.6.1\n",
      "Uninstalling gcsfs-0.6.1:\n",
      "  Successfully uninstalled gcsfs-0.6.1\n",
      "Found existing installation: google-cloud 0.34.0\n",
      "Uninstalling google-cloud-0.34.0:\n",
      "  Successfully uninstalled google-cloud-0.34.0\n",
      "Found existing installation: google-cloud-storage 1.23.0\n",
      "Uninstalling google-cloud-storage-1.23.0:\n",
      "  Successfully uninstalled google-cloud-storage-1.23.0\n",
      "Found existing installation: matplotlib 3.2.1\n",
      "Uninstalling matplotlib-3.2.1:\n",
      "  Successfully uninstalled matplotlib-3.2.1\n",
      "Found existing installation: cloudml-hypertune 0.1.0.dev6\n",
      "Uninstalling cloudml-hypertune-0.1.0.dev6:\n",
      "  Successfully uninstalled cloudml-hypertune-0.1.0.dev6\n"
     ]
    }
   ],
   "source": [
    "# Uninstall old packages\n",
    "!pip3 uninstall -r requirements_uninstall.txt -y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mWARNING: --use-feature=2020-resolver no longer has any effect, since it is now the default dependency resolver in pip. This will become an error in pip 21.0.\u001b[0m\n",
      "Collecting gcsfs==0.6.1\n",
      "  Downloading gcsfs-0.6.1-py2.py3-none-any.whl (17 kB)\n",
      "Requirement already satisfied: google-auth>=1.2 in /opt/conda/lib/python3.7/site-packages (from gcsfs==0.6.1->-r requirements-rt2.1.txt (line 7)) (1.24.0)\n",
      "Requirement already satisfied: requests in /opt/conda/lib/python3.7/site-packages (from gcsfs==0.6.1->-r requirements-rt2.1.txt (line 7)) (2.25.1)\n",
      "Requirement already satisfied: google-auth-oauthlib in /opt/conda/lib/python3.7/site-packages (from gcsfs==0.6.1->-r requirements-rt2.1.txt (line 7)) (0.4.2)\n",
      "Requirement already satisfied: fsspec>=0.6.0 in /opt/conda/lib/python3.7/site-packages (from gcsfs==0.6.1->-r requirements-rt2.1.txt (line 7)) (0.8.5)\n",
      "Requirement already satisfied: decorator in /opt/conda/lib/python3.7/site-packages (from gcsfs==0.6.1->-r requirements-rt2.1.txt (line 7)) (4.4.2)\n",
      "Collecting google-api-python-client==1.7.11\n",
      "  Downloading google-api-python-client-1.7.11.tar.gz (142 kB)\n",
      "\u001b[K     |████████████████████████████████| 142 kB 9.7 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: httplib2<1dev,>=0.9.2 in /opt/conda/lib/python3.7/site-packages (from google-api-python-client==1.7.11->-r requirements-rt2.1.txt (line 6)) (0.18.1)\n",
      "Requirement already satisfied: google-auth-httplib2>=0.0.3 in /opt/conda/lib/python3.7/site-packages (from google-api-python-client==1.7.11->-r requirements-rt2.1.txt (line 6)) (0.0.4)\n",
      "Requirement already satisfied: six<2dev,>=1.6.1 in /opt/conda/lib/python3.7/site-packages (from google-api-python-client==1.7.11->-r requirements-rt2.1.txt (line 6)) (1.15.0)\n",
      "Requirement already satisfied: uritemplate<4dev,>=3.0.0 in /opt/conda/lib/python3.7/site-packages (from google-api-python-client==1.7.11->-r requirements-rt2.1.txt (line 6)) (3.0.1)\n",
      "Collecting google-cloud==0.34.0\n",
      "  Downloading google_cloud-0.34.0-py2.py3-none-any.whl (1.8 kB)\n",
      "Collecting google-cloud-storage==1.23.0\n",
      "  Downloading google_cloud_storage-1.23.0-py2.py3-none-any.whl (72 kB)\n",
      "\u001b[K     |████████████████████████████████| 72 kB 14.1 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: google-resumable-media<0.6dev,>=0.5.0 in /home/jupyter/.local/lib/python3.7/site-packages (from google-cloud-storage==1.23.0->-r requirements-rt2.1.txt (line 9)) (0.5.1)\n",
      "Requirement already satisfied: google-cloud-core<2.0dev,>=1.0.3 in /opt/conda/lib/python3.7/site-packages (from google-cloud-storage==1.23.0->-r requirements-rt2.1.txt (line 9)) (1.3.0)\n",
      "Collecting matplotlib==3.2.1\n",
      "  Downloading matplotlib-3.2.1-cp37-cp37m-manylinux1_x86_64.whl (12.4 MB)\n",
      "\u001b[K     |████████████████████████████████| 12.4 MB 9.1 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: cycler>=0.10 in /opt/conda/lib/python3.7/site-packages (from matplotlib==3.2.1->-r requirements-rt2.1.txt (line 10)) (0.10.0)\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in /opt/conda/lib/python3.7/site-packages (from matplotlib==3.2.1->-r requirements-rt2.1.txt (line 10)) (1.3.1)\n",
      "Requirement already satisfied: python-dateutil>=2.1 in /opt/conda/lib/python3.7/site-packages (from matplotlib==3.2.1->-r requirements-rt2.1.txt (line 10)) (2.8.1)\n",
      "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /opt/conda/lib/python3.7/site-packages (from matplotlib==3.2.1->-r requirements-rt2.1.txt (line 10)) (2.4.7)\n",
      "Collecting numpy==1.18.0\n",
      "  Downloading numpy-1.18.0-cp37-cp37m-manylinux1_x86_64.whl (20.1 MB)\n",
      "\u001b[K     |████████████████████████████████| 20.1 MB 29.9 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting pandas==1.2.1\n",
      "  Downloading pandas-1.2.1-cp37-cp37m-manylinux1_x86_64.whl (9.9 MB)\n",
      "\u001b[K     |████████████████████████████████| 9.9 MB 79.4 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: pytz>=2017.3 in /opt/conda/lib/python3.7/site-packages (from pandas==1.2.1->-r requirements-rt2.1.txt (line 3)) (2020.5)\n",
      "Collecting scikit-learn==0.22\n",
      "  Downloading scikit_learn-0.22-cp37-cp37m-manylinux1_x86_64.whl (7.0 MB)\n",
      "\u001b[K     |████████████████████████████████| 7.0 MB 56.0 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: joblib>=0.11 in /opt/conda/lib/python3.7/site-packages (from scikit-learn==0.22->-r requirements-rt2.1.txt (line 5)) (1.0.0)\n",
      "Collecting scipy==1.4.1\n",
      "  Downloading scipy-1.4.1-cp37-cp37m-manylinux1_x86_64.whl (26.1 MB)\n",
      "\u001b[K     |████████████████████████████████| 26.1 MB 71.2 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting tensorflow==2.1.0\n",
      "  Downloading tensorflow-2.1.0-cp37-cp37m-manylinux2010_x86_64.whl (421.8 MB)\n",
      "\u001b[K     |██████████████████████████▋     | 350.4 MB 93.0 MB/s eta 0:00:01"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "IOPub data rate exceeded.\n",
      "The notebook server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--NotebookApp.iopub_data_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "NotebookApp.iopub_data_rate_limit=1000000.0 (bytes/sec)\n",
      "NotebookApp.rate_limit_window=3.0 (secs)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[?25hRequirement already satisfied: termcolor>=1.1.0 in /opt/conda/lib/python3.7/site-packages (from tensorflow==2.1.0->-r requirements-rt2.1.txt (line 1)) (1.1.0)\n",
      "Requirement already satisfied: wheel>=0.26 in /opt/conda/lib/python3.7/site-packages (from tensorflow==2.1.0->-r requirements-rt2.1.txt (line 1)) (0.36.2)\n",
      "Requirement already satisfied: wrapt>=1.11.1 in /opt/conda/lib/python3.7/site-packages (from tensorflow==2.1.0->-r requirements-rt2.1.txt (line 1)) (1.12.1)\n",
      "Requirement already satisfied: protobuf>=3.8.0 in /opt/conda/lib/python3.7/site-packages (from tensorflow==2.1.0->-r requirements-rt2.1.txt (line 1)) (3.14.0)\n",
      "Requirement already satisfied: keras-applications>=1.0.8 in /opt/conda/lib/python3.7/site-packages (from tensorflow==2.1.0->-r requirements-rt2.1.txt (line 1)) (1.0.8)\n",
      "Requirement already satisfied: google-pasta>=0.1.6 in /opt/conda/lib/python3.7/site-packages (from tensorflow==2.1.0->-r requirements-rt2.1.txt (line 1)) (0.2.0)\n",
      "Requirement already satisfied: astor>=0.6.0 in /opt/conda/lib/python3.7/site-packages (from tensorflow==2.1.0->-r requirements-rt2.1.txt (line 1)) (0.8.1)\n",
      "Requirement already satisfied: opt-einsum>=2.3.2 in /opt/conda/lib/python3.7/site-packages (from tensorflow==2.1.0->-r requirements-rt2.1.txt (line 1)) (3.3.0)\n",
      "Requirement already satisfied: keras-preprocessing>=1.1.0 in /opt/conda/lib/python3.7/site-packages (from tensorflow==2.1.0->-r requirements-rt2.1.txt (line 1)) (1.1.0)\n",
      "Requirement already satisfied: tensorboard<2.2.0,>=2.1.0 in /opt/conda/lib/python3.7/site-packages (from tensorflow==2.1.0->-r requirements-rt2.1.txt (line 1)) (2.1.1)\n",
      "Requirement already satisfied: gast==0.2.2 in /opt/conda/lib/python3.7/site-packages (from tensorflow==2.1.0->-r requirements-rt2.1.txt (line 1)) (0.2.2)\n",
      "Requirement already satisfied: tensorflow-estimator<2.2.0,>=2.1.0rc0 in /opt/conda/lib/python3.7/site-packages (from tensorflow==2.1.0->-r requirements-rt2.1.txt (line 1)) (2.1.0)\n",
      "Requirement already satisfied: absl-py>=0.7.0 in /opt/conda/lib/python3.7/site-packages (from tensorflow==2.1.0->-r requirements-rt2.1.txt (line 1)) (0.8.1)\n",
      "Requirement already satisfied: grpcio>=1.8.6 in /opt/conda/lib/python3.7/site-packages (from tensorflow==2.1.0->-r requirements-rt2.1.txt (line 1)) (1.34.0)\n",
      "Requirement already satisfied: cachetools<5.0,>=2.0.0 in /opt/conda/lib/python3.7/site-packages (from google-auth>=1.2->gcsfs==0.6.1->-r requirements-rt2.1.txt (line 7)) (4.1.1)\n",
      "Requirement already satisfied: rsa<5,>=3.1.4 in /opt/conda/lib/python3.7/site-packages (from google-auth>=1.2->gcsfs==0.6.1->-r requirements-rt2.1.txt (line 7)) (4.6)\n",
      "Requirement already satisfied: setuptools>=40.3.0 in /opt/conda/lib/python3.7/site-packages (from google-auth>=1.2->gcsfs==0.6.1->-r requirements-rt2.1.txt (line 7)) (51.1.0)\n",
      "Requirement already satisfied: pyasn1-modules>=0.2.1 in /opt/conda/lib/python3.7/site-packages (from google-auth>=1.2->gcsfs==0.6.1->-r requirements-rt2.1.txt (line 7)) (0.2.8)\n",
      "Requirement already satisfied: google-api-core<2.0.0dev,>=1.16.0 in /opt/conda/lib/python3.7/site-packages (from google-cloud-core<2.0dev,>=1.0.3->google-cloud-storage==1.23.0->-r requirements-rt2.1.txt (line 9)) (1.22.4)\n",
      "Requirement already satisfied: googleapis-common-protos<2.0dev,>=1.6.0 in /opt/conda/lib/python3.7/site-packages (from google-api-core<2.0.0dev,>=1.16.0->google-cloud-core<2.0dev,>=1.0.3->google-cloud-storage==1.23.0->-r requirements-rt2.1.txt (line 9)) (1.52.0)\n",
      "Requirement already satisfied: h5py in /opt/conda/lib/python3.7/site-packages (from keras-applications>=1.0.8->tensorflow==2.1.0->-r requirements-rt2.1.txt (line 1)) (3.1.0)\n",
      "Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /opt/conda/lib/python3.7/site-packages (from pyasn1-modules>=0.2.1->google-auth>=1.2->gcsfs==0.6.1->-r requirements-rt2.1.txt (line 7)) (0.4.8)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /opt/conda/lib/python3.7/site-packages (from requests->gcsfs==0.6.1->-r requirements-rt2.1.txt (line 7)) (1.26.2)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.7/site-packages (from requests->gcsfs==0.6.1->-r requirements-rt2.1.txt (line 7)) (2020.12.5)\n",
      "Requirement already satisfied: idna<3,>=2.5 in /opt/conda/lib/python3.7/site-packages (from requests->gcsfs==0.6.1->-r requirements-rt2.1.txt (line 7)) (2.10)\n",
      "Requirement already satisfied: chardet<5,>=3.0.2 in /opt/conda/lib/python3.7/site-packages (from requests->gcsfs==0.6.1->-r requirements-rt2.1.txt (line 7)) (4.0.0)\n",
      "Requirement already satisfied: werkzeug>=0.11.15 in /opt/conda/lib/python3.7/site-packages (from tensorboard<2.2.0,>=2.1.0->tensorflow==2.1.0->-r requirements-rt2.1.txt (line 1)) (1.0.1)\n",
      "Requirement already satisfied: markdown>=2.6.8 in /opt/conda/lib/python3.7/site-packages (from tensorboard<2.2.0,>=2.1.0->tensorflow==2.1.0->-r requirements-rt2.1.txt (line 1)) (3.3.3)\n",
      "Requirement already satisfied: requests-oauthlib>=0.7.0 in /opt/conda/lib/python3.7/site-packages (from google-auth-oauthlib->gcsfs==0.6.1->-r requirements-rt2.1.txt (line 7)) (1.3.0)\n",
      "Requirement already satisfied: importlib-metadata in /opt/conda/lib/python3.7/site-packages (from markdown>=2.6.8->tensorboard<2.2.0,>=2.1.0->tensorflow==2.1.0->-r requirements-rt2.1.txt (line 1)) (3.3.0)\n",
      "Requirement already satisfied: oauthlib>=3.0.0 in /opt/conda/lib/python3.7/site-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib->gcsfs==0.6.1->-r requirements-rt2.1.txt (line 7)) (3.1.0)\n",
      "Collecting cloudml-hypertune\n",
      "  Downloading cloudml-hypertune-0.1.0.dev6.tar.gz (3.2 kB)\n",
      "Requirement already satisfied: cached-property in /opt/conda/lib/python3.7/site-packages (from h5py->keras-applications>=1.0.8->tensorflow==2.1.0->-r requirements-rt2.1.txt (line 1)) (1.5.2)\n",
      "Requirement already satisfied: typing-extensions>=3.6.4 in /opt/conda/lib/python3.7/site-packages (from importlib-metadata->markdown>=2.6.8->tensorboard<2.2.0,>=2.1.0->tensorflow==2.1.0->-r requirements-rt2.1.txt (line 1)) (3.7.4.3)\n",
      "Requirement already satisfied: zipp>=0.5 in /opt/conda/lib/python3.7/site-packages (from importlib-metadata->markdown>=2.6.8->tensorboard<2.2.0,>=2.1.0->tensorflow==2.1.0->-r requirements-rt2.1.txt (line 1)) (3.4.0)\n",
      "Building wheels for collected packages: google-api-python-client, cloudml-hypertune\n",
      "  Building wheel for google-api-python-client (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for google-api-python-client: filename=google_api_python_client-1.7.11-py3-none-any.whl size=56530 sha256=7d6884c2b687fb86669161ec7f08e100b3ea57160e65627ac52b7f72ae46e4dd\n",
      "  Stored in directory: /tmp/pip-ephem-wheel-cache-diuucn_4/wheels/b6/04/f6/ae7211f4531abd8ac91e3fae5d19a4d8b52803362548ea9f66\n",
      "  Building wheel for cloudml-hypertune (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for cloudml-hypertune: filename=cloudml_hypertune-0.1.0.dev6-py2.py3-none-any.whl size=3988 sha256=d3c57cc268536741cf279981d70c280e0e2d91eeaef688c7046694324c1ecdce\n",
      "  Stored in directory: /tmp/pip-ephem-wheel-cache-diuucn_4/wheels/a7/ff/87/e7bed0c2741fe219b3d6da67c2431d7f7fedb183032e00f81e\n",
      "Successfully built google-api-python-client cloudml-hypertune\n",
      "Installing collected packages: numpy, scipy, tensorflow, scikit-learn, pandas, matplotlib, google-cloud-storage, google-cloud, google-api-python-client, gcsfs, cloudml-hypertune\n",
      "\u001b[33m  WARNING: The scripts f2py, f2py3 and f2py3.7 are installed in '/home/jupyter/.local/bin' which is not on PATH.\n",
      "  Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.\u001b[0m\n",
      "\u001b[33m  WARNING: The scripts estimator_ckpt_converter, saved_model_cli, tensorboard, tf_upgrade_v2, tflite_convert, toco and toco_from_protos are installed in '/home/jupyter/.local/bin' which is not on PATH.\n",
      "  Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.\u001b[0m\n",
      "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "witwidget 1.7.0 requires oauth2client>=4.1.3, but you have oauth2client 3.0.0 which is incompatible.\n",
      "tfx 0.21.4 requires kubernetes<11,>=10.0.1, but you have kubernetes 12.0.1 which is incompatible.\n",
      "tfx 0.21.4 requires pyarrow<0.16,>=0.15, but you have pyarrow 2.0.0 which is incompatible.\n",
      "tfx-bsl 0.21.4 requires pyarrow<0.16.0,>=0.15.0, but you have pyarrow 2.0.0 which is incompatible.\n",
      "tensorflow-model-analysis 0.21.6 requires pyarrow<1,>=0.15, but you have pyarrow 2.0.0 which is incompatible.\n",
      "tensorflow-data-validation 0.21.5 requires joblib<0.15,>=0.12, but you have joblib 1.0.0 which is incompatible.\n",
      "tensorflow-data-validation 0.21.5 requires pandas<1,>=0.24, but you have pandas 1.2.1 which is incompatible.\n",
      "tensorflow-data-validation 0.21.5 requires scikit-learn<0.22,>=0.18, but you have scikit-learn 0.22 which is incompatible.\n",
      "pandas-profiling 2.8.0 requires visions[type_image_path]==0.4.4, but you have visions 0.6.4 which is incompatible.\n",
      "explainable-ai-sdk 1.1.0 requires matplotlib>=3.2.2, but you have matplotlib 3.2.1 which is incompatible.\n",
      "apache-beam 2.17.0 requires httplib2<=0.12.0,>=0.8, but you have httplib2 0.18.1 which is incompatible.\n",
      "apache-beam 2.17.0 requires pyarrow<0.16.0,>=0.15.1; python_version >= \"3.0\" or platform_system != \"Windows\", but you have pyarrow 2.0.0 which is incompatible.\u001b[0m\n",
      "Successfully installed cloudml-hypertune-0.1.0.dev6 gcsfs-0.6.1 google-api-python-client-1.7.11 google-cloud-0.34.0 google-cloud-storage-1.23.0 matplotlib-3.2.1 numpy-1.18.0 pandas-1.2.1 scikit-learn-0.22 scipy-1.4.1 tensorflow-2.1.0\n"
     ]
    }
   ],
   "source": [
    "# Install packages\n",
    "# https://cloud.google.com/ai-platform/training/docs/runtime-version-list\n",
    "!pip3 install -r requirements-rt2.1.txt --user --no-cache-dir --use-feature=2020-resolver\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TF Version=  2.1.0\n",
      "Keras Version=  2.2.4-tf\n"
     ]
    }
   ],
   "source": [
    "# Import packages\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "import os\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '2' \n",
    "#0 = all messages are logged (default behavior)\n",
    "#1 = INFO messages are not printed\n",
    "#2 = INFO and WARNING messages are not printed\n",
    "#3 = INFO, WARNING, and ERROR messages are not printed\n",
    "\n",
    "import numpy as np\n",
    "from google.cloud import storage\n",
    "import tensorflow as tf\n",
    "#import matplotlib.pyplot as plt\n",
    "#from tensorflow.keras import models\n",
    "\n",
    "print(\"TF Version= \", tf.__version__)\n",
    "print(\"Keras Version= \", tf.keras.__version__)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Utility functions\n",
    "\n",
    "#------\n",
    "def find_best_model_dir(model_dir, offset=1, maxFlag=1):\n",
    "    # Get a list of model directories\n",
    "    all_models = ! gsutil ls $model_dir\n",
    "    print(\"\")\n",
    "    print(\"All Models = \")\n",
    "    print(*all_models, sep='\\n')\n",
    "\n",
    "    # Check if model dirs exist\n",
    "    if ((\"CommandException\" in all_models[0]) or (len(all_models) <= 1)):\n",
    "        print(\"Create the models first.\")\n",
    "        return \"\"\n",
    "\n",
    "    # Find the best model from checkpoints\n",
    "    import re\n",
    "    best_acc = -np.Inf\n",
    "    if (maxFlag != 1):\n",
    "        best_acc = np.Inf\n",
    "    best_model_dir = \"\"\n",
    "    tup_list = []\n",
    "    for i in range(1,len(all_models)):\n",
    "        all_floats = re.findall(r\"[-+]?\\d*\\.\\d+|\\d+\", all_models[i]) #Find the floats in the string\n",
    "        cur_acc = -float(all_floats[-offset]) #which item is the model optimization metric\n",
    "        tup_list.append([all_models[i],cur_acc])\n",
    "        if (maxFlag*(cur_acc > best_acc) or (1-maxFlag)*(cur_acc < best_acc)):\n",
    "            best_acc = cur_acc\n",
    "            best_model_dir = all_models[i]\n",
    "    if maxFlag:\n",
    "        tup_list.sort(key=lambda tup: tup[1], reverse=False)\n",
    "    else:\n",
    "        tup_list.sort(key=lambda tup: tup[1], reverse=True)\n",
    "    for i in range(len(tup_list)):\n",
    "        print(tup_list[i][0])\n",
    "    print(\"Best Accuracy  from Checkpoints = \", best_acc)\n",
    "    print(\"Best Model Dir from Checkpoints = \", best_model_dir)\n",
    "    \n",
    "    return best_model_dir\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from oauth2client.client import GoogleCredentials\n",
    "from googleapiclient import discovery\n",
    "from googleapiclient import errors\n",
    "import json\n",
    "\n",
    "#------\n",
    "# Python module to get the best hypertuned model parameters\n",
    "def pyth_get_hypertuned_parameters(project_name, job_name, maxFlag):\n",
    "    # Define the credentials for the service account\n",
    "    #credentials = service_account.Credentials.from_service_account_file(<PATH TO CREDENTIALS JSON>)\n",
    "    credentials = GoogleCredentials.get_application_default()\n",
    "\n",
    "    # Define the project id and the job id and format it for the api request\n",
    "    project_id = 'projects/{}'.format(project_name)\n",
    "    job_id = '{}/jobs/{}'.format(project_id, job_name)\n",
    "\n",
    "    # Build the service\n",
    "    cloudml = discovery.build('ml', 'v1', cache_discovery=False, credentials=credentials)\n",
    "\n",
    "    # Execute the request and pass in the job id\n",
    "    request = cloudml.projects().jobs().get(name=job_id)\n",
    "\n",
    "    try:\n",
    "        response = request.execute()\n",
    "        # Handle a successful request\n",
    "    except errors.HttpError as err:\n",
    "        tf.compat.v1.logging.error('There was an error getting the hyperparameters. Check the details:')\n",
    "        tf.compat.v1.logging.error(err._get_reason())\n",
    "\n",
    "    # Get just the best hp values\n",
    "    if maxFlag:\n",
    "        best_model = response['trainingOutput']['trials'][0]\n",
    "    else:\n",
    "        best_model = response['trainingOutput']['trials'][-1]\n",
    "    #print('Best Hyperparameters:')\n",
    "    #print(json.dumps(best_model, indent=4))\n",
    "\n",
    "    nTrials = len(response['trainingOutput']['trials'])\n",
    "    for i in range(0,nTrials):\n",
    "        state = response['trainingOutput']['trials'][i]['state']\n",
    "        trialId = response['trainingOutput']['trials'][i]['trialId']\n",
    "        objV = -1\n",
    "        if (state == 'SUCCEEDED'):\n",
    "            objV = response['trainingOutput']['trials'][i]['finalMetric']['objectiveValue']\n",
    "        print('objective=', objV, ' trialId=', trialId, state)\n",
    "        d = response['trainingOutput']['trials'][i]['hyperparameters']\n",
    "        for key, value in d.items():\n",
    "            print('    ', key, value)\n",
    "    return best_model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot original image overlaid by the fault labels\n",
    "# image_array is np.array(num_images, x_size, y_size)\n",
    "# label_array is np.array(num_images, x_size, y_size)\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def plot_images(image_array, label_array):\n",
    "    fig, axarr = plt.subplots(4,4, figsize=(16, 16))\n",
    "    for i in range(4):\n",
    "        for j in range(4):\n",
    "            axarr[i,j].set_title('Image-'+str(4*i+j))\n",
    "            axarr[i,j].imshow(image_array[4*i+j,:,:], cmap='gray')\n",
    "            axarr[i,j].imshow(label_array[4*i+j,:,:], alpha=0.3)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Your current project:\n",
      "img-seg-3d\n"
     ]
    }
   ],
   "source": [
    "!echo \"Your current project:\"\n",
    "!gcloud config list project --format \"value(core.project)\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "USER = 'elvinzhu' # Change it to your user name\n",
    "PROJECT_ID = 'img-seg-3d' # Change it to your project id\n",
    "BUCKET_NAME = 'tuti_asset' # Change it to your bucket name\n",
    "FOLDER_NAME = 'tf-train-job' # Change it to your folder name\n",
    "REGION = 'us-central1'\n",
    "RUNTIME_VERSION = 2.1\n",
    "JOB_DIR   = 'gs://' + BUCKET_NAME + '/' + FOLDER_NAME + '/jobdir'\n",
    "MODEL_DIR = 'gs://' + BUCKET_NAME + '/' + FOLDER_NAME + '/models'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[core]\n",
      "account = 122476304848-compute@developer.gserviceaccount.com\n",
      "disable_usage_reporting = True\n",
      "project = img-seg-3d\n",
      "\n",
      "Your active configuration is: [default]\n"
     ]
    }
   ],
   "source": [
    "!gcloud config list\n",
    "#!gcloud config config-helper --format \"value(configuration.properties.core.project)\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CommandException: 1 files/objects could not be removed.\n",
      "CommandException: 1 files/objects could not be removed.\n"
     ]
    }
   ],
   "source": [
    "# Clean old job logs, job packages and models\n",
    "!gsutil -m -q rm $JOB_DIR/packages/**\n",
    "!gsutil -m -q rm $MODEL_DIR/model**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# UNET-FPN32-sl1 Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the tf_trainer directory and load the trainer files in it\n",
    "!mkdir -p trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting ./trainer/inputs.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile ./trainer/inputs.py\n",
    "\n",
    "# Create the train and label lists\n",
    "import math\n",
    "import numpy as np\n",
    "#import imageio\n",
    "#from google.cloud import storage\n",
    "import io\n",
    "import tensorflow as tf\n",
    "from PIL import Image\n",
    "\n",
    "#------\n",
    "def readImage(image_path):\n",
    "    img = tf.io.gfile.GFile(image_path, 'rb').read()\n",
    "    img = io.BytesIO(img)\n",
    "    img = np.array(Image.open(img).convert('L')).astype(dtype='int32')\n",
    "    height, width = img.shape\n",
    "    # clipping \n",
    "    h = int(height/16)*16\n",
    "    w = int(width/16)*16\n",
    "    image = img[:h, :w]\n",
    "    return image\n",
    "\n",
    "#------\n",
    "def load_data(data_file_path: str, label_file_path: str, rangeIndices, batch_size) -> tf.data.Dataset:\n",
    "    images = []\n",
    "    for i in rangeIndices:\n",
    "        im = readImage(data_file_path+'/image_inline_i%04d.png' % i)\n",
    "        im = np.array(im).astype(dtype='float32')/255\n",
    "        (h,w) = im.shape\n",
    "        im = np.reshape(im, (h,w,1))\n",
    "        images.append(im)\n",
    "\n",
    "    labels = []\n",
    "    for i in rangeIndices:\n",
    "        im = readImage(label_file_path+'/image_inline_i%04d.png' % i)\n",
    "        im = np.array(im).astype(dtype='float32')\n",
    "        (h,w) = im.shape\n",
    "        im = np.reshape(im, (h,w,1))\n",
    "        labels.append(im)\n",
    "\n",
    "    #for items in images:\n",
    "    #    print(items)\n",
    "\n",
    "    seismic = np.array(images)\n",
    "    label = np.array(labels)\n",
    "    print(\"Data  Shape = \", images.shape)\n",
    "    print(\"Label Shape = \", label.shape)\n",
    "    \n",
    "    #return images, labels\n",
    "    \n",
    "    dataset = tf.data.Dataset.from_tensor_slices((images, labels))\n",
    "    dataset = dataset.shuffle(100).batch(batch_size)\n",
    "    \n",
    "    return dataset\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting ./trainer/model.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile ./trainer/model.py\n",
    "\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "from tensorflow.keras import backend as K\n",
    "from tensorflow.keras import Input, layers, models, regularizers\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Layer\n",
    "from tensorflow.keras.activations import softmax\n",
    "\n",
    "\n",
    "def tf_model():\n",
    "    return\n",
    "\n",
    "def custom_loss(y_true, y_pred):\n",
    "    return custom_loss\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Package for distributed training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting ./setup.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile ./setup.py\n",
    "\n",
    "# python3\n",
    "\n",
    "# ==============================================================================\n",
    "# Copyright 2020 Google LLC. This software is provided as-is, without warranty\n",
    "# or representation for any use or purpose. Your use of it is subject to your\n",
    "# agreement with Google.\n",
    "# ==============================================================================\n",
    "\n",
    "from setuptools import find_packages\n",
    "from setuptools import setup\n",
    "\n",
    "#Runtime 2.2\n",
    "#REQUIRED_PACKAGES = ['tensorflow==2.3.1',\n",
    "#                     'pandas==1.0.4',\n",
    "#                     'scikit-learn==0.23.1',\n",
    "#                     'google-cloud-storage==1.29.0',\n",
    "#                     'cloudml-hypertune',\n",
    "#                     'pillow',\n",
    "#                    ]\n",
    "#Runtime 2.1\n",
    "REQUIRED_PACKAGES = ['tensorflow==2.1.0',\n",
    "                     'pandas==0.25.3',\n",
    "                     'scikit-learn==0.22',\n",
    "                     'google-cloud-storage==1.23.0',\n",
    "                     'cloudml-hypertune',\n",
    "                     'pillow',\n",
    "                    ]\n",
    "setup(\n",
    "    name='trainer',\n",
    "    version='0.1',\n",
    "    install_requires=REQUIRED_PACKAGES,\n",
    "    packages=find_packages(),\n",
    "    include_package_data=True,\n",
    "    description='Trainer package for Tensorflow Task'\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting ./trainer/__init__.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile ./trainer/__init__.py\n",
    "# python3\n",
    "\n",
    "# ==============================================================================\n",
    "# Copyright 2020 Google LLC. This software is provided as-is, without warranty\n",
    "# or representation for any use or purpose. Your use of it is subject to your\n",
    "# agreement with Google.\n",
    "# ==============================================================================\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting ./trainer/train.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile ./trainer/train.py\n",
    "\n",
    "# python3\n",
    "# ==============================================================================\n",
    "# Copyright 2020 Google LLC. This software is provided as-is, without warranty\n",
    "# or representation for any use or purpose. Your use of it is subject to your\n",
    "# agreement with Google.\n",
    "# ==============================================================================\n",
    "\n",
    "import os\n",
    "import json\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import datetime as datetime\n",
    "from pytz import timezone\n",
    "import hypertune\n",
    "import argparse\n",
    "from trainer import model\n",
    "from trainer import inputs\n",
    "\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "import os\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3' \n",
    "#0 = all messages are logged (default behavior)\n",
    "#1 = INFO messages are not printed\n",
    "#2 = INFO and WARNING messages are not printed\n",
    "#3 = INFO, WARNING, and ERROR messages are not printed\n",
    "\n",
    "\n",
    "def parse_arguments():\n",
    "    \"\"\"Argument parser.\n",
    "    Returns:\n",
    "      Dictionary of arguments.\n",
    "    \"\"\"\n",
    "\n",
    "    parser = argparse.ArgumentParser()\n",
    "    parser.add_argument('--depth', default=5, type=int, \n",
    "                        help='Hyperparameter: depth of net')\n",
    "    parser.add_argument('--dropout_rate', default=0.2, type=float, \n",
    "                        help='Hyperparameter: Drop out rate')\n",
    "    parser.add_argument('--learning_rate', default=0.00005, type=float, \n",
    "                        help='Hyperparameter: initial learning rate')\n",
    "    parser.add_argument('--batch_size', default=1, type=int, \n",
    "                        help='batch size of the deep network')\n",
    "    parser.add_argument('--epochs', default=2, type=int, \n",
    "                        help='epoch.')\n",
    "    parser.add_argument('--num_samples', default=3000, type=int,\n",
    "                        help='Number of training samples to use.')\n",
    "    parser.add_argument('--model_dir', default=\"\",\n",
    "                        help='Directory to store models and logs.')\n",
    "    parser.add_argument('--verbosity', choices=['DEBUG','ERROR','FATAL','INFO','WARN'],\n",
    "                        default='FATAL')\n",
    "    args, _ = parser.parse_known_args()\n",
    "    return args\n",
    "\n",
    "\n",
    "def get_callbacks(args, early_stop_patience: int = 3):\n",
    "    \"\"\"Creates Keras callbacks for model training.\"\"\"\n",
    "\n",
    "    # Get trialId\n",
    "    trialId = json.loads(os.environ.get(\"TF_CONFIG\", \"{}\")).get(\"task\", {}).get(\"trial\", \"\")\n",
    "    if trialId == '':\n",
    "        trialId = '0'\n",
    "    print(\"trialId=\", trialId)\n",
    "\n",
    "    curTime = datetime.datetime.now(timezone('US/Pacific')).strftime('%H%M%S')\n",
    "    \n",
    "    # Modify model_dir paths to include trialId\n",
    "    model_dir = args.model_dir + \"/checkpoints/cp-\"+curTime+\"-\"+trialId+\"-{custom_mse:.4f}\"\n",
    "    log_dir   = args.model_dir + \"/log_dir\"\n",
    "\n",
    "    tensorboard_cb = tf.keras.callbacks.TensorBoard(log_dir, histogram_freq=1)\n",
    "    checkpoint_cb  = tf.keras.callbacks.ModelCheckpoint(model_dir, monitor='custom_mse', mode='min', \n",
    "                                                        verbose=0, save_best_only=True,\n",
    "                                                        save_weights_only=False)\n",
    "    earlystop_cb   = tf.keras.callbacks.EarlyStopping(monitor='loss', patience=3)\n",
    "\n",
    "    return [checkpoint_cb, tensorboard_cb, earlystop_cb]\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "\n",
    "    # ---------------------------------------\n",
    "    # Parse Arguments\n",
    "    # ---------------------------------------\n",
    "    args = parse_arguments()\n",
    "    #args.model_dir = MODEL_DIR + datetime.datetime.now(timezone('US/Pacific')).strftime('/model_%m%d%Y_%H%M')\n",
    "    print(args)\n",
    "\n",
    "    #tf.compat.v1.logging.set_verbosity(tf.compat.v1.logging.ERROR)  # or any {DEBUG, INFO, WARN, ERROR, FATAL}\n",
    "\n",
    "    # ---------------------------------------\n",
    "    # Input Data & Preprocessing\n",
    "    # ---------------------------------------\n",
    "    print(\"Input and pre-process data ...\")\n",
    "    # Extract train_seismic, train_label\n",
    "    data_dir = 'gs://../images'\n",
    "    label_dir = 'gs://../labels'\n",
    "    train_dataset = inputs.load_data(data_dir, label_dir, range(0,args.num_samples), args.batch_size)\n",
    "    \n",
    "    # ---------------------------------------\n",
    "    # Train model\n",
    "    # ---------------------------------------\n",
    "    print(\"Creating model ...\")\n",
    "    tf_model = model.tf_model(depth=args.depth,\n",
    "                              dropout_rate=args.dropout_rate)\n",
    "    tf_model.compile(optimizer=tf.keras.optimizers.Adam(lr=args.learning_rate),\n",
    "                     loss=model.custom_loss,  # loss is the custom loss I shared with you\n",
    "                     metrics=[model.custom_mse])\n",
    "    \n",
    "    print(\"Fitting model ...\")\n",
    "    callbacks = get_callbacks(args, 3)\n",
    "    history = tf_model.fit(train_dataset, \n",
    "                           epochs=args.epochs,\n",
    "                           validation_split = 0.0,\n",
    "                           callbacks=callbacks)\n",
    "\n",
    "    # TBD save history for visualization\n",
    "\n",
    "    final_epoch_accuracy = history.history['custom_mse'][-1]\n",
    "    final_epoch_count = len(history.history['custom_mse'])\n",
    "\n",
    "    print('final_epoch_accuracy = %.6f' % final_epoch_accuracy)\n",
    "    print('final_epoch_count = %02d' % final_epoch_count)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DEPTH = 07\n",
      "DROPOUT_RATE = 0.1500\n",
      "LEARNING_RATE = 0.000050\n",
      "EPOCHS = 02\n",
      "BATCH_SIZE = 01\n",
      "NUM_SAMPLES = 3000\n",
      "MODEL_DIR = gs://tuti_asset/tf-train-job/models/model_01302021_2240\n",
      "Namespace(batch_size=1, depth=7, dropout_rate=0.15000017803209056, epochs=2, learning_rate=5e-05, model_dir='gs://tuti_asset/tf-train-job/models/model_01302021_2240', num_samples=3000, verbosity='FATAL')\n",
      "Input and pre-process data ...\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/conda/lib/python3.7/runpy.py\", line 193, in _run_module_as_main\n",
      "    \"__main__\", mod_spec)\n",
      "  File \"/opt/conda/lib/python3.7/runpy.py\", line 85, in _run_code\n",
      "    exec(code, run_globals)\n",
      "  File \"/home/jupyter/tuti-repo/trainer/train.py\", line 101, in <module>\n",
      "    train_dataset = inputs.load_data(data_dir, label_dir, range(0,args.num_samples), args.batch_size)\n",
      "  File \"/home/jupyter/tuti-repo/trainer/inputs.py\", line 27, in load_data\n",
      "    im = readImage(data_file_path+'/image_inline_i%04d.png' % i)\n",
      "  File \"/home/jupyter/tuti-repo/trainer/inputs.py\", line 13, in readImage\n",
      "    img = tf.io.gfile.GFile(image_path, 'rb').read()\n",
      "  File \"/home/jupyter/.local/lib/python3.7/site-packages/tensorflow_core/python/lib/io/file_io.py\", line 124, in read\n",
      "    length = self.size() - self.tell()\n",
      "  File \"/home/jupyter/.local/lib/python3.7/site-packages/tensorflow_core/python/lib/io/file_io.py\", line 102, in size\n",
      "    return stat(self.__name).length\n",
      "  File \"/home/jupyter/.local/lib/python3.7/site-packages/tensorflow_core/python/lib/io/file_io.py\", line 729, in stat\n",
      "    return stat_v2(filename)\n",
      "  File \"/home/jupyter/.local/lib/python3.7/site-packages/tensorflow_core/python/lib/io/file_io.py\", line 746, in stat_v2\n",
      "    pywrap_tensorflow.Stat(compat.as_bytes(path), file_statistics)\n",
      "tensorflow.python.framework.errors_impl.NotFoundError: Error executing an HTTP request: HTTP response code 404 with body 'Not Found'\n",
      "\t when reading gs://../images/image_inline_i0000.png/\n",
      "CPU times: user 47.6 ms, sys: 30.4 ms, total: 78 ms\n",
      "Wall time: 2.49 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# Run the training manually\n",
    "# Training parameters\n",
    "from datetime import datetime\n",
    "from pytz import timezone\n",
    "\n",
    "DEPTH = 7\n",
    "DROPOUT_RATE = 0.15000017803209056\n",
    "LEARNING_RATE = 0.00005\n",
    "EPOCHS = 2\n",
    "BATCH_SIZE = 1\n",
    "NUM_SAMPLES = 3000\n",
    "\n",
    "MODEL_DIR_PYTH = MODEL_DIR + datetime.now(timezone('US/Pacific')).strftime('/model_%m%d%Y_%H%M')\n",
    "\n",
    "print('DEPTH = %02d' % DEPTH)\n",
    "print('DROPOUT_RATE = %.4f' % DROPOUT_RATE)\n",
    "print('LEARNING_RATE = %.6f' % LEARNING_RATE)\n",
    "print('EPOCHS = %02d' % EPOCHS)\n",
    "print('BATCH_SIZE = %02d' % BATCH_SIZE)\n",
    "print('NUM_SAMPLES = %d' % NUM_SAMPLES)\n",
    "print(\"MODEL_DIR =\", MODEL_DIR_PYTH)\n",
    "\n",
    "# Run training\n",
    "! python3 -m trainer.train \\\n",
    "--depth=$DEPTH \\\n",
    "--dropout_rate=$DROPOUT_RATE \\\n",
    "--learning_rate=$LEARNING_RATE \\\n",
    "--epochs=$EPOCHS \\\n",
    "--batch_size=$BATCH_SIZE \\\n",
    "--num_samples=$NUM_SAMPLES \\\n",
    "--model_dir=$MODEL_DIR_PYTH\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test with latest saved model\n",
    "best_model_dir_pyth = find_best_model_dir(MODEL_DIR_PYTH+'/checkpoints', offset=1, maxFlag=0)\n",
    "#acc = test_saved_model(best_model_dir_pyth, 0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "from trainer import model\n",
    "\n",
    "# Copy the model from storage to local memory\n",
    "!gsutil -m cp -r $best_model_dir_pyth* ./model_dir\n",
    "\n",
    "# Load the model\n",
    "loaded_model = tf.keras.models.load_model('./model_dir', compile=False, \n",
    "               custom_objects={\"custom_loss\": model.custom_loss, \"custom_mse\": model.custom_mse})\n",
    "print(\"Signature \", loaded_model.signatures)\n",
    "print(\"\")\n",
    "\n",
    "# Display model\n",
    "tf.keras.utils.plot_model(loaded_model, show_shapes=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "------\n",
    "# Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the config directory and load the trainer files in it\n",
    "!mkdir -p config\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile ./config/config.yaml\n",
    "\n",
    "# python3\n",
    "# ==============================================================================\n",
    "# Copyright 2020 Google LLC. This software is provided as-is, without warranty\n",
    "# or representation for any use or purpose. Your use of it is subject to your\n",
    "# agreement with Google.\n",
    "# ==============================================================================\n",
    "\n",
    "# https://cloud.google.com/sdk/gcloud/reference/ai-platform/jobs/submit/training#--scale-tier\n",
    "# https://www.kaggle.com/c/passenger-screening-algorithm-challenge/discussion/37087\n",
    "# https://cloud.google.com/ai-platform/training/docs/using-gpus\n",
    "\n",
    "#trainingInput:\n",
    "#  scaleTier: CUSTOM\n",
    "#  masterType: n1-highmem-8\n",
    "#  masterConfig:\n",
    "#    acceleratorConfig:\n",
    "#      count: 1\n",
    "#      type: NVIDIA_TESLA_T4\n",
    "\n",
    "#  masterType: n1-highcpu-16\n",
    "#  workerType: cloud_tpu\n",
    "#  workerCount: 1\n",
    "#  workerConfig:\n",
    "#    acceleratorConfig:\n",
    "#      type: TPU_V3\n",
    "#      count: 8\n",
    "\n",
    "#trainingInput:\n",
    "#  scaleTier: CUSTOM\n",
    "#  masterType: complex_model_m\n",
    "#  workerType: complex_model_m\n",
    "#  parameterServerType: large_model\n",
    "#  workerCount: 6\n",
    "#  parameterServerCount: 1\n",
    "#  scheduling:\n",
    "#    maxWaitTime: 3600s\n",
    "#    maxRunningTime: 7200s\n",
    "\n",
    "#trainingInput:\n",
    "#  runtimeVersion: \"2.2\"\n",
    "#  scaleTier: CUSTOM\n",
    "#  masterType: standard_gpu\n",
    "#  workerCount: 9\n",
    "#  workerType: standard_gpu\n",
    "#  parameterServerCount: 3\n",
    "#  parameterServerType: standard\n",
    "\n",
    "#trainingInput:\n",
    "#  scaleTier: BASIC-GPU\n",
    "    \n",
    "#trainingInput:\n",
    "#  region: us-central1\n",
    "#  scaleTier: CUSTOM\n",
    "#  masterType: complex_model_m\n",
    "#  workerType: complex_model_m_gpu\n",
    "#  parameterServerType: large_model\n",
    "#  workerCount: 4\n",
    "#  parameterServerCount: 2\n",
    "\n",
    "trainingInput:\n",
    "    scaleTier: CUSTOM\n",
    "    masterType: n1-highmem-16\n",
    "    masterConfig:\n",
    "        acceleratorConfig:\n",
    "            count: 2\n",
    "            type: NVIDIA_TESLA_V100\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "from pytz import timezone\n",
    "JOBNAME_TRN = 'tf_train_'+ USER + '_' + \\\n",
    "           datetime.now(timezone('US/Pacific')).strftime(\"%m%d%y_%H%M\")\n",
    "JOB_CONFIG = \"config/config.yaml\"\n",
    "MODEL_DIR_TRN = MODEL_DIR + datetime.now(timezone('US/Pacific')).strftime('/model_%m%d%Y_%H%M')\n",
    "print(\"Job Name = \", JOBNAME_TRN)\n",
    "print(\"Job Dir  = \", JOB_DIR)\n",
    "print(\"MODEL_DIR =\", MODEL_DIR_TRN)\n",
    "\n",
    "# Training parameters\n",
    "DEPTH = 6\n",
    "DROPOUT_RATE = 0.15\n",
    "#DEPTH = 5\n",
    "#DROPOUT_RATE = 0.2\n",
    "LEARNING_RATE = 0.00005\n",
    "EPOCHS = 2\n",
    "BATCH_SIZE = 1\n",
    "NUM_SAMPLES = 3000\n",
    "\n",
    "print('DEPTH = %02d' % DEPTH)\n",
    "print('DROPOUT_RATE = %.4f' % DROPOUT_RATE)\n",
    "print('LEARNING_RATE = %.6f' % LEARNING_RATE)\n",
    "print('EPOCHS = %02d' % EPOCHS)\n",
    "print('BATCH_SIZE = %02d' % BATCH_SIZE)\n",
    "print('NUM_SAMPLES = %d' % NUM_SAMPLES)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://cloud.google.com/sdk/gcloud/reference/ai-platform/jobs/submit/training\n",
    "\n",
    "TRAIN_LABELS = \"mode=train,owner=\"+USER\n",
    "\n",
    "# submit the training job\n",
    "! gcloud ai-platform jobs submit training $JOBNAME_TRN \\\n",
    "  --package-path $(pwd)/trainer \\\n",
    "  --module-name trainer.train \\\n",
    "  --region $REGION \\\n",
    "  --python-version 3.7 \\\n",
    "  --runtime-version $RUNTIME_VERSION \\\n",
    "  --job-dir $JOB_DIR \\\n",
    "  --config $JOB_CONFIG \\\n",
    "  --labels $TRAIN_LABELS \\\n",
    "  -- \\\n",
    "  --depth=$DEPTH \\\n",
    "  --dropout_rate=$DROPOUT_RATE \\\n",
    "  --learning_rate=$LEARNING_RATE \\\n",
    "  --epochs=$EPOCHS \\\n",
    "  --batch_size=$BATCH_SIZE \\\n",
    "  --num_samples=$NUM_SAMPLES \\\n",
    "  --model_dir=$MODEL_DIR_TRN\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check the training job status\n",
    "! gcloud ai-platform jobs describe $JOBNAME_TRN\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print Errors\n",
    "response = ! gcloud logging read \"resource.labels.job_id=$JOBNAME_TRN severity>=ERROR\"\n",
    "for i in range(0,len(response)):\n",
    "    if 'message' in response[i]:\n",
    "        print(response[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test with latest saved model\n",
    "best_model_dir_trn = find_best_model_dir(MODEL_DIR_TRN+'/checkpoints', offset=1, maxFlag=0)\n",
    "#acc = test_saved_model(best_model_dir_trn, 0)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "------\n",
    "# Hyper Parameter Tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the tf directory and load the trainer files in it\n",
    "!cp ./trainer/train.py ./trainer/train_hpt.py\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile -a ./trainer/train_hpt.py\n",
    "\n",
    "    \"\"\"This method updates a CAIP HPTuning Job with a final metric for the job.\n",
    "    In TF2.X the user must either use hypertune or a custom callback with\n",
    "    tf.summary.scalar to update CAIP HP Tuning jobs. This function uses\n",
    "    hypertune, which appears to be the preferred solution. Hypertune also works\n",
    "    with containers, without code change.\n",
    "    Args:\n",
    "        metric_tag: The metric being optimized.  This MUST MATCH the\n",
    "          hyperparameterMetricTag specificed in the hyperparameter tuning yaml.\n",
    "        metric_value: The value to report at the end of model training.\n",
    "        global_step: An int value to specify the number of trainin steps completed\n",
    "          at the time the metric was reported.\n",
    "    \"\"\"\n",
    "\n",
    "    hpt = hypertune.HyperTune()\n",
    "    hpt.report_hyperparameter_tuning_metric(\n",
    "        hyperparameter_metric_tag='custom_mse',\n",
    "        metric_value=final_epoch_accuracy,\n",
    "        global_step=final_epoch_count\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile ./config/hptuning_config.yaml\n",
    "\n",
    "# python3\n",
    "# ==============================================================================\n",
    "# Copyright 2020 Google LLC. This software is provided as-is, without warranty\n",
    "# or representation for any use or purpose. Your use of it is subject to your\n",
    "# agreement with Google.\n",
    "# ==============================================================================\n",
    "\n",
    "# https://cloud.google.com/ai-platform/training/docs/reference/rest/v1/projects.jobs\n",
    "\n",
    "#trainingInput:\n",
    "#  scaleTier: CUSTOM\n",
    "#  masterType: n1-highmem-8\n",
    "#  masterConfig:\n",
    "#    acceleratorConfig:\n",
    "#      count: 1\n",
    "#      type: NVIDIA_TESLA_T4\n",
    "#\n",
    "#  masterType: standard_p100\n",
    "#  workerType: standard_p100\n",
    "#  parameterServerType: standard_p100\n",
    "#  workerCount: 8\n",
    "#  parameterServerCount: 1\n",
    "#  runtimeVersion: $RUNTIME_VERSION\n",
    "#  pythonVersion: '3.7'\n",
    "\n",
    "#trainingInput:\n",
    "#  scaleTier: CUSTOM\n",
    "#  masterType: complex_model_m\n",
    "#  workerType: complex_model_m\n",
    "#  parameterServerType: large_model\n",
    "#  workerCount: 9\n",
    "#  parameterServerCount: 3\n",
    "#  scheduling:\n",
    "#    maxWaitTime: 3600s\n",
    "#    maxRunningTime: 7200s\n",
    "\n",
    "#trainingInput:\n",
    "#  scaleTier: BASIC-GPU\n",
    "\n",
    "trainingInput:\n",
    "  scaleTier: CUSTOM\n",
    "  masterType: n1-highmem-16\n",
    "  masterConfig:\n",
    "    acceleratorConfig:\n",
    "      count: 2\n",
    "      type: NVIDIA_TESLA_V100\n",
    "  hyperparameters:\n",
    "    goal: MAXIMIZE\n",
    "    hyperparameterMetricTag: custom_mse\n",
    "    maxTrials: 32\n",
    "    maxParallelTrials: 4\n",
    "    enableTrialEarlyStopping: True\n",
    "    params:\n",
    "    - parameterName: depth\n",
    "      type: INTEGER\n",
    "      minValue: 4\n",
    "      maxValue: 6\n",
    "      scaleType: UNIT_LINEAR_SCALE\n",
    "    - parameterName: dropout_rate\n",
    "      type: DOUBLE\n",
    "      minValue: 0.1\n",
    "      maxValue: 0.3\n",
    "      scaleType: UNIT_LINEAR_SCALE\n",
    "    - parameterName: n_segmentation_levels\n",
    "      type: DISCRETE\n",
    "      discreteValues:\n",
    "      - 2\n",
    "      - 3\n",
    "    - parameterName: n_base_filters\n",
    "      type: DISCRETE\n",
    "      discreteValues:\n",
    "      - 8\n",
    "      - 12\n",
    "      - 16\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "from pytz import timezone\n",
    "\n",
    "JOBNAME_HPT = 'tf_hptrn_' + USER + '_' + \\\n",
    "              datetime.now(timezone('US/Pacific')).strftime(\"%m%d%y_%H%M\")\n",
    "JOB_CONFIG = \"./config/hptuning_config.yaml\"\n",
    "MODEL_DIR_HPT = MODEL_DIR + datetime.now(timezone('US/Pacific')).strftime('/model_%m%d%Y_%H%M')\n",
    "print(\"Job Name = \", JOBNAME_HPT)\n",
    "print(\"Job Dir  = \", JOB_DIR)\n",
    "print(\"MODEL_DIR =\", MODEL_DIR_HPT)\n",
    "\n",
    "# Training parameters\n",
    "LEARNING_RATE = 0.00005\n",
    "EPOCHS = 2\n",
    "BATCH_SIZE = 1\n",
    "NUM_SAMPLES = 3000\n",
    "\n",
    "print('LEARNING_RATE = %.6f' % LEARNING_RATE)\n",
    "print('EPOCHS = %02d' % EPOCHS)\n",
    "print('BATCH_SIZE = %02d' % BATCH_SIZE)\n",
    "print('NUM_SAMPLES = %d' % NUM_SAMPLES)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# submit the training job\n",
    "HT_LABELS = \"mode=hypertrain,owner=\"+USER\n",
    "\n",
    "! gcloud ai-platform jobs submit training $JOBNAME_HPT \\\n",
    "  --package-path $(pwd)/trainer \\\n",
    "  --module-name trainer.train_hpt \\\n",
    "  --python-version 3.7 \\\n",
    "  --runtime-version $RUNTIME_VERSION \\\n",
    "  --region $REGION \\\n",
    "  --job-dir $JOB_DIR \\\n",
    "  --config $JOB_CONFIG \\\n",
    "  --labels $HT_LABELS \\\n",
    "  -- \\\n",
    "  --learning_rate=$LEARNING_RATE \\\n",
    "  --epochs=$EPOCHS \\\n",
    "  --batch_size=$BATCH_SIZE \\\n",
    "  --num_samples=$NUM_SAMPLES \\\n",
    "  --model_dir=$MODEL_DIR_HPT\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check the hyperparameter training job status\n",
    "! gcloud ai-platform jobs describe $JOBNAME_HPT\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print Errors\n",
    "response = ! gcloud logging read \"resource.labels.job_id=$JOBNAME_HPT severity>=ERROR\"\n",
    "for i in range(0,len(response)):\n",
    "    if 'message' in response[i]:\n",
    "        print(response[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the best model parameters from Cloud API\n",
    "best_model = pyth_get_hypertuned_parameters(PROJECT_ID, JOBNAME_HPT, 0)\n",
    "DEPTH = best_model['hyperparameters']['depth']\n",
    "DROPOUT_RATE = best_model['hyperparameters']['dropout_rate']\n",
    "N_SEG_LEVELS = best_model['hyperparameters']['n_segmentation_levels']\n",
    "N_BASE_FILTERS = best_model['hyperparameters']['n_base_filters']\n",
    "print('')\n",
    "print('Objective=', best_model['finalMetric']['objectiveValue'])\n",
    "print('DEPTH =', DEPTH)\n",
    "print('DROPOUT_RATE =', DROPOUT_RATE)\n",
    "print('N_SEG_LEVELS =', N_SEG_LEVELS)\n",
    "print('N_BASE_FILTERS =', N_BASE_FILTERS)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find count of checkpoints\n",
    "all_models = ! gsutil ls {MODEL_DIR_HPT+'/checkpoints'}\n",
    "print(\"Total Hypertrained Models=\", len(all_models))\n",
    "\n",
    "# Test with latest saved model\n",
    "best_model_dir_hyp = find_best_model_dir(MODEL_DIR_HPT+'/checkpoints', offset=1, maxFlag=0)\n",
    "#acc = test_saved_model(best_model_dir_hyp, 0)\n",
    "\n",
    "#import keras.backend as K\n",
    "#loaded_model = tf.keras.models.load_model(MODEL_DIR_PARAM+'/checkpoints')\n",
    "#print(\"learning_rate=\", K.eval(loaded_model.optimizer.lr))\n",
    "#tf.keras.utils.plot_model(loaded_model, show_shapes=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "--------\n",
    "# Deploy the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## https://cloud.google.com/ai-platform/prediction/docs/machine-types-online-prediction#available_machine_types\n",
    "# We need 2 versions of the same model:\n",
    "# 1. Batch prediction model deployed on a mls1-c1-m2 cluster\n",
    "# 2. Online prediction model deployed on a n1-standard-16 cluster\n",
    "# Batch prediction does not support GPU and n1-standard-16 clusters.\n",
    "\n",
    "# Run the Deploy Model section twice:\n",
    "# 1. As a BATCH Mode version use MODEL_VERSION = MODEL_VERSION_BATCH\n",
    "# 2. As a ONLINE Mode version use MODEL_VERSION = MODEL_VERSION_ONLINE\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from googleapiclient import discovery\n",
    "from googleapiclient import errors\n",
    "import time\n",
    "import yaml\n",
    "\n",
    "def print_all_versions_of_model(project_id, model_name):\n",
    "    project_id = 'projects/{}'.format(project_id)\n",
    "    model_id = '{}/models/{}'.format(project_id, model_name)\n",
    "    credentials = GoogleCredentials.get_application_default()\n",
    "    service = discovery.build('ml', 'v1', cache_discovery=False, credentials=credentials)\n",
    "    request = service.projects().models().versions().list(parent=model_id)\n",
    "    response = request.execute()\n",
    "\n",
    "    # check if model has versions\n",
    "    all_versions = []\n",
    "    if 'versions' in response.keys():    \n",
    "        all_versions = response['versions']\n",
    "\n",
    "    # print all model versions\n",
    "    print(\"\\nVersions for Model:\", model_name)\n",
    "    print(\"Total Number of versions = \", len(all_versions))\n",
    "    if (len(all_versions) > 0):\n",
    "        print(\"\")\n",
    "        print(yaml.dump(all_versions))\n",
    "    \n",
    "    return all_versions\n",
    "\n",
    "#------------------------------------------------------------------------------#\n",
    "def delete_model_versions(project_id, model_name, version_name):\n",
    "    project_id = 'projects/{}'.format(project_id)\n",
    "    model_id = '{}/models/{}'.format(project_id, model_name)\n",
    "    credentials = GoogleCredentials.get_application_default()\n",
    "    service = discovery.build('ml', 'v1', cache_discovery=False, credentials=credentials)\n",
    "\n",
    "    # get model versions\n",
    "    all_versions = print_all_versions_of_model(PROJECT_ID, MODEL_NAME)\n",
    "\n",
    "    # check if current version is a default version\n",
    "    cur_ver_default = 0\n",
    "    for i in range(len(all_versions)):\n",
    "        if MODEL_VERSION in all_versions[i]['name']:\n",
    "            if 'isDefault' in all_versions[i].keys():\n",
    "                if all_versions[i]['isDefault'] == True:\n",
    "                    cur_ver_default = 1\n",
    "    print(\"cur_ver_default=\", cur_ver_default)\n",
    "\n",
    "    # if not default delete this version only\n",
    "    if (cur_ver_default == 0) and (len(all_versions) > 0):\n",
    "        print(\"This is not the default version\")\n",
    "        print(\"Delete only this version\")\n",
    "        for i in range(len(all_versions)):\n",
    "            if MODEL_VERSION in all_versions[i]['name']:\n",
    "                print(\"Deleting non default version:\", version['name'])\n",
    "                request = service.projects().models().versions().delete(name = all_versions[i]['name'])\n",
    "                try:\n",
    "                    request.execute()\n",
    "                except errors.HttpError as err:\n",
    "                    reason = err._get_reason()\n",
    "                    print(\"Delete Error Reason:\", reason)\n",
    "\n",
    "    if (cur_ver_default == 1) and (len(all_versions) > 0):\n",
    "        print(\"This is the default version\")\n",
    "        print(\"Delete all versions\")\n",
    "        if (cur_ver_default == 1) and (len(all_versions) > 0):\n",
    "            print(\"This is the default version\")\n",
    "            print(\"Delete all versions\")\n",
    "            while len(all_versions) >= 1:\n",
    "                for version in all_versions:\n",
    "                    #print(\"Deleting version:\", version['name'])\n",
    "                    request = service.projects().models().versions().delete(name = version['name'])\n",
    "                    try:\n",
    "                        request.execute()\n",
    "                    except errors.HttpError as err:\n",
    "                        reason = err._get_reason()\n",
    "                        #print(\"Delete Error Reason:\", reason)\n",
    "                        if 'Cannot delete the default version' in reason:\n",
    "                            next\n",
    "                request = service.projects().models().versions().list(parent=model_id)\n",
    "                response = request.execute()\n",
    "                time.sleep(1)\n",
    "                try:\n",
    "                    all_versions = response['versions']\n",
    "                except:\n",
    "                    all_versions = []\n",
    "                    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL_NAME = \"unetfpn16sl1\"\n",
    "MODEL_VERSION_BATCH  = \"cchatterj_v1_batch\"\n",
    "MODEL_VERSION_ONLINE = \"cchatterj_v1_online\"\n",
    "\n",
    "#Run this as Batch first then Online\n",
    "MODEL_VERSION = MODEL_VERSION_BATCH\n",
    "#MODEL_VERSION = MODEL_VERSION_ONLINE\n",
    "\n",
    "# List all models\n",
    "print(\"\\nList all models\")\n",
    "!gcloud ai-platform models list\n",
    "\n",
    "# List all versions of the model\n",
    "#!gcloud ai-platform versions list --model $MODEL_NAME\n",
    "_ = print_all_versions_of_model(PROJECT_ID, MODEL_NAME)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!gcloud ai-platform versions delete $MODEL_VERSION --model $MODEL_NAME -q\n",
    "#!gcloud ai-platform models delete $MODEL_NAME -q\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create the model if it doesn't already exist\n",
    "modelname = !gcloud ai-platform models list | grep -w $MODEL_NAME\n",
    "print(modelname)\n",
    "if len(modelname) <= 1:\n",
    "    print(\"Creating model \" + MODEL_NAME)\n",
    "    ! gcloud ai-platform models create $MODEL_NAME --regions $REGION --enable-logging\n",
    "else:\n",
    "    print(\"Model \" + MODEL_NAME + \" exist\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check if current version is default\n",
    "#    if default model delete all versions\n",
    "#    if not default model delete only this version\n",
    "\n",
    "print(\"Model Version=\", MODEL_VERSION)\n",
    "delete_model_versions(PROJECT_ID, MODEL_NAME, MODEL_VERSION)\n",
    "\n",
    "# List the models\n",
    "print(\"\\nList all models:\")\n",
    "!gcloud ai-platform models list\n",
    "\n",
    "# List all versions of the model\n",
    "_ = print_all_versions_of_model(PROJECT_ID, MODEL_NAME)\n",
    "#!gcloud ai-platform versions list --model $MODEL_NAME\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "# Get a list of model directories\n",
    "best_model_dir = best_model_dir_hyp  #best_model_dir_hyp\n",
    "print(\"Best Model Dir: \", best_model_dir)\n",
    "\n",
    "MODEL_FRAMEWORK = \"TENSORFLOW\"\n",
    "MODEL_DESCRIPTION = \"UNET_FPN_SL1\"\n",
    "MODEL_LABELS=\"team=total,phase=test,owner=\"+USER\n",
    "\n",
    "MACHINE_TYPE = \"mls1-c1-m2\"\n",
    "if (MODEL_VERSION == MODEL_VERSION_BATCH):\n",
    "    MACHINE_TYPE = \"mls1-c1-m2\"\n",
    "    MODEL_LABELS = MODEL_LABELS+\",mode=batch\"\n",
    "if (MODEL_VERSION == MODEL_VERSION_ONLINE):\n",
    "    MACHINE_TYPE = \"n1-standard-16\"\n",
    "    MODEL_LABELS = MODEL_LABELS+\",mode=online\"\n",
    "\n",
    "# Deploy the model\n",
    "! gcloud beta ai-platform versions create $MODEL_VERSION \\\n",
    "  --model $MODEL_NAME \\\n",
    "  --origin $best_model_dir \\\n",
    "  --runtime-version $RUNTIME_VERSION \\\n",
    "  --python-version=3.7 \\\n",
    "  --description=$MODEL_DESCRIPTION \\\n",
    "  --labels $MODEL_LABELS \\\n",
    "  --machine-type=$MACHINE_TYPE \n",
    "#  --framework $MODEL_FRAMEWORK \\\n",
    "#  --region=$REGION\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# List all models\n",
    "print(\"\")\n",
    "!gcloud ai-platform models list\n",
    "\n",
    "# List a ll versions of model\n",
    "_ = print_all_versions_of_model(PROJECT_ID, MODEL_NAME)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "------\n",
    "# Predictions with the deployed model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from trainer import inputs\n",
    "print(\"Input and pre-process data ...\")\n",
    "# Extract train_seismic, train_label\n",
    "data_dir = 'gs://codev-test-data/FAULT_TRAINING_SET/SYNTHETIC_OPTIM/seismic'\n",
    "label_dir = 'gs://codev-test-data/FAULT_TRAINING_SET/SYNTHETIC_OPTIM/label'\n",
    "test_dataset = inputs.load_data(data_dir, label_dir, range(0,16), 1)\n",
    "X_test1 = list(test_dataset.as_numpy_iterator())\n",
    "X_test1 = np.array(X_test1).astype(dtype='float32')\n",
    "X_test_images = X_test1[:,0,0,:,:,:]\n",
    "X_test_labels = X_test1[:,1,0,:,:,:]\n",
    "\n",
    "print(\"X_test shape =\", X_test_images.shape)\n",
    "\n",
    "# plot the results\n",
    "plot_images(X_test_images[:,:,:,0], X_test_labels[:,:,:,0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "from trainer import model\n",
    "\n",
    "# Copy the model from storage to local memory\n",
    "!gsutil -m cp -r $best_model_dir_hyp* ./model_dir\n",
    "\n",
    "# Load the model\n",
    "loaded_model = tf.keras.models.load_model('./model_dir', compile=False, \n",
    "               custom_objects={\"custom_loss\": model.custom_loss,\"custom_mse\": model.custom_mse})\n",
    "print(\"Signature \", loaded_model.signatures)\n",
    "\n",
    "# Check the model layers\n",
    "model_layers = [layer.name for layer in loaded_model.layers]\n",
    "print(\"\")\n",
    "print(\"Model Input  Layer=\", model_layers[0])\n",
    "print(\"Model Output Layer=\", model_layers[-1])\n",
    "print(\"\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Batch Prediction with GCLOUD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write batch data to file in GCS\n",
    "\n",
    "import shutil\n",
    "\n",
    "# Clean current directory\n",
    "DATA_DIR = './batch_data'\n",
    "shutil.rmtree(DATA_DIR, ignore_errors=True)\n",
    "os.makedirs(DATA_DIR)\n",
    "\n",
    "nTotalImages = X_test_images.shape[0]\n",
    "nFiles = min(100,nTotalImages)\n",
    "nImagesPerFile = min(100, nTotalImages//nFiles)\n",
    "print(\"nTotalImages =\", nTotalImages)\n",
    "print(\"nFiles =\", nFiles)\n",
    "print(\"nImagesPerFile =\", nImagesPerFile)\n",
    "\n",
    "# Create nFiles files with nImagesPerFile images each\n",
    "for i in range(nFiles):\n",
    "    with open(f'{DATA_DIR}/unkeyed_batch_{i}.json', \"w\") as file:\n",
    "        for z in range(nImagesPerFile):\n",
    "            print(f'{{\"{model_layers[0]}\": {X_test_images[i*nImagesPerFile+z].tolist()}}}', file=file)\n",
    "            #key = f'key_{i}_{z}'\n",
    "            #print(f'{{\"image\": {X_test_images[z].tolist()}, \"key\": \"{key}\"}}', file=file)\n",
    "\n",
    "# Write batch data to file\n",
    "! gsutil -m cp -r ./batch_data gs://$BUCKET_NAME/$FOLDER_NAME/\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "from pytz import timezone\n",
    "\n",
    "DATA_FORMAT=\"text\" # JSON data format\n",
    "INPUT_PATHS='gs://' + BUCKET_NAME + '/' + FOLDER_NAME + '/batch_data/*'\n",
    "OUTPUT_PATH='gs://' + BUCKET_NAME + '/' + FOLDER_NAME + '/batch_predictions'\n",
    "PRED_LABELS=\"mode=batch,team=engineering,phase=test,owner=\"+USER\n",
    "SIGNATURE_NAME=\"serving_default\"\n",
    "\n",
    "JOBNAME_BATCH = 'tf_batch_predict_'+ USER + '_' + \\\n",
    "           datetime.now(timezone('US/Pacific')).strftime(\"%m%d%y_%H%M\")\n",
    "\n",
    "print(\"INPUT_PATHS = \", INPUT_PATHS)\n",
    "print(\"OUTPUT_PATH = \", OUTPUT_PATH)\n",
    "print(\"Job Name    = \", JOBNAME_BATCH)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Submit batch predict job\n",
    "# Use  MODEL_VERSION_BATCH not MODEL_VERSION_ONLINE\n",
    "MODEL_VERSION = MODEL_VERSION_BATCH\n",
    "\n",
    "! gcloud ai-platform jobs submit prediction $JOBNAME_BATCH \\\n",
    "    --model $MODEL_NAME \\\n",
    "    --version $MODEL_VERSION \\\n",
    "    --input-paths $INPUT_PATHS \\\n",
    "    --output-path $OUTPUT_PATH \\\n",
    "    --region $REGION \\\n",
    "    --data-format $DATA_FORMAT \\\n",
    "    --labels $PRED_LABELS \\\n",
    "    --signature-name $SIGNATURE_NAME\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check the batch prediction job status\n",
    "! gcloud ai-platform jobs describe $JOBNAME_BATCH\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print Errors\n",
    "response = ! gcloud logging read \"resource.labels.job_id=$JOBNAME_BATCH severity>=ERROR\"\n",
    "for i in range(0,len(response)):\n",
    "    if 'message' in response[i]:\n",
    "        print(response[i])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!gsutil cat $OUTPUT_PATH/prediction.results-00000-of-00016\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Online Prediction with python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "# Use MODEL_VERSION_ONLINE not MODEL_VERSION_BATCH\n",
    "MODEL_VERSION = MODEL_VERSION_ONLINE\n",
    "\n",
    "from oauth2client.client import GoogleCredentials\n",
    "from googleapiclient import discovery\n",
    "from googleapiclient import errors\n",
    "import json\n",
    "\n",
    "#tf.compat.v1.logging.set_verbosity(tf.compat.v1.logging.ERROR)\n",
    "#tf.get_logger().setLevel('ERROR')\n",
    "\n",
    "project_id = 'projects/{}'.format(PROJECT_ID)\n",
    "model_name = '{}/models/{}'.format(project_id, MODEL_NAME)\n",
    "if MODEL_VERSION is not None:\n",
    "    model_name += '/versions/{}'.format(MODEL_VERSION)\n",
    "credentials = GoogleCredentials.get_application_default()\n",
    "service = discovery.build('ml', 'v1', cache_discovery=False, credentials=credentials)\n",
    "\n",
    "pprobas_temp = []\n",
    "batch_size = 1\n",
    "n_samples = min(1000,len(X_test_images))\n",
    "print(\"n_samples=\", n_samples)\n",
    "for i in range(0, n_samples, batch_size):\n",
    "    j = min(i+batch_size, n_samples)\n",
    "    print(\"Processing samples\", i, j)\n",
    "    request = service.projects().predict(name=model_name, \\\n",
    "                                         body={'instances': X_test_images[i:j].tolist()})\n",
    "    try:\n",
    "        response = request.execute()\n",
    "    except errors.HttpError as err:\n",
    "        # Something went wrong, print out some information.\n",
    "        tf.compat.v1.logging.error('There was an error getting the job info, Check the details:')\n",
    "        tf.compat.v1.logging.error(err._get_reason())\n",
    "\n",
    "    pprobas_temp += response['predictions']\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# If Model Version Online is used\n",
    "if (MODEL_VERSION_ONLINE in model_name):\n",
    "    print(\"Online Version is Used\")\n",
    "    pprobas = np.array(pprobas_temp)[:,:,:,0]\n",
    "    print(\"pprobas shape=\", pprobas.shape)\n",
    "    plot_images(X_test_images[:,:,:,0], pprobas)\n",
    "\n",
    "# If Model Version Batch is used\n",
    "if (MODEL_VERSION_BATCH in model_name):\n",
    "    print(\"Batch Version is Used\")\n",
    "    pprobas = [] \n",
    "    for i in range(0,n_samples):\n",
    "        pprobas.append(list(pprobas_temp[i].values()))\n",
    "    pprobas = np.array(pprobas)\n",
    "    pprobas = pprobas[:,0,:,:,0]\n",
    "    print(\"pprobas shape=\", pprobas.shape)\n",
    "    plot_images(X_test_images[:,:,:,0], pprobas)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "environment": {
   "name": "tf2-gpu.2-1.m61",
   "type": "gcloud",
   "uri": "gcr.io/deeplearning-platform-release/tf2-gpu.2-1:m61"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
