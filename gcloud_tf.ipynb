{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==============================================================================\n",
    "# Copyright 2020 Google LLC. This software is provided as-is, without warranty\n",
    "# or representation for any use or purpose. Your use of it is subject to your\n",
    "# agreement with Google.\n",
    "# ==============================================================================\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Uninstall old packages\n",
    "!pip3 uninstall -r requirements_uninstall.txt -y\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install packages\n",
    "# https://cloud.google.com/ai-platform/training/docs/runtime-version-list\n",
    "!pip3 install -r requirements-rt2.1.txt --user --no-cache-dir --use-feature=2020-resolver\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import packages\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "import os\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '2' \n",
    "#0 = all messages are logged (default behavior)\n",
    "#1 = INFO messages are not printed\n",
    "#2 = INFO and WARNING messages are not printed\n",
    "#3 = INFO, WARNING, and ERROR messages are not printed\n",
    "\n",
    "import numpy as np\n",
    "from google.cloud import storage\n",
    "import tensorflow as tf\n",
    "#import matplotlib.pyplot as plt\n",
    "#from tensorflow.keras import models\n",
    "\n",
    "print(\"TF Version= \", tf.__version__)\n",
    "print(\"Keras Version= \", tf.keras.__version__)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Utility functions\n",
    "\n",
    "#------\n",
    "def find_best_model_dir(model_dir, offset=1, maxFlag=1):\n",
    "    # Get a list of model directories\n",
    "    all_models = ! gsutil ls $model_dir\n",
    "    print(\"\")\n",
    "    print(\"All Models = \")\n",
    "    print(*all_models, sep='\\n')\n",
    "\n",
    "    # Check if model dirs exist\n",
    "    if ((\"CommandException\" in all_models[0]) or (len(all_models) <= 1)):\n",
    "        print(\"Create the models first.\")\n",
    "        return \"\"\n",
    "\n",
    "    # Find the best model from checkpoints\n",
    "    import re\n",
    "    best_acc = -np.Inf\n",
    "    if (maxFlag != 1):\n",
    "        best_acc = np.Inf\n",
    "    best_model_dir = \"\"\n",
    "    tup_list = []\n",
    "    for i in range(1,len(all_models)):\n",
    "        all_floats = re.findall(r\"[-+]?\\d*\\.\\d+|\\d+\", all_models[i]) #Find the floats in the string\n",
    "        cur_acc = -float(all_floats[-offset]) #which item is the model optimization metric\n",
    "        tup_list.append([all_models[i],cur_acc])\n",
    "        if (maxFlag*(cur_acc > best_acc) or (1-maxFlag)*(cur_acc < best_acc)):\n",
    "            best_acc = cur_acc\n",
    "            best_model_dir = all_models[i]\n",
    "    if maxFlag:\n",
    "        tup_list.sort(key=lambda tup: tup[1], reverse=False)\n",
    "    else:\n",
    "        tup_list.sort(key=lambda tup: tup[1], reverse=True)\n",
    "    for i in range(len(tup_list)):\n",
    "        print(tup_list[i][0])\n",
    "    print(\"Best Accuracy  from Checkpoints = \", best_acc)\n",
    "    print(\"Best Model Dir from Checkpoints = \", best_model_dir)\n",
    "    \n",
    "    return best_model_dir\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from oauth2client.client import GoogleCredentials\n",
    "from googleapiclient import discovery\n",
    "from googleapiclient import errors\n",
    "import json\n",
    "\n",
    "#------\n",
    "# Python module to get the best hypertuned model parameters\n",
    "def pyth_get_hypertuned_parameters(project_name, job_name, maxFlag):\n",
    "    # Define the credentials for the service account\n",
    "    #credentials = service_account.Credentials.from_service_account_file(<PATH TO CREDENTIALS JSON>)\n",
    "    credentials = GoogleCredentials.get_application_default()\n",
    "\n",
    "    # Define the project id and the job id and format it for the api request\n",
    "    project_id = 'projects/{}'.format(project_name)\n",
    "    job_id = '{}/jobs/{}'.format(project_id, job_name)\n",
    "\n",
    "    # Build the service\n",
    "    cloudml = discovery.build('ml', 'v1', cache_discovery=False, credentials=credentials)\n",
    "\n",
    "    # Execute the request and pass in the job id\n",
    "    request = cloudml.projects().jobs().get(name=job_id)\n",
    "\n",
    "    try:\n",
    "        response = request.execute()\n",
    "        # Handle a successful request\n",
    "    except errors.HttpError as err:\n",
    "        tf.compat.v1.logging.error('There was an error getting the hyperparameters. Check the details:')\n",
    "        tf.compat.v1.logging.error(err._get_reason())\n",
    "\n",
    "    # Get just the best hp values\n",
    "    if maxFlag:\n",
    "        best_model = response['trainingOutput']['trials'][0]\n",
    "    else:\n",
    "        best_model = response['trainingOutput']['trials'][-1]\n",
    "    #print('Best Hyperparameters:')\n",
    "    #print(json.dumps(best_model, indent=4))\n",
    "\n",
    "    nTrials = len(response['trainingOutput']['trials'])\n",
    "    for i in range(0,nTrials):\n",
    "        state = response['trainingOutput']['trials'][i]['state']\n",
    "        trialId = response['trainingOutput']['trials'][i]['trialId']\n",
    "        objV = -1\n",
    "        if (state == 'SUCCEEDED'):\n",
    "            objV = response['trainingOutput']['trials'][i]['finalMetric']['objectiveValue']\n",
    "        print('objective=', objV, ' trialId=', trialId, state)\n",
    "        d = response['trainingOutput']['trials'][i]['hyperparameters']\n",
    "        for key, value in d.items():\n",
    "            print('    ', key, value)\n",
    "    return best_model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot original image overlaid by the fault labels\n",
    "# image_array is np.array(num_images, x_size, y_size)\n",
    "# label_array is np.array(num_images, x_size, y_size)\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def plot_images(image_array, label_array):\n",
    "    fig, axarr = plt.subplots(4,4, figsize=(16, 16))\n",
    "    for i in range(4):\n",
    "        for j in range(4):\n",
    "            axarr[i,j].set_title('Image-'+str(4*i+j))\n",
    "            axarr[i,j].imshow(image_array[4*i+j,:,:], cmap='gray')\n",
    "            axarr[i,j].imshow(label_array[4*i+j,:,:], alpha=0.3)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "USER = 'cchatterj'\n",
    "PROJECT_ID = 'cchatterjee-sandbox' #$(gcloud config list project --format \"value(core.project)\")\n",
    "BUCKET_NAME = 'chanchal-sandbox'\n",
    "FOLDER_NAME = 'tensorflow_results'\n",
    "REGION = 'us-central1'\n",
    "RUNTIME_VERSION = 2.1\n",
    "JOB_DIR   = 'gs://' + BUCKET_NAME + '/' + FOLDER_NAME + '/jobdir'\n",
    "MODEL_DIR = 'gs://' + BUCKET_NAME + '/' + FOLDER_NAME + '/models'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!gcloud config list\n",
    "#!gcloud config config-helper --format \"value(configuration.properties.core.project)\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clean old job logs, job packages and models\n",
    "!gsutil -m -q rm $JOB_DIR/packages/**\n",
    "!gsutil -m -q rm $MODEL_DIR/model**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# UNET-FPN32-sl1 Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the tf_trainer directory and load the trainer files in it\n",
    "!mkdir -p trainer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile ./trainer/inputs.py\n",
    "\n",
    "# Create the train and label lists\n",
    "import math\n",
    "import numpy as np\n",
    "#import imageio\n",
    "#from google.cloud import storage\n",
    "import io\n",
    "import tensorflow as tf\n",
    "from PIL import Image\n",
    "\n",
    "#------\n",
    "def readImage(image_path):\n",
    "    img = tf.io.gfile.GFile(image_path, 'rb').read()\n",
    "    img = io.BytesIO(img)\n",
    "    img = np.array(Image.open(img).convert('L')).astype(dtype='int32')\n",
    "    height, width = img.shape\n",
    "    # clipping \n",
    "    h = int(height/16)*16\n",
    "    w = int(width/16)*16\n",
    "    image = img[:h, :w]\n",
    "    return image\n",
    "\n",
    "#------\n",
    "def load_data(data_file_path: str, label_file_path: str, rangeIndices, batch_size) -> tf.data.Dataset:\n",
    "    images = []\n",
    "    for i in rangeIndices:\n",
    "        im = readImage(data_file_path+'/image_inline_i%04d.png' % i)\n",
    "        im = np.array(im).astype(dtype='float32')/255\n",
    "        (h,w) = im.shape\n",
    "        im = np.reshape(im, (h,w,1))\n",
    "        images.append(im)\n",
    "\n",
    "    labels = []\n",
    "    for i in rangeIndices:\n",
    "        im = readImage(label_file_path+'/image_inline_i%04d.png' % i)\n",
    "        im = np.array(im).astype(dtype='float32')\n",
    "        (h,w) = im.shape\n",
    "        im = np.reshape(im, (h,w,1))\n",
    "        labels.append(im)\n",
    "\n",
    "    #for items in images:\n",
    "    #    print(items)\n",
    "\n",
    "    seismic = np.array(images)\n",
    "    label = np.array(labels)\n",
    "    print(\"Data  Shape = \", images.shape)\n",
    "    print(\"Label Shape = \", label.shape)\n",
    "    \n",
    "    #return images, labels\n",
    "    \n",
    "    dataset = tf.data.Dataset.from_tensor_slices((images, labels))\n",
    "    dataset = dataset.shuffle(100).batch(batch_size)\n",
    "    \n",
    "    return dataset\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile ./trainer/model.py\n",
    "\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "from tensorflow.keras import backend as K\n",
    "from tensorflow.keras import Input, layers, models, regularizers\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Layer\n",
    "from tensorflow.keras.activations import softmax\n",
    "\n",
    "\n",
    "def tf_model():\n",
    "    return\n",
    "\n",
    "def custom_loss(y_true, y_pred):\n",
    "    return custom_loss\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Package for distributed training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile ./setup.py\n",
    "\n",
    "# python3\n",
    "\n",
    "# ==============================================================================\n",
    "# Copyright 2020 Google LLC. This software is provided as-is, without warranty\n",
    "# or representation for any use or purpose. Your use of it is subject to your\n",
    "# agreement with Google.\n",
    "# ==============================================================================\n",
    "\n",
    "from setuptools import find_packages\n",
    "from setuptools import setup\n",
    "\n",
    "#Runtime 2.2\n",
    "#REQUIRED_PACKAGES = ['tensorflow==2.3.1',\n",
    "#                     'pandas==1.0.4',\n",
    "#                     'scikit-learn==0.23.1',\n",
    "#                     'google-cloud-storage==1.29.0',\n",
    "#                     'cloudml-hypertune',\n",
    "#                     'pillow',\n",
    "#                    ]\n",
    "#Runtime 2.1\n",
    "REQUIRED_PACKAGES = ['tensorflow==2.1.0',\n",
    "                     'pandas==0.25.3',\n",
    "                     'scikit-learn==0.22',\n",
    "                     'google-cloud-storage==1.23.0',\n",
    "                     'cloudml-hypertune',\n",
    "                     'pillow',\n",
    "                    ]\n",
    "setup(\n",
    "    name='trainer',\n",
    "    version='0.1',\n",
    "    install_requires=REQUIRED_PACKAGES,\n",
    "    packages=find_packages(),\n",
    "    include_package_data=True,\n",
    "    description='Trainer package for Tensorflow Task'\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile ./trainer/__init__.py\n",
    "# python3\n",
    "\n",
    "# ==============================================================================\n",
    "# Copyright 2020 Google LLC. This software is provided as-is, without warranty\n",
    "# or representation for any use or purpose. Your use of it is subject to your\n",
    "# agreement with Google.\n",
    "# ==============================================================================\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile ./trainer/train.py\n",
    "\n",
    "# python3\n",
    "# ==============================================================================\n",
    "# Copyright 2020 Google LLC. This software is provided as-is, without warranty\n",
    "# or representation for any use or purpose. Your use of it is subject to your\n",
    "# agreement with Google.\n",
    "# ==============================================================================\n",
    "\n",
    "import os\n",
    "import json\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import datetime as datetime\n",
    "from pytz import timezone\n",
    "import hypertune\n",
    "import argparse\n",
    "from trainer import model\n",
    "from trainer import inputs\n",
    "\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "import os\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3' \n",
    "#0 = all messages are logged (default behavior)\n",
    "#1 = INFO messages are not printed\n",
    "#2 = INFO and WARNING messages are not printed\n",
    "#3 = INFO, WARNING, and ERROR messages are not printed\n",
    "\n",
    "\n",
    "def parse_arguments():\n",
    "    \"\"\"Argument parser.\n",
    "    Returns:\n",
    "      Dictionary of arguments.\n",
    "    \"\"\"\n",
    "\n",
    "    parser = argparse.ArgumentParser()\n",
    "    parser.add_argument('--depth', default=5, type=int, \n",
    "                        help='Hyperparameter: depth of net')\n",
    "    parser.add_argument('--dropout_rate', default=0.2, type=float, \n",
    "                        help='Hyperparameter: Drop out rate')\n",
    "    parser.add_argument('--learning_rate', default=0.00005, type=float, \n",
    "                        help='Hyperparameter: initial learning rate')\n",
    "    parser.add_argument('--batch_size', default=1, type=int, \n",
    "                        help='batch size of the deep network')\n",
    "    parser.add_argument('--epochs', default=2, type=int, \n",
    "                        help='epoch.')\n",
    "    parser.add_argument('--num_samples', default=3000, type=int,\n",
    "                        help='Number of training samples to use.')\n",
    "    parser.add_argument('--model_dir', default=\"\",\n",
    "                        help='Directory to store models and logs.')\n",
    "    parser.add_argument('--verbosity', choices=['DEBUG','ERROR','FATAL','INFO','WARN'],\n",
    "                        default='FATAL')\n",
    "    args, _ = parser.parse_known_args()\n",
    "    return args\n",
    "\n",
    "\n",
    "def get_callbacks(args, early_stop_patience: int = 3):\n",
    "    \"\"\"Creates Keras callbacks for model training.\"\"\"\n",
    "\n",
    "    # Get trialId\n",
    "    trialId = json.loads(os.environ.get(\"TF_CONFIG\", \"{}\")).get(\"task\", {}).get(\"trial\", \"\")\n",
    "    if trialId == '':\n",
    "        trialId = '0'\n",
    "    print(\"trialId=\", trialId)\n",
    "\n",
    "    curTime = datetime.datetime.now(timezone('US/Pacific')).strftime('%H%M%S')\n",
    "    \n",
    "    # Modify model_dir paths to include trialId\n",
    "    model_dir = args.model_dir + \"/checkpoints/cp-\"+curTime+\"-\"+trialId+\"-{custom_mse:.4f}\"\n",
    "    log_dir   = args.model_dir + \"/log_dir\"\n",
    "\n",
    "    tensorboard_cb = tf.keras.callbacks.TensorBoard(log_dir, histogram_freq=1)\n",
    "    checkpoint_cb  = tf.keras.callbacks.ModelCheckpoint(model_dir, monitor='custom_mse', mode='min', \n",
    "                                                        verbose=0, save_best_only=True,\n",
    "                                                        save_weights_only=False)\n",
    "    earlystop_cb   = tf.keras.callbacks.EarlyStopping(monitor='loss', patience=3)\n",
    "\n",
    "    return [checkpoint_cb, tensorboard_cb, earlystop_cb]\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "\n",
    "    # ---------------------------------------\n",
    "    # Parse Arguments\n",
    "    # ---------------------------------------\n",
    "    args = parse_arguments()\n",
    "    #args.model_dir = MODEL_DIR + datetime.datetime.now(timezone('US/Pacific')).strftime('/model_%m%d%Y_%H%M')\n",
    "    print(args)\n",
    "\n",
    "    #tf.compat.v1.logging.set_verbosity(tf.compat.v1.logging.ERROR)  # or any {DEBUG, INFO, WARN, ERROR, FATAL}\n",
    "\n",
    "    # ---------------------------------------\n",
    "    # Input Data & Preprocessing\n",
    "    # ---------------------------------------\n",
    "    print(\"Input and pre-process data ...\")\n",
    "    # Extract train_seismic, train_label\n",
    "    data_dir = 'gs://../images'\n",
    "    label_dir = 'gs://../labels'\n",
    "    train_dataset = inputs.load_data(data_dir, label_dir, range(0,args.num_samples), args.batch_size)\n",
    "    \n",
    "    # ---------------------------------------\n",
    "    # Train model\n",
    "    # ---------------------------------------\n",
    "    print(\"Creating model ...\")\n",
    "    tf_model = model.tf_model(depth=args.depth,\n",
    "                              dropout_rate=args.dropout_rate)\n",
    "    tf_model.compile(optimizer=tf.keras.optimizers.Adam(lr=args.learning_rate),\n",
    "                     loss=model.custom_loss,  # loss is the custom loss I shared with you\n",
    "                     metrics=[model.custom_mse])\n",
    "    \n",
    "    print(\"Fitting model ...\")\n",
    "    callbacks = get_callbacks(args, 3)\n",
    "    history = tf_model.fit(train_dataset, \n",
    "                           epochs=args.epochs,\n",
    "                           validation_split = 0.0,\n",
    "                           callbacks=callbacks)\n",
    "\n",
    "    # TBD save history for visualization\n",
    "\n",
    "    final_epoch_accuracy = history.history['custom_mse'][-1]\n",
    "    final_epoch_count = len(history.history['custom_mse'])\n",
    "\n",
    "    print('final_epoch_accuracy = %.6f' % final_epoch_accuracy)\n",
    "    print('final_epoch_count = %02d' % final_epoch_count)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "# Run the training manually\n",
    "# Training parameters\n",
    "from datetime import datetime\n",
    "from pytz import timezone\n",
    "\n",
    "DEPTH = 7\n",
    "DROPOUT_RATE = 0.15000017803209056\n",
    "#DEPTH = 5\n",
    "#DROPOUT_RATE = 0.2\n",
    "#N_SEG_LEVELS = 3\n",
    "LEARNING_RATE = 0.00005\n",
    "EPOCHS = 2\n",
    "BATCH_SIZE = 1\n",
    "NUM_SAMPLES = 3000\n",
    "\n",
    "MODEL_DIR_PYTH = MODEL_DIR + datetime.now(timezone('US/Pacific')).strftime('/model_%m%d%Y_%H%M')\n",
    "\n",
    "print('DEPTH = %02d' % DEPTH)\n",
    "print('DROPOUT_RATE = %.4f' % DROPOUT_RATE)\n",
    "print('LEARNING_RATE = %.6f' % LEARNING_RATE)\n",
    "print('EPOCHS = %02d' % EPOCHS)\n",
    "print('BATCH_SIZE = %02d' % BATCH_SIZE)\n",
    "print('NUM_SAMPLES = %d' % NUM_SAMPLES)\n",
    "print(\"MODEL_DIR =\", MODEL_DIR_PYTH)\n",
    "\n",
    "# Run training\n",
    "! python3 -m trainer.train --depth=$DEPTH --dropout_rate=$DROPOUT_RATE \\\n",
    "    --n_base_filters=$N_BASE_FILTERS --n_segmentation_levels=$N_SEG_LEVELS \\\n",
    "    --learning_rate=$LEARNING_RATE \\\n",
    "    --epochs=$EPOCHS --batch_size=$BATCH_SIZE --num_samples=$NUM_SAMPLES \\\n",
    "    --model_dir=$MODEL_DIR_PYTH\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test with latest saved model\n",
    "best_model_dir_pyth = find_best_model_dir(MODEL_DIR_PYTH+'/checkpoints', offset=1, maxFlag=0)\n",
    "#acc = test_saved_model(best_model_dir_pyth, 0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "from trainer import model\n",
    "\n",
    "# Copy the model from storage to local memory\n",
    "!gsutil -m cp -r $best_model_dir_pyth* ./model_dir\n",
    "\n",
    "# Load the model\n",
    "loaded_model = tf.keras.models.load_model('./model_dir', compile=False, \n",
    "               custom_objects={\"custom_loss\": model.custom_loss, \"custom_mse\": model.custom_mse})\n",
    "print(\"Signature \", loaded_model.signatures)\n",
    "print(\"\")\n",
    "\n",
    "# Display model\n",
    "tf.keras.utils.plot_model(loaded_model, show_shapes=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "------\n",
    "# Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the config directory and load the trainer files in it\n",
    "!mkdir -p config\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile ./config/config.yaml\n",
    "\n",
    "# python3\n",
    "# ==============================================================================\n",
    "# Copyright 2020 Google LLC. This software is provided as-is, without warranty\n",
    "# or representation for any use or purpose. Your use of it is subject to your\n",
    "# agreement with Google.\n",
    "# ==============================================================================\n",
    "\n",
    "# https://cloud.google.com/sdk/gcloud/reference/ai-platform/jobs/submit/training#--scale-tier\n",
    "# https://www.kaggle.com/c/passenger-screening-algorithm-challenge/discussion/37087\n",
    "# https://cloud.google.com/ai-platform/training/docs/using-gpus\n",
    "\n",
    "#trainingInput:\n",
    "#  scaleTier: CUSTOM\n",
    "#  masterType: n1-highmem-8\n",
    "#  masterConfig:\n",
    "#    acceleratorConfig:\n",
    "#      count: 1\n",
    "#      type: NVIDIA_TESLA_T4\n",
    "\n",
    "#  masterType: n1-highcpu-16\n",
    "#  workerType: cloud_tpu\n",
    "#  workerCount: 1\n",
    "#  workerConfig:\n",
    "#    acceleratorConfig:\n",
    "#      type: TPU_V3\n",
    "#      count: 8\n",
    "\n",
    "#trainingInput:\n",
    "#  scaleTier: CUSTOM\n",
    "#  masterType: complex_model_m\n",
    "#  workerType: complex_model_m\n",
    "#  parameterServerType: large_model\n",
    "#  workerCount: 6\n",
    "#  parameterServerCount: 1\n",
    "#  scheduling:\n",
    "#    maxWaitTime: 3600s\n",
    "#    maxRunningTime: 7200s\n",
    "\n",
    "#trainingInput:\n",
    "#  runtimeVersion: \"2.2\"\n",
    "#  scaleTier: CUSTOM\n",
    "#  masterType: standard_gpu\n",
    "#  workerCount: 9\n",
    "#  workerType: standard_gpu\n",
    "#  parameterServerCount: 3\n",
    "#  parameterServerType: standard\n",
    "\n",
    "#trainingInput:\n",
    "#  scaleTier: BASIC-GPU\n",
    "    \n",
    "#trainingInput:\n",
    "#  region: us-central1\n",
    "#  scaleTier: CUSTOM\n",
    "#  masterType: complex_model_m\n",
    "#  workerType: complex_model_m_gpu\n",
    "#  parameterServerType: large_model\n",
    "#  workerCount: 4\n",
    "#  parameterServerCount: 2\n",
    "\n",
    "trainingInput:\n",
    "    scaleTier: CUSTOM\n",
    "    masterType: n1-highmem-16\n",
    "    masterConfig:\n",
    "        acceleratorConfig:\n",
    "            count: 2\n",
    "            type: NVIDIA_TESLA_V100\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "from pytz import timezone\n",
    "JOBNAME_TRN = 'tf_train_'+ USER + '_' + \\\n",
    "           datetime.now(timezone('US/Pacific')).strftime(\"%m%d%y_%H%M\")\n",
    "JOB_CONFIG = \"config/config.yaml\"\n",
    "MODEL_DIR_TRN = MODEL_DIR + datetime.now(timezone('US/Pacific')).strftime('/model_%m%d%Y_%H%M')\n",
    "print(\"Job Name = \", JOBNAME_TRN)\n",
    "print(\"Job Dir  = \", JOB_DIR)\n",
    "print(\"MODEL_DIR =\", MODEL_DIR_TRN)\n",
    "\n",
    "# Training parameters\n",
    "DEPTH = 6\n",
    "DROPOUT_RATE = 0.15\n",
    "#DEPTH = 5\n",
    "#DROPOUT_RATE = 0.2\n",
    "LEARNING_RATE = 0.00005\n",
    "EPOCHS = 2\n",
    "BATCH_SIZE = 1\n",
    "NUM_SAMPLES = 3000\n",
    "\n",
    "print('DEPTH = %02d' % DEPTH)\n",
    "print('DROPOUT_RATE = %.4f' % DROPOUT_RATE)\n",
    "print('LEARNING_RATE = %.6f' % LEARNING_RATE)\n",
    "print('EPOCHS = %02d' % EPOCHS)\n",
    "print('BATCH_SIZE = %02d' % BATCH_SIZE)\n",
    "print('NUM_SAMPLES = %d' % NUM_SAMPLES)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://cloud.google.com/sdk/gcloud/reference/ai-platform/jobs/submit/training\n",
    "\n",
    "TRAIN_LABELS = \"mode=train,owner=\"+USER\n",
    "\n",
    "# submit the training job\n",
    "! gcloud ai-platform jobs submit training $JOBNAME_TRN \\\n",
    "  --package-path $(pwd)/trainer \\\n",
    "  --module-name trainer.train \\\n",
    "  --region $REGION \\\n",
    "  --python-version 3.7 \\\n",
    "  --runtime-version $RUNTIME_VERSION \\\n",
    "  --job-dir $JOB_DIR \\\n",
    "  --config $JOB_CONFIG \\\n",
    "  --labels $TRAIN_LABELS \\\n",
    "  -- \\\n",
    "  --depth=$DEPTH \\\n",
    "  --dropout_rate=$DROPOUT_RATE \\\n",
    "  --learning_rate=$LEARNING_RATE \\\n",
    "  --epochs=$EPOCHS \\\n",
    "  --batch_size=$BATCH_SIZE \\\n",
    "  --num_samples=$NUM_SAMPLES \\\n",
    "  --model_dir=$MODEL_DIR_TRN\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check the training job status\n",
    "! gcloud ai-platform jobs describe $JOBNAME_TRN\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print Errors\n",
    "response = ! gcloud logging read \"resource.labels.job_id=$JOBNAME_TRN severity>=ERROR\"\n",
    "for i in range(0,len(response)):\n",
    "    if 'message' in response[i]:\n",
    "        print(response[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test with latest saved model\n",
    "best_model_dir_trn = find_best_model_dir(MODEL_DIR_TRN+'/checkpoints', offset=1, maxFlag=0)\n",
    "#acc = test_saved_model(best_model_dir_trn, 0)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "------\n",
    "# Hyper Parameter Tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the tf directory and load the trainer files in it\n",
    "!cp ./trainer/train.py ./trainer/train_hpt.py\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile -a ./trainer/train_hpt.py\n",
    "\n",
    "    \"\"\"This method updates a CAIP HPTuning Job with a final metric for the job.\n",
    "    In TF2.X the user must either use hypertune or a custom callback with\n",
    "    tf.summary.scalar to update CAIP HP Tuning jobs. This function uses\n",
    "    hypertune, which appears to be the preferred solution. Hypertune also works\n",
    "    with containers, without code change.\n",
    "    Args:\n",
    "        metric_tag: The metric being optimized.  This MUST MATCH the\n",
    "          hyperparameterMetricTag specificed in the hyperparameter tuning yaml.\n",
    "        metric_value: The value to report at the end of model training.\n",
    "        global_step: An int value to specify the number of trainin steps completed\n",
    "          at the time the metric was reported.\n",
    "    \"\"\"\n",
    "\n",
    "    hpt = hypertune.HyperTune()\n",
    "    hpt.report_hyperparameter_tuning_metric(\n",
    "        hyperparameter_metric_tag='custom_mse',\n",
    "        metric_value=final_epoch_accuracy,\n",
    "        global_step=final_epoch_count\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile ./config/hptuning_config.yaml\n",
    "\n",
    "# python3\n",
    "# ==============================================================================\n",
    "# Copyright 2020 Google LLC. This software is provided as-is, without warranty\n",
    "# or representation for any use or purpose. Your use of it is subject to your\n",
    "# agreement with Google.\n",
    "# ==============================================================================\n",
    "\n",
    "# https://cloud.google.com/ai-platform/training/docs/reference/rest/v1/projects.jobs\n",
    "\n",
    "#trainingInput:\n",
    "#  scaleTier: CUSTOM\n",
    "#  masterType: n1-highmem-8\n",
    "#  masterConfig:\n",
    "#    acceleratorConfig:\n",
    "#      count: 1\n",
    "#      type: NVIDIA_TESLA_T4\n",
    "#\n",
    "#  masterType: standard_p100\n",
    "#  workerType: standard_p100\n",
    "#  parameterServerType: standard_p100\n",
    "#  workerCount: 8\n",
    "#  parameterServerCount: 1\n",
    "#  runtimeVersion: $RUNTIME_VERSION\n",
    "#  pythonVersion: '3.7'\n",
    "\n",
    "#trainingInput:\n",
    "#  scaleTier: CUSTOM\n",
    "#  masterType: complex_model_m\n",
    "#  workerType: complex_model_m\n",
    "#  parameterServerType: large_model\n",
    "#  workerCount: 9\n",
    "#  parameterServerCount: 3\n",
    "#  scheduling:\n",
    "#    maxWaitTime: 3600s\n",
    "#    maxRunningTime: 7200s\n",
    "\n",
    "#trainingInput:\n",
    "#  scaleTier: BASIC-GPU\n",
    "\n",
    "trainingInput:\n",
    "  scaleTier: CUSTOM\n",
    "  masterType: n1-highmem-16\n",
    "  masterConfig:\n",
    "    acceleratorConfig:\n",
    "      count: 2\n",
    "      type: NVIDIA_TESLA_V100\n",
    "  hyperparameters:\n",
    "    goal: MAXIMIZE\n",
    "    hyperparameterMetricTag: custom_mse\n",
    "    maxTrials: 32\n",
    "    maxParallelTrials: 4\n",
    "    enableTrialEarlyStopping: True\n",
    "    params:\n",
    "    - parameterName: depth\n",
    "      type: INTEGER\n",
    "      minValue: 4\n",
    "      maxValue: 6\n",
    "      scaleType: UNIT_LINEAR_SCALE\n",
    "    - parameterName: dropout_rate\n",
    "      type: DOUBLE\n",
    "      minValue: 0.1\n",
    "      maxValue: 0.3\n",
    "      scaleType: UNIT_LINEAR_SCALE\n",
    "    - parameterName: n_segmentation_levels\n",
    "      type: DISCRETE\n",
    "      discreteValues:\n",
    "      - 2\n",
    "      - 3\n",
    "    - parameterName: n_base_filters\n",
    "      type: DISCRETE\n",
    "      discreteValues:\n",
    "      - 8\n",
    "      - 12\n",
    "      - 16\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "from pytz import timezone\n",
    "\n",
    "JOBNAME_HPT = 'tf_hptrn_' + USER + '_' + \\\n",
    "              datetime.now(timezone('US/Pacific')).strftime(\"%m%d%y_%H%M\")\n",
    "JOB_CONFIG = \"./config/hptuning_config.yaml\"\n",
    "MODEL_DIR_HPT = MODEL_DIR + datetime.now(timezone('US/Pacific')).strftime('/model_%m%d%Y_%H%M')\n",
    "print(\"Job Name = \", JOBNAME_HPT)\n",
    "print(\"Job Dir  = \", JOB_DIR)\n",
    "print(\"MODEL_DIR =\", MODEL_DIR_HPT)\n",
    "\n",
    "# Training parameters\n",
    "LEARNING_RATE = 0.00005\n",
    "EPOCHS = 2\n",
    "BATCH_SIZE = 1\n",
    "NUM_SAMPLES = 3000\n",
    "\n",
    "print('LEARNING_RATE = %.6f' % LEARNING_RATE)\n",
    "print('EPOCHS = %02d' % EPOCHS)\n",
    "print('BATCH_SIZE = %02d' % BATCH_SIZE)\n",
    "print('NUM_SAMPLES = %d' % NUM_SAMPLES)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# submit the training job\n",
    "HT_LABELS = \"mode=hypertrain,owner=\"+USER\n",
    "\n",
    "! gcloud ai-platform jobs submit training $JOBNAME_HPT \\\n",
    "  --package-path $(pwd)/trainer \\\n",
    "  --module-name trainer.train_hpt \\\n",
    "  --python-version 3.7 \\\n",
    "  --runtime-version $RUNTIME_VERSION \\\n",
    "  --region $REGION \\\n",
    "  --job-dir $JOB_DIR \\\n",
    "  --config $JOB_CONFIG \\\n",
    "  --labels $HT_LABELS \\\n",
    "  -- \\\n",
    "  --learning_rate=$LEARNING_RATE \\\n",
    "  --epochs=$EPOCHS \\\n",
    "  --batch_size=$BATCH_SIZE \\\n",
    "  --num_samples=$NUM_SAMPLES \\\n",
    "  --model_dir=$MODEL_DIR_HPT\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check the hyperparameter training job status\n",
    "! gcloud ai-platform jobs describe $JOBNAME_HPT\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print Errors\n",
    "response = ! gcloud logging read \"resource.labels.job_id=$JOBNAME_HPT severity>=ERROR\"\n",
    "for i in range(0,len(response)):\n",
    "    if 'message' in response[i]:\n",
    "        print(response[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the best model parameters from Cloud API\n",
    "best_model = pyth_get_hypertuned_parameters(PROJECT_ID, JOBNAME_HPT, 0)\n",
    "DEPTH = best_model['hyperparameters']['depth']\n",
    "DROPOUT_RATE = best_model['hyperparameters']['dropout_rate']\n",
    "N_SEG_LEVELS = best_model['hyperparameters']['n_segmentation_levels']\n",
    "N_BASE_FILTERS = best_model['hyperparameters']['n_base_filters']\n",
    "print('')\n",
    "print('Objective=', best_model['finalMetric']['objectiveValue'])\n",
    "print('DEPTH =', DEPTH)\n",
    "print('DROPOUT_RATE =', DROPOUT_RATE)\n",
    "print('N_SEG_LEVELS =', N_SEG_LEVELS)\n",
    "print('N_BASE_FILTERS =', N_BASE_FILTERS)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find count of checkpoints\n",
    "all_models = ! gsutil ls {MODEL_DIR_HPT+'/checkpoints'}\n",
    "print(\"Total Hypertrained Models=\", len(all_models))\n",
    "\n",
    "# Test with latest saved model\n",
    "best_model_dir_hyp = find_best_model_dir(MODEL_DIR_HPT+'/checkpoints', offset=1, maxFlag=0)\n",
    "#acc = test_saved_model(best_model_dir_hyp, 0)\n",
    "\n",
    "#import keras.backend as K\n",
    "#loaded_model = tf.keras.models.load_model(MODEL_DIR_PARAM+'/checkpoints')\n",
    "#print(\"learning_rate=\", K.eval(loaded_model.optimizer.lr))\n",
    "#tf.keras.utils.plot_model(loaded_model, show_shapes=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "--------\n",
    "# Deploy the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## https://cloud.google.com/ai-platform/prediction/docs/machine-types-online-prediction#available_machine_types\n",
    "# We need 2 versions of the same model:\n",
    "# 1. Batch prediction model deployed on a mls1-c1-m2 cluster\n",
    "# 2. Online prediction model deployed on a n1-standard-16 cluster\n",
    "# Batch prediction does not support GPU and n1-standard-16 clusters.\n",
    "\n",
    "# Run the Deploy Model section twice:\n",
    "# 1. As a BATCH Mode version use MODEL_VERSION = MODEL_VERSION_BATCH\n",
    "# 2. As a ONLINE Mode version use MODEL_VERSION = MODEL_VERSION_ONLINE\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from googleapiclient import discovery\n",
    "from googleapiclient import errors\n",
    "import time\n",
    "import yaml\n",
    "\n",
    "def print_all_versions_of_model(project_id, model_name):\n",
    "    project_id = 'projects/{}'.format(project_id)\n",
    "    model_id = '{}/models/{}'.format(project_id, model_name)\n",
    "    credentials = GoogleCredentials.get_application_default()\n",
    "    service = discovery.build('ml', 'v1', cache_discovery=False, credentials=credentials)\n",
    "    request = service.projects().models().versions().list(parent=model_id)\n",
    "    response = request.execute()\n",
    "\n",
    "    # check if model has versions\n",
    "    all_versions = []\n",
    "    if 'versions' in response.keys():    \n",
    "        all_versions = response['versions']\n",
    "\n",
    "    # print all model versions\n",
    "    print(\"\\nVersions for Model:\", model_name)\n",
    "    print(\"Total Number of versions = \", len(all_versions))\n",
    "    if (len(all_versions) > 0):\n",
    "        print(\"\")\n",
    "        print(yaml.dump(all_versions))\n",
    "    \n",
    "    return all_versions\n",
    "\n",
    "#------------------------------------------------------------------------------#\n",
    "def delete_model_versions(project_id, model_name, version_name):\n",
    "    project_id = 'projects/{}'.format(project_id)\n",
    "    model_id = '{}/models/{}'.format(project_id, model_name)\n",
    "    credentials = GoogleCredentials.get_application_default()\n",
    "    service = discovery.build('ml', 'v1', cache_discovery=False, credentials=credentials)\n",
    "\n",
    "    # get model versions\n",
    "    all_versions = print_all_versions_of_model(PROJECT_ID, MODEL_NAME)\n",
    "\n",
    "    # check if current version is a default version\n",
    "    cur_ver_default = 0\n",
    "    for i in range(len(all_versions)):\n",
    "        if MODEL_VERSION in all_versions[i]['name']:\n",
    "            if 'isDefault' in all_versions[i].keys():\n",
    "                if all_versions[i]['isDefault'] == True:\n",
    "                    cur_ver_default = 1\n",
    "    print(\"cur_ver_default=\", cur_ver_default)\n",
    "\n",
    "    # if not default delete this version only\n",
    "    if (cur_ver_default == 0) and (len(all_versions) > 0):\n",
    "        print(\"This is not the default version\")\n",
    "        print(\"Delete only this version\")\n",
    "        for i in range(len(all_versions)):\n",
    "            if MODEL_VERSION in all_versions[i]['name']:\n",
    "                print(\"Deleting non default version:\", version['name'])\n",
    "                request = service.projects().models().versions().delete(name = all_versions[i]['name'])\n",
    "                try:\n",
    "                    request.execute()\n",
    "                except errors.HttpError as err:\n",
    "                    reason = err._get_reason()\n",
    "                    print(\"Delete Error Reason:\", reason)\n",
    "\n",
    "    if (cur_ver_default == 1) and (len(all_versions) > 0):\n",
    "        print(\"This is the default version\")\n",
    "        print(\"Delete all versions\")\n",
    "        if (cur_ver_default == 1) and (len(all_versions) > 0):\n",
    "            print(\"This is the default version\")\n",
    "            print(\"Delete all versions\")\n",
    "            while len(all_versions) >= 1:\n",
    "                for version in all_versions:\n",
    "                    #print(\"Deleting version:\", version['name'])\n",
    "                    request = service.projects().models().versions().delete(name = version['name'])\n",
    "                    try:\n",
    "                        request.execute()\n",
    "                    except errors.HttpError as err:\n",
    "                        reason = err._get_reason()\n",
    "                        #print(\"Delete Error Reason:\", reason)\n",
    "                        if 'Cannot delete the default version' in reason:\n",
    "                            next\n",
    "                request = service.projects().models().versions().list(parent=model_id)\n",
    "                response = request.execute()\n",
    "                time.sleep(1)\n",
    "                try:\n",
    "                    all_versions = response['versions']\n",
    "                except:\n",
    "                    all_versions = []\n",
    "                    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL_NAME = \"unetfpn16sl1\"\n",
    "MODEL_VERSION_BATCH  = \"cchatterj_v1_batch\"\n",
    "MODEL_VERSION_ONLINE = \"cchatterj_v1_online\"\n",
    "\n",
    "#Run this as Batch first then Online\n",
    "MODEL_VERSION = MODEL_VERSION_BATCH\n",
    "#MODEL_VERSION = MODEL_VERSION_ONLINE\n",
    "\n",
    "# List all models\n",
    "print(\"\\nList all models\")\n",
    "!gcloud ai-platform models list\n",
    "\n",
    "# List all versions of the model\n",
    "#!gcloud ai-platform versions list --model $MODEL_NAME\n",
    "_ = print_all_versions_of_model(PROJECT_ID, MODEL_NAME)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!gcloud ai-platform versions delete $MODEL_VERSION --model $MODEL_NAME -q\n",
    "#!gcloud ai-platform models delete $MODEL_NAME -q\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create the model if it doesn't already exist\n",
    "modelname = !gcloud ai-platform models list | grep -w $MODEL_NAME\n",
    "print(modelname)\n",
    "if len(modelname) <= 1:\n",
    "    print(\"Creating model \" + MODEL_NAME)\n",
    "    ! gcloud ai-platform models create $MODEL_NAME --regions $REGION --enable-logging\n",
    "else:\n",
    "    print(\"Model \" + MODEL_NAME + \" exist\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check if current version is default\n",
    "#    if default model delete all versions\n",
    "#    if not default model delete only this version\n",
    "\n",
    "print(\"Model Version=\", MODEL_VERSION)\n",
    "delete_model_versions(PROJECT_ID, MODEL_NAME, MODEL_VERSION)\n",
    "\n",
    "# List the models\n",
    "print(\"\\nList all models:\")\n",
    "!gcloud ai-platform models list\n",
    "\n",
    "# List all versions of the model\n",
    "_ = print_all_versions_of_model(PROJECT_ID, MODEL_NAME)\n",
    "#!gcloud ai-platform versions list --model $MODEL_NAME\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "# Get a list of model directories\n",
    "best_model_dir = best_model_dir_hyp  #best_model_dir_hyp\n",
    "print(\"Best Model Dir: \", best_model_dir)\n",
    "\n",
    "MODEL_FRAMEWORK = \"TENSORFLOW\"\n",
    "MODEL_DESCRIPTION = \"UNET_FPN_SL1\"\n",
    "MODEL_LABELS=\"team=total,phase=test,owner=\"+USER\n",
    "\n",
    "MACHINE_TYPE = \"mls1-c1-m2\"\n",
    "if (MODEL_VERSION == MODEL_VERSION_BATCH):\n",
    "    MACHINE_TYPE = \"mls1-c1-m2\"\n",
    "    MODEL_LABELS = MODEL_LABELS+\",mode=batch\"\n",
    "if (MODEL_VERSION == MODEL_VERSION_ONLINE):\n",
    "    MACHINE_TYPE = \"n1-standard-16\"\n",
    "    MODEL_LABELS = MODEL_LABELS+\",mode=online\"\n",
    "\n",
    "# Deploy the model\n",
    "! gcloud beta ai-platform versions create $MODEL_VERSION \\\n",
    "  --model $MODEL_NAME \\\n",
    "  --origin $best_model_dir \\\n",
    "  --runtime-version $RUNTIME_VERSION \\\n",
    "  --python-version=3.7 \\\n",
    "  --description=$MODEL_DESCRIPTION \\\n",
    "  --labels $MODEL_LABELS \\\n",
    "  --machine-type=$MACHINE_TYPE \n",
    "#  --framework $MODEL_FRAMEWORK \\\n",
    "#  --region=$REGION\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# List all models\n",
    "print(\"\")\n",
    "!gcloud ai-platform models list\n",
    "\n",
    "# List a ll versions of model\n",
    "_ = print_all_versions_of_model(PROJECT_ID, MODEL_NAME)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "------\n",
    "# Predictions with the deployed model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from trainer import inputs\n",
    "print(\"Input and pre-process data ...\")\n",
    "# Extract train_seismic, train_label\n",
    "data_dir = 'gs://codev-test-data/FAULT_TRAINING_SET/SYNTHETIC_OPTIM/seismic'\n",
    "label_dir = 'gs://codev-test-data/FAULT_TRAINING_SET/SYNTHETIC_OPTIM/label'\n",
    "test_dataset = inputs.load_data(data_dir, label_dir, range(0,16), 1)\n",
    "X_test1 = list(test_dataset.as_numpy_iterator())\n",
    "X_test1 = np.array(X_test1).astype(dtype='float32')\n",
    "X_test_images = X_test1[:,0,0,:,:,:]\n",
    "X_test_labels = X_test1[:,1,0,:,:,:]\n",
    "\n",
    "print(\"X_test shape =\", X_test_images.shape)\n",
    "\n",
    "# plot the results\n",
    "plot_images(X_test_images[:,:,:,0], X_test_labels[:,:,:,0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "from trainer import model\n",
    "\n",
    "# Copy the model from storage to local memory\n",
    "!gsutil -m cp -r $best_model_dir_hyp* ./model_dir\n",
    "\n",
    "# Load the model\n",
    "loaded_model = tf.keras.models.load_model('./model_dir', compile=False, \n",
    "               custom_objects={\"custom_loss\": model.custom_loss,\"custom_mse\": model.custom_mse})\n",
    "print(\"Signature \", loaded_model.signatures)\n",
    "\n",
    "# Check the model layers\n",
    "model_layers = [layer.name for layer in loaded_model.layers]\n",
    "print(\"\")\n",
    "print(\"Model Input  Layer=\", model_layers[0])\n",
    "print(\"Model Output Layer=\", model_layers[-1])\n",
    "print(\"\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Batch Prediction with GCLOUD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write batch data to file in GCS\n",
    "\n",
    "import shutil\n",
    "\n",
    "# Clean current directory\n",
    "DATA_DIR = './batch_data'\n",
    "shutil.rmtree(DATA_DIR, ignore_errors=True)\n",
    "os.makedirs(DATA_DIR)\n",
    "\n",
    "nTotalImages = X_test_images.shape[0]\n",
    "nFiles = min(100,nTotalImages)\n",
    "nImagesPerFile = min(100, nTotalImages//nFiles)\n",
    "print(\"nTotalImages =\", nTotalImages)\n",
    "print(\"nFiles =\", nFiles)\n",
    "print(\"nImagesPerFile =\", nImagesPerFile)\n",
    "\n",
    "# Create nFiles files with nImagesPerFile images each\n",
    "for i in range(nFiles):\n",
    "    with open(f'{DATA_DIR}/unkeyed_batch_{i}.json', \"w\") as file:\n",
    "        for z in range(nImagesPerFile):\n",
    "            print(f'{{\"{model_layers[0]}\": {X_test_images[i*nImagesPerFile+z].tolist()}}}', file=file)\n",
    "            #key = f'key_{i}_{z}'\n",
    "            #print(f'{{\"image\": {X_test_images[z].tolist()}, \"key\": \"{key}\"}}', file=file)\n",
    "\n",
    "# Write batch data to file\n",
    "! gsutil -m cp -r ./batch_data gs://$BUCKET_NAME/$FOLDER_NAME/\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "from pytz import timezone\n",
    "\n",
    "DATA_FORMAT=\"text\" # JSON data format\n",
    "INPUT_PATHS='gs://' + BUCKET_NAME + '/' + FOLDER_NAME + '/batch_data/*'\n",
    "OUTPUT_PATH='gs://' + BUCKET_NAME + '/' + FOLDER_NAME + '/batch_predictions'\n",
    "PRED_LABELS=\"mode=batch,team=engineering,phase=test,owner=\"+USER\n",
    "SIGNATURE_NAME=\"serving_default\"\n",
    "\n",
    "JOBNAME_BATCH = 'tf_batch_predict_'+ USER + '_' + \\\n",
    "           datetime.now(timezone('US/Pacific')).strftime(\"%m%d%y_%H%M\")\n",
    "\n",
    "print(\"INPUT_PATHS = \", INPUT_PATHS)\n",
    "print(\"OUTPUT_PATH = \", OUTPUT_PATH)\n",
    "print(\"Job Name    = \", JOBNAME_BATCH)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Submit batch predict job\n",
    "# Use  MODEL_VERSION_BATCH not MODEL_VERSION_ONLINE\n",
    "MODEL_VERSION = MODEL_VERSION_BATCH\n",
    "\n",
    "! gcloud ai-platform jobs submit prediction $JOBNAME_BATCH \\\n",
    "    --model $MODEL_NAME \\\n",
    "    --version $MODEL_VERSION \\\n",
    "    --input-paths $INPUT_PATHS \\\n",
    "    --output-path $OUTPUT_PATH \\\n",
    "    --region $REGION \\\n",
    "    --data-format $DATA_FORMAT \\\n",
    "    --labels $PRED_LABELS \\\n",
    "    --signature-name $SIGNATURE_NAME\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check the batch prediction job status\n",
    "! gcloud ai-platform jobs describe $JOBNAME_BATCH\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print Errors\n",
    "response = ! gcloud logging read \"resource.labels.job_id=$JOBNAME_BATCH severity>=ERROR\"\n",
    "for i in range(0,len(response)):\n",
    "    if 'message' in response[i]:\n",
    "        print(response[i])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!gsutil cat $OUTPUT_PATH/prediction.results-00000-of-00016\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Online Prediction with python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "# Use MODEL_VERSION_ONLINE not MODEL_VERSION_BATCH\n",
    "MODEL_VERSION = MODEL_VERSION_ONLINE\n",
    "\n",
    "from oauth2client.client import GoogleCredentials\n",
    "from googleapiclient import discovery\n",
    "from googleapiclient import errors\n",
    "import json\n",
    "\n",
    "#tf.compat.v1.logging.set_verbosity(tf.compat.v1.logging.ERROR)\n",
    "#tf.get_logger().setLevel('ERROR')\n",
    "\n",
    "project_id = 'projects/{}'.format(PROJECT_ID)\n",
    "model_name = '{}/models/{}'.format(project_id, MODEL_NAME)\n",
    "if MODEL_VERSION is not None:\n",
    "    model_name += '/versions/{}'.format(MODEL_VERSION)\n",
    "credentials = GoogleCredentials.get_application_default()\n",
    "service = discovery.build('ml', 'v1', cache_discovery=False, credentials=credentials)\n",
    "\n",
    "pprobas_temp = []\n",
    "batch_size = 1\n",
    "n_samples = min(1000,len(X_test_images))\n",
    "print(\"n_samples=\", n_samples)\n",
    "for i in range(0, n_samples, batch_size):\n",
    "    j = min(i+batch_size, n_samples)\n",
    "    print(\"Processing samples\", i, j)\n",
    "    request = service.projects().predict(name=model_name, \\\n",
    "                                         body={'instances': X_test_images[i:j].tolist()})\n",
    "    try:\n",
    "        response = request.execute()\n",
    "    except errors.HttpError as err:\n",
    "        # Something went wrong, print out some information.\n",
    "        tf.compat.v1.logging.error('There was an error getting the job info, Check the details:')\n",
    "        tf.compat.v1.logging.error(err._get_reason())\n",
    "\n",
    "    pprobas_temp += response['predictions']\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# If Model Version Online is used\n",
    "if (MODEL_VERSION_ONLINE in model_name):\n",
    "    print(\"Online Version is Used\")\n",
    "    pprobas = np.array(pprobas_temp)[:,:,:,0]\n",
    "    print(\"pprobas shape=\", pprobas.shape)\n",
    "    plot_images(X_test_images[:,:,:,0], pprobas)\n",
    "\n",
    "# If Model Version Batch is used\n",
    "if (MODEL_VERSION_BATCH in model_name):\n",
    "    print(\"Batch Version is Used\")\n",
    "    pprobas = [] \n",
    "    for i in range(0,n_samples):\n",
    "        pprobas.append(list(pprobas_temp[i].values()))\n",
    "    pprobas = np.array(pprobas)\n",
    "    pprobas = pprobas[:,0,:,:,0]\n",
    "    print(\"pprobas shape=\", pprobas.shape)\n",
    "    plot_images(X_test_images[:,:,:,0], pprobas)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "environment": {
   "name": "tf2-2-3-gpu.2-3.m56",
   "type": "gcloud",
   "uri": "gcr.io/deeplearning-platform-release/tf2-2-3-gpu.2-3:m56"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
