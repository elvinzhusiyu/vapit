{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ============================================================================== \\\n",
    " Copyright 2020 Google LLC. This software is provided as-is, without warranty \\\n",
    " or representation for any use or purpose. Your use of it is subject to your \\\n",
    " agreement with Google. \\\n",
    " ============================================================================== \n",
    " \n",
    " Author: Elvin Zhu, Chanchal Chatterjee \\\n",
    " Email: elvinzhu@google.com \\\n",
    "<img src=\"img/google-cloud-icon.jpg\" alt=\"Drawing\" style=\"width: 200px;\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Install pakcages requried for training, deployment and prediction with ai platform.\n",
    "\n",
    "https://cloud.google.com/ai-platform/training/docs/runtime-version-list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting tensorflow==2.1.0\n",
      "  Downloading tensorflow-2.1.0-cp37-cp37m-manylinux2010_x86_64.whl (421.8 MB)\n",
      "Collecting numpy==1.18.0\n",
      "  Downloading numpy-1.18.0-cp37-cp37m-manylinux1_x86_64.whl (20.1 MB)\n",
      "Collecting pandas==1.2.1\n",
      "  Downloading pandas-1.2.1-cp37-cp37m-manylinux1_x86_64.whl (9.9 MB)\n",
      "Collecting scipy==1.4.1\n",
      "  Downloading scipy-1.4.1-cp37-cp37m-manylinux1_x86_64.whl (26.1 MB)\n",
      "Collecting scikit-learn==0.22\n",
      "  Downloading scikit_learn-0.22-cp37-cp37m-manylinux1_x86_64.whl (7.0 MB)\n",
      "Collecting google-api-python-client==1.7.11\n",
      "  Downloading google-api-python-client-1.7.11.tar.gz (142 kB)\n",
      "Collecting gcsfs==0.6.1\n",
      "  Downloading gcsfs-0.6.1-py2.py3-none-any.whl (17 kB)\n",
      "Collecting google-cloud==0.34.0\n",
      "  Downloading google_cloud-0.34.0-py2.py3-none-any.whl (1.8 kB)\n",
      "Collecting google-cloud-storage==1.23.0\n",
      "  Downloading google_cloud_storage-1.23.0-py2.py3-none-any.whl (72 kB)\n",
      "Collecting matplotlib==3.2.1\n",
      "  Downloading matplotlib-3.2.1-cp37-cp37m-manylinux1_x86_64.whl (12.4 MB)\n",
      "Collecting xgboost==1.3.3\n",
      "  Downloading xgboost-1.3.3-py3-none-manylinux2010_x86_64.whl (157.5 MB)\n",
      "Collecting cloudml-hypertune\n",
      "  Downloading cloudml-hypertune-0.1.0.dev6.tar.gz (3.2 kB)\n",
      "Requirement already satisfied: astor>=0.6.0 in /opt/conda/lib/python3.7/site-packages (from tensorflow==2.1.0->-r ./requirements.txt (line 1)) (0.8.1)\n",
      "Requirement already satisfied: termcolor>=1.1.0 in /opt/conda/lib/python3.7/site-packages (from tensorflow==2.1.0->-r ./requirements.txt (line 1)) (1.1.0)\n",
      "Collecting tensorboard<2.2.0,>=2.1.0\n",
      "  Downloading tensorboard-2.1.1-py3-none-any.whl (3.8 MB)\n",
      "Requirement already satisfied: keras-applications>=1.0.8 in /opt/conda/lib/python3.7/site-packages (from tensorflow==2.1.0->-r ./requirements.txt (line 1)) (1.0.8)\n",
      "Requirement already satisfied: wrapt>=1.11.1 in /opt/conda/lib/python3.7/site-packages (from tensorflow==2.1.0->-r ./requirements.txt (line 1)) (1.12.1)\n",
      "Requirement already satisfied: wheel>=0.26 in /opt/conda/lib/python3.7/site-packages (from tensorflow==2.1.0->-r ./requirements.txt (line 1)) (0.36.2)\n",
      "Requirement already satisfied: six>=1.12.0 in /opt/conda/lib/python3.7/site-packages (from tensorflow==2.1.0->-r ./requirements.txt (line 1)) (1.16.0)\n",
      "Requirement already satisfied: opt-einsum>=2.3.2 in /opt/conda/lib/python3.7/site-packages (from tensorflow==2.1.0->-r ./requirements.txt (line 1)) (3.3.0)\n",
      "Requirement already satisfied: keras-preprocessing>=1.1.0 in /opt/conda/lib/python3.7/site-packages (from tensorflow==2.1.0->-r ./requirements.txt (line 1)) (1.1.0)\n",
      "Requirement already satisfied: gast==0.2.2 in /opt/conda/lib/python3.7/site-packages (from tensorflow==2.1.0->-r ./requirements.txt (line 1)) (0.2.2)\n",
      "Requirement already satisfied: protobuf>=3.8.0 in /opt/conda/lib/python3.7/site-packages (from tensorflow==2.1.0->-r ./requirements.txt (line 1)) (3.16.0)\n",
      "Requirement already satisfied: google-pasta>=0.1.6 in /opt/conda/lib/python3.7/site-packages (from tensorflow==2.1.0->-r ./requirements.txt (line 1)) (0.2.0)\n",
      "Requirement already satisfied: absl-py>=0.7.0 in /opt/conda/lib/python3.7/site-packages (from tensorflow==2.1.0->-r ./requirements.txt (line 1)) (0.8.1)\n",
      "Requirement already satisfied: tensorflow-estimator<2.2.0,>=2.1.0rc0 in /opt/conda/lib/python3.7/site-packages (from tensorflow==2.1.0->-r ./requirements.txt (line 1)) (2.1.0)\n",
      "Requirement already satisfied: grpcio>=1.8.6 in /opt/conda/lib/python3.7/site-packages (from tensorflow==2.1.0->-r ./requirements.txt (line 1)) (1.38.0)\n",
      "Requirement already satisfied: python-dateutil>=2.7.3 in /opt/conda/lib/python3.7/site-packages (from pandas==1.2.1->-r ./requirements.txt (line 3)) (2.8.1)\n",
      "Requirement already satisfied: pytz>=2017.3 in /opt/conda/lib/python3.7/site-packages (from pandas==1.2.1->-r ./requirements.txt (line 3)) (2021.1)\n",
      "Requirement already satisfied: joblib>=0.11 in /opt/conda/lib/python3.7/site-packages (from scikit-learn==0.22->-r ./requirements.txt (line 5)) (1.0.1)\n",
      "Requirement already satisfied: httplib2<1dev,>=0.9.2 in /opt/conda/lib/python3.7/site-packages (from google-api-python-client==1.7.11->-r ./requirements.txt (line 6)) (0.19.1)\n",
      "Requirement already satisfied: google-auth>=1.4.1 in /opt/conda/lib/python3.7/site-packages (from google-api-python-client==1.7.11->-r ./requirements.txt (line 6)) (1.30.0)\n",
      "Requirement already satisfied: google-auth-httplib2>=0.0.3 in /opt/conda/lib/python3.7/site-packages (from google-api-python-client==1.7.11->-r ./requirements.txt (line 6)) (0.1.0)\n",
      "Requirement already satisfied: uritemplate<4dev,>=3.0.0 in /opt/conda/lib/python3.7/site-packages (from google-api-python-client==1.7.11->-r ./requirements.txt (line 6)) (3.0.1)\n",
      "Requirement already satisfied: decorator in /opt/conda/lib/python3.7/site-packages (from gcsfs==0.6.1->-r ./requirements.txt (line 7)) (5.0.9)\n",
      "Requirement already satisfied: fsspec>=0.6.0 in /opt/conda/lib/python3.7/site-packages (from gcsfs==0.6.1->-r ./requirements.txt (line 7)) (2021.5.0)\n",
      "Requirement already satisfied: requests in /opt/conda/lib/python3.7/site-packages (from gcsfs==0.6.1->-r ./requirements.txt (line 7)) (2.25.1)\n",
      "Requirement already satisfied: google-auth-oauthlib in /opt/conda/lib/python3.7/site-packages (from gcsfs==0.6.1->-r ./requirements.txt (line 7)) (0.4.4)\n",
      "Requirement already satisfied: google-cloud-core<2.0dev,>=1.0.3 in /opt/conda/lib/python3.7/site-packages (from google-cloud-storage==1.23.0->-r ./requirements.txt (line 9)) (1.6.0)\n",
      "Collecting google-resumable-media<0.6dev,>=0.5.0\n",
      "  Downloading google_resumable_media-0.5.1-py2.py3-none-any.whl (38 kB)\n",
      "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /opt/conda/lib/python3.7/site-packages (from matplotlib==3.2.1->-r ./requirements.txt (line 10)) (2.4.7)\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in /opt/conda/lib/python3.7/site-packages (from matplotlib==3.2.1->-r ./requirements.txt (line 10)) (1.3.1)\n",
      "Requirement already satisfied: cycler>=0.10 in /opt/conda/lib/python3.7/site-packages (from matplotlib==3.2.1->-r ./requirements.txt (line 10)) (0.10.0)\n",
      "Requirement already satisfied: setuptools>=40.3.0 in /opt/conda/lib/python3.7/site-packages (from google-auth>=1.4.1->google-api-python-client==1.7.11->-r ./requirements.txt (line 6)) (49.6.0.post20210108)\n",
      "Requirement already satisfied: rsa<5,>=3.1.4 in /opt/conda/lib/python3.7/site-packages (from google-auth>=1.4.1->google-api-python-client==1.7.11->-r ./requirements.txt (line 6)) (4.7.2)\n",
      "Requirement already satisfied: cachetools<5.0,>=2.0.0 in /opt/conda/lib/python3.7/site-packages (from google-auth>=1.4.1->google-api-python-client==1.7.11->-r ./requirements.txt (line 6)) (4.2.2)\n",
      "Requirement already satisfied: pyasn1-modules>=0.2.1 in /opt/conda/lib/python3.7/site-packages (from google-auth>=1.4.1->google-api-python-client==1.7.11->-r ./requirements.txt (line 6)) (0.2.7)\n",
      "Requirement already satisfied: google-api-core<2.0.0dev,>=1.21.0 in /opt/conda/lib/python3.7/site-packages (from google-cloud-core<2.0dev,>=1.0.3->google-cloud-storage==1.23.0->-r ./requirements.txt (line 9)) (1.26.3)\n",
      "Requirement already satisfied: packaging>=14.3 in /opt/conda/lib/python3.7/site-packages (from google-api-core<2.0.0dev,>=1.21.0->google-cloud-core<2.0dev,>=1.0.3->google-cloud-storage==1.23.0->-r ./requirements.txt (line 9)) (20.9)\n",
      "Requirement already satisfied: googleapis-common-protos<2.0dev,>=1.6.0 in /opt/conda/lib/python3.7/site-packages (from google-api-core<2.0.0dev,>=1.21.0->google-cloud-core<2.0dev,>=1.0.3->google-cloud-storage==1.23.0->-r ./requirements.txt (line 9)) (1.53.0)\n",
      "Requirement already satisfied: h5py in /opt/conda/lib/python3.7/site-packages (from keras-applications>=1.0.8->tensorflow==2.1.0->-r ./requirements.txt (line 1)) (2.10.0)\n",
      "Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /opt/conda/lib/python3.7/site-packages (from pyasn1-modules>=0.2.1->google-auth>=1.4.1->google-api-python-client==1.7.11->-r ./requirements.txt (line 6)) (0.4.8)\n",
      "Requirement already satisfied: chardet<5,>=3.0.2 in /opt/conda/lib/python3.7/site-packages (from requests->gcsfs==0.6.1->-r ./requirements.txt (line 7)) (4.0.0)\n",
      "Requirement already satisfied: idna<3,>=2.5 in /opt/conda/lib/python3.7/site-packages (from requests->gcsfs==0.6.1->-r ./requirements.txt (line 7)) (2.10)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /opt/conda/lib/python3.7/site-packages (from requests->gcsfs==0.6.1->-r ./requirements.txt (line 7)) (1.26.5)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.7/site-packages (from requests->gcsfs==0.6.1->-r ./requirements.txt (line 7)) (2020.12.5)\n",
      "Requirement already satisfied: werkzeug>=0.11.15 in /opt/conda/lib/python3.7/site-packages (from tensorboard<2.2.0,>=2.1.0->tensorflow==2.1.0->-r ./requirements.txt (line 1)) (2.0.1)\n",
      "Requirement already satisfied: markdown>=2.6.8 in /opt/conda/lib/python3.7/site-packages (from tensorboard<2.2.0,>=2.1.0->tensorflow==2.1.0->-r ./requirements.txt (line 1)) (3.3.4)\n",
      "Requirement already satisfied: requests-oauthlib>=0.7.0 in /opt/conda/lib/python3.7/site-packages (from google-auth-oauthlib->gcsfs==0.6.1->-r ./requirements.txt (line 7)) (1.3.0)\n",
      "Requirement already satisfied: importlib-metadata in /opt/conda/lib/python3.7/site-packages (from markdown>=2.6.8->tensorboard<2.2.0,>=2.1.0->tensorflow==2.1.0->-r ./requirements.txt (line 1)) (4.2.0)\n",
      "Requirement already satisfied: oauthlib>=3.0.0 in /opt/conda/lib/python3.7/site-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib->gcsfs==0.6.1->-r ./requirements.txt (line 7)) (3.0.1)\n",
      "Requirement already satisfied: typing-extensions>=3.6.4 in /opt/conda/lib/python3.7/site-packages (from importlib-metadata->markdown>=2.6.8->tensorboard<2.2.0,>=2.1.0->tensorflow==2.1.0->-r ./requirements.txt (line 1)) (3.7.4.3)\n",
      "Requirement already satisfied: zipp>=0.5 in /opt/conda/lib/python3.7/site-packages (from importlib-metadata->markdown>=2.6.8->tensorboard<2.2.0,>=2.1.0->tensorflow==2.1.0->-r ./requirements.txt (line 1)) (3.4.1)\n",
      "Building wheels for collected packages: google-api-python-client, cloudml-hypertune\n",
      "  Building wheel for google-api-python-client (setup.py): started\n",
      "  Building wheel for google-api-python-client (setup.py): finished with status 'done'\n",
      "  Created wheel for google-api-python-client: filename=google_api_python_client-1.7.11-py3-none-any.whl size=56530 sha256=e3a44ac80df83b6eadd3a57be572ae48354b06c4fe05083f73a8ba2cff68f195\n",
      "  Stored in directory: /home/jupyter/.cache/pip/wheels/b6/04/f6/ae7211f4531abd8ac91e3fae5d19a4d8b52803362548ea9f66\n",
      "  Building wheel for cloudml-hypertune (setup.py): started\n",
      "  Building wheel for cloudml-hypertune (setup.py): finished with status 'done'\n",
      "  Created wheel for cloudml-hypertune: filename=cloudml_hypertune-0.1.0.dev6-py2.py3-none-any.whl size=3988 sha256=b53a7daf188d98f26a40e9806881a6186458940ce5bd76a1c0302529a78f99be\n",
      "  Stored in directory: /home/jupyter/.cache/pip/wheels/a7/ff/87/e7bed0c2741fe219b3d6da67c2431d7f7fedb183032e00f81e\n",
      "Successfully built google-api-python-client cloudml-hypertune\n",
      "Installing collected packages: numpy, tensorboard, scipy, google-resumable-media, xgboost, tensorflow, scikit-learn, pandas, matplotlib, google-cloud-storage, google-cloud, google-api-python-client, gcsfs, cloudml-hypertune\n",
      "Successfully installed cloudml-hypertune-0.1.0.dev6 gcsfs-0.6.1 google-api-python-client-1.7.11 google-cloud-0.34.0 google-cloud-storage-1.23.0 google-resumable-media-0.5.1 matplotlib-3.2.1 numpy-1.18.0 pandas-1.2.1 scikit-learn-0.22 scipy-1.4.1 tensorboard-2.1.1 tensorflow-2.1.0 xgboost-1.3.3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  WARNING: The scripts f2py, f2py3 and f2py3.7 are installed in '/home/jupyter/.local/bin' which is not on PATH.\n",
      "  Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.\n",
      "  WARNING: The script tensorboard is installed in '/home/jupyter/.local/bin' which is not on PATH.\n",
      "  Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.\n",
      "  WARNING: The scripts estimator_ckpt_converter, saved_model_cli, tensorboard, tf_upgrade_v2, tflite_convert, toco and toco_from_protos are installed in '/home/jupyter/.local/bin' which is not on PATH.\n",
      "  Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.\n",
      "ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "witwidget 1.7.0 requires oauth2client>=4.1.3, but you have oauth2client 3.0.0 which is incompatible.\n",
      "tfx 0.21.5 requires click<8,>=7, but you have click 8.0.1 which is incompatible.\n",
      "tfx 0.21.5 requires docker<5,>=4.1, but you have docker 5.0.0 which is incompatible.\n",
      "tfx 0.21.5 requires kubernetes<11,>=10.0.1, but you have kubernetes 17.17.0 which is incompatible.\n",
      "tfx 0.21.5 requires pyarrow<0.16,>=0.15, but you have pyarrow 4.0.0 which is incompatible.\n",
      "tfx-bsl 0.21.4 requires pyarrow<0.16.0,>=0.15.0, but you have pyarrow 4.0.0 which is incompatible.\n",
      "tensorflow-model-analysis 0.21.6 requires pyarrow<1,>=0.15, but you have pyarrow 4.0.0 which is incompatible.\n",
      "tensorflow-data-validation 0.21.5 requires joblib<0.15,>=0.12, but you have joblib 1.0.1 which is incompatible.\n",
      "tensorflow-data-validation 0.21.5 requires pandas<1,>=0.24, but you have pandas 1.2.1 which is incompatible.\n",
      "tensorflow-data-validation 0.21.5 requires scikit-learn<0.22,>=0.18, but you have scikit-learn 0.22 which is incompatible.\n",
      "tensorflow-cloud 0.1.13 requires tensorboard>=2.3.0, but you have tensorboard 2.1.1 which is incompatible.\n",
      "phik 0.11.2 requires scipy>=1.5.2, but you have scipy 1.4.1 which is incompatible.\n",
      "google-cloud-bigquery 2.17.0 requires google-resumable-media<2.0dev,>=0.6.0, but you have google-resumable-media 0.5.1 which is incompatible.\n",
      "google-cloud-aiplatform 1.0.1 requires google-cloud-storage<2.0.0dev,>=1.32.0, but you have google-cloud-storage 1.23.0 which is incompatible.\n",
      "explainable-ai-sdk 1.2.2 requires matplotlib>=3.2.2, but you have matplotlib 3.2.1 which is incompatible.\n",
      "caip-notebooks-serverextension 1.0.0 requires google-cloud-storage>=1.24.1, but you have google-cloud-storage 1.23.0 which is incompatible.\n",
      "apache-beam 2.17.0 requires httplib2<=0.12.0,>=0.8, but you have httplib2 0.19.1 which is incompatible.\n",
      "apache-beam 2.17.0 requires pyarrow<0.16.0,>=0.15.1; python_version >= \"3.0\" or platform_system != \"Windows\", but you have pyarrow 4.0.0 which is incompatible.\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "cd /home/jupyter/vapit/ai-platform-xgboost\n",
    "python3 -m pip install -r ./requirements.txt --user"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create training application package\n",
    "\n",
    "The easiest (and recommended) way to create a training application package uses gcloud to package and upload the application when you submit your training job. This method allows you to create a very simple file structure. For this tutorial, the file structure of your training application package should appear similar to the following:\n",
    "\n",
    "```\n",
    "config/\n",
    "    config.yaml\n",
    "    config_hpt.yaml\n",
    "    \n",
    "trainer/ \n",
    "    __init__.py\n",
    "    train.py\n",
    "    train_hpt.py\n",
    "```\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile ./setup.py\n",
    "\n",
    "# python3\n",
    "# ==============================================================================\n",
    "# Copyright 2020 Google LLC. This software is provided as-is, without warranty\n",
    "# or representation for any use or purpose. Your use of it is subject to your\n",
    "# agreement with Google.\n",
    "# ==============================================================================\n",
    "\n",
    "from setuptools import find_packages\n",
    "from setuptools import setup\n",
    "\n",
    "REQUIRED_PACKAGES = [\n",
    "    'tensorflow==2.1.0',\n",
    "    'numpy==1.18.0',\n",
    "    'pandas==1.2.1',\n",
    "    'scipy==1.4.1',\n",
    "    'scikit-learn==0.22',\n",
    "    'google-cloud-storage==1.23.0',\n",
    "    'xgboost==1.3.3',\n",
    "    'cloudml-hypertune',\n",
    "    ]\n",
    " \n",
    "setup(\n",
    "    name='trainer',\n",
    "    version='0.1',\n",
    "    install_requires=REQUIRED_PACKAGES,\n",
    "    packages=find_packages(),\n",
    "    include_package_data=True,\n",
    "    description='Trainer package for XGBoost Task'\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile ./trainer/__init__.py\n",
    "\n",
    "# python3\n",
    "# ==============================================================================\n",
    "# Copyright 2020 Google LLC. This software is provided as-is, without warranty\n",
    "# or representation for any use or purpose. Your use of it is subject to your\n",
    "# agreement with Google.\n",
    "# ==============================================================================\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create your training code (Example showed here is to use XGBoost to classify structured mortgage data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting ./trainer/train.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile ./trainer/train.py\n",
    "\n",
    "\n",
    "# python3\n",
    "# ==============================================================================\n",
    "# Copyright 2020 Google LLC. This software is provided as-is, without warranty\n",
    "# or representation for any use or purpose. Your use of it is subject to your\n",
    "# agreement with Google.\n",
    "# ==============================================================================\n",
    "\n",
    "import argparse\n",
    "# import hypertune\n",
    "import os\n",
    "import subprocess\n",
    "import sys\n",
    "import pandas as pd\n",
    "from sklearn import metrics\n",
    "from xgboost import XGBClassifier\n",
    "\n",
    "def train_xgboost(args):\n",
    "    \"\"\" Train a XGBoost model\n",
    "    Args:\n",
    "        args: structure with the following field:\n",
    "            bucket_name, str, gcs bucket name to store trained model\n",
    "            blob_name, str, gcs blob name to store trained model\n",
    "            train_feature_name, str, name of the train feature csv\n",
    "            train_label_name, str, name of train label csv\n",
    "            no_classes, int, number of prediction classes in the model\n",
    "            n_estimators, int, number of estimators (hypertune)\n",
    "            max_depth, int, maximum depth of trees (hypertune)\n",
    "            booster, str, type of boosters (hypertune)\n",
    "    Return:\n",
    "        xgboost model object\n",
    "    \n",
    "    \"\"\"\n",
    "    \n",
    "    x_train = pd.read_csv(args.train_feature_name)\n",
    "    y_train = pd.read_csv(args.train_label_name)\n",
    "   \n",
    "    # ---------------------------------------\n",
    "    # Train model\n",
    "    # ---------------------------------------\n",
    "\n",
    "    params = {\n",
    "        'n_estimators': args.n_estimators,\n",
    "        'max_depth': args.max_depth,\n",
    "        'booster': args.booster,\n",
    "        'min_child_weight': 1,\n",
    "        'learning_rate': 0.1,\n",
    "        'gamma': 0,\n",
    "        'subsample': 1,\n",
    "        'colsample_bytree': 1,\n",
    "        'reg_alpha': 0,\n",
    "        'objective': 'multi:softprob',\n",
    "        'num_class': args.no_classes,\n",
    "        }\n",
    "    xgb_model = XGBClassifier(**params, use_label_encoder=False)\n",
    "    xgb_model.fit(x_train, y_train)\n",
    "\n",
    "    # ---------------------------------------\n",
    "    # Save the model to local\n",
    "    # ---------------------------------------\n",
    "\n",
    "    temp_name = './model.bst'\n",
    "    bst = xgb_model.get_booster()\n",
    "    bst.save_model(temp_name)\n",
    "    \n",
    "    # ---------------------------------------\n",
    "    # Move local model to gcs\n",
    "    # ---------------------------------------\n",
    "    \n",
    "    target_path = os.path.join(args.job_dir, 'model.bst')\n",
    "    if temp_name != target_path:\n",
    "        subprocess.check_call(['gsutil', 'cp', temp_name, target_path],\n",
    "            stderr=sys.stdout)\n",
    "\n",
    "    return xgb_model\n",
    "    \n",
    "if __name__ == \"__main__\":\n",
    "    parser = argparse.ArgumentParser()\n",
    "    parser.add_argument(\"--job-dir\", type=str, help=\"Required by ai platform training\", default='./')\n",
    "    parser.add_argument(\"--train_feature_name\", type=str, help=\"Path to training feature csv file\")\n",
    "    parser.add_argument(\"--train_label_name\", type=str, help=\"Path to training label csv file\")\n",
    "    parser.add_argument(\"--no_classes\", type=int, help=\"Number of target classes in the label\")\n",
    "    parser.add_argument(\"--n_estimators\", type=int, help=\"Number of estimators in the xgboost model\")\n",
    "    parser.add_argument(\"--max_depth\", type=int, help=\"Maximum depth of trees in xgboost\")\n",
    "    parser.add_argument(\"--booster\", type=str, help=\"Type of booster\")\n",
    "    args = parser.parse_args()\n",
    "    model = train_xgboost(args)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create another version of training script which implement metric reporting summary for hyperparameter tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile ./trainer/train_hpt.py\n",
    "\n",
    "# python3\n",
    "# ==============================================================================\n",
    "# Copyright 2020 Google LLC. This software is provided as-is, without warranty\n",
    "# or representation for any use or purpose. Your use of it is subject to your\n",
    "# agreement with Google.\n",
    "# ==============================================================================\n",
    "\n",
    "import argparse\n",
    "import hypertune\n",
    "import os\n",
    "import subprocess\n",
    "import sys\n",
    "import pandas as pd\n",
    "from sklearn import metrics\n",
    "from xgboost import XGBClassifier\n",
    "\n",
    "from sklearn import preprocessing\n",
    "import hypertune\n",
    "\n",
    "def train_xgboost(args):\n",
    "    \"\"\" Train a XGBoost model\n",
    "    Args:\n",
    "        args: structure with the following field:\n",
    "            bucket_name, str, gcs bucket name to store trained model\n",
    "            blob_name, str, gcs blob name to store trained model\n",
    "            train_feature_name, str, name of the train feature csv\n",
    "            train_label_name, str, name of train label csv\n",
    "            no_classes, int, number of prediction classes in the model\n",
    "            n_estimators, int, number of estimators (hypertune)\n",
    "            max_depth, int, maximum depth of trees (hypertune)\n",
    "            booster, str, type of boosters (hypertune)\n",
    "    Return:\n",
    "        xgboost model object\n",
    "    \n",
    "    \"\"\"\n",
    "    \n",
    "    x_train = pd.read_csv(args.train_feature_name)\n",
    "    y_train = pd.read_csv(args.train_label_name)\n",
    "   \n",
    "    # ---------------------------------------\n",
    "    # Train model\n",
    "    # ---------------------------------------\n",
    "\n",
    "    params = {\n",
    "        'n_estimators': args.n_estimators,\n",
    "        'max_depth': args.max_depth,\n",
    "        'booster': args.booster,\n",
    "        'min_child_weight': 1,\n",
    "        'learning_rate': 0.1,\n",
    "        'gamma': 0,\n",
    "        'subsample': 1,\n",
    "        'colsample_bytree': 1,\n",
    "        'reg_alpha': 0,\n",
    "        'objective': 'multi:softprob',\n",
    "        'num_class': args.no_classes,\n",
    "        }\n",
    "    xgb_model = XGBClassifier(**params, use_label_encoder=False)\n",
    "    print(x_train.shape)\n",
    "    print(y_train.shape)\n",
    "    xgb_model.fit(x_train, y_train)\n",
    "\n",
    "    # ---------------------------------------\n",
    "    # Save the model to local\n",
    "    # ---------------------------------------\n",
    "\n",
    "    temp_name = 'model.bst'\n",
    "    bst = xgb_model.get_booster()\n",
    "    bst.save_model(temp_name)\n",
    "    \n",
    "    # ---------------------------------------\n",
    "    # Move local model to gcs\n",
    "    # ---------------------------------------\n",
    "    \n",
    "    subprocess.check_call(['gsutil', 'cp', temp_name, os.path.join(args.job_dir, 'model.bst')],\n",
    "        stderr=sys.stdout)\n",
    "\n",
    "    return xgb_model\n",
    "\n",
    "def test_xgboost(xgb_model, args):\n",
    "\n",
    "    # Load test data\n",
    "    x_val = pd.read_csv(args.val_feature_name)\n",
    "    y_val = pd.read_csv(args.val_label_name)\n",
    "    \n",
    "    # Perform predictions\n",
    "    pred_val = xgb_model.predict(x_val)\n",
    "    \n",
    "    # One-hot encoding class labels\n",
    "    lb = preprocessing.LabelBinarizer()\n",
    "    lb.fit(y_val)\n",
    "    y_val = lb.transform(y_val)\n",
    "    pred_val = lb.transform(pred_val)\n",
    "\n",
    "    # Define the score we want to use to evaluate the classifier on\n",
    "    score = metrics.roc_auc_score(y_val, pred_val, average='macro')\n",
    "    return score\n",
    "    \n",
    "if __name__ == \"__main__\":\n",
    "    parser = argparse.ArgumentParser()\n",
    "    parser.add_argument(\"--job-dir\", type=str, help=\"Required by ai platform training\", default='./')\n",
    "    parser.add_argument(\"--train_feature_name\", type=str, help=\"Path to training feature csv file\")\n",
    "    parser.add_argument(\"--train_label_name\", type=str, help=\"Path to training label csv file\")\n",
    "    parser.add_argument(\"--val_feature_name\", type=str, help=\"Path to validation feature csv file\")\n",
    "    parser.add_argument(\"--val_label_name\", type=str, help=\"Path to validation label csv file\")\n",
    "    parser.add_argument(\"--no_classes\", type=int, help=\"Number of target classes in the label\")\n",
    "    parser.add_argument(\"--n_estimators\", type=int, help=\"Number of estimators in the xgboost model\")\n",
    "    parser.add_argument(\"--max_depth\", type=int, help=\"Maximum depth of trees in xgboost\")\n",
    "    parser.add_argument(\"--booster\", type=str, help=\"Type of booster\")\n",
    "    args = parser.parse_args()\n",
    "\n",
    "    xgb_model = train_xgboost(args)\n",
    "    score = test_xgboost(xgb_model, args)\n",
    "    \n",
    "    # The default name of the metric is training/hptuning/metric. \n",
    "    # We recommend that you assign a custom name. The only functional difference is that \n",
    "    # if you use a custom name, you must set the hyperparameterMetricTag value in the \n",
    "    # HyperparameterSpec object in your job request to match your chosen name.\n",
    "    # https://cloud.google.com/ml-engine/reference/rest/v1/projects.jobs#HyperparameterSpec\n",
    "    hpt = hypertune.HyperTune()\n",
    "    hpt.report_hyperparameter_tuning_metric(\n",
    "        metric_value=score,\n",
    "        hyperparameter_metric_tag='roc_auc',\n",
    "        global_step=1000\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Configure for AI Platform Training\n",
    "Create config file for Cloud AI Platform training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile ./config/config.yaml\n",
    "# python3\n",
    "# ==============================================================================\n",
    "# Copyright 2020 Google LLC. This software is provided as-is, without warranty\n",
    "# or representation for any use or purpose. Your use of it is subject to your\n",
    "# agreement with Google.\n",
    "# ==============================================================================\n",
    "\n",
    "#trainingInput:\n",
    "#  scaleTier: CUSTOM\n",
    "#  masterType: n1-highmem-8\n",
    "#  masterConfig:\n",
    "#    acceleratorConfig:\n",
    "#      count: 1\n",
    "#      type: NVIDIA_TESLA_T4\n",
    "\n",
    "trainingInput:\n",
    "  scaleTier: STANDARD-1\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Configure for Hyperparameter Tuning\n",
    "Similarly create config file for Cloud AI Platform Hyperparameter tuning. Moreover, the hyperparameter search space is needed to be configured.\n",
    "\n",
    "The supported hyperparameter types are listed in the job reference documentation. In the ParameterSpec object, you specify the type for each hyperparameter and the related value ranges as described in the following table:\n",
    "\n",
    "|Type        | Value ranges        |Value data            |\n",
    "|------------|---------------------|----------------------|\n",
    "|DOUBLE      |minValue & maxValue  | Floating-point values|\n",
    "|INTEGER     |minValue & maxValue  |Integer values        |\n",
    "|CATEGORICAL |categoricalValues    |List of category strings|\n",
    "|DISCRETE    |discreteValues       |List of values in ascending order|\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile ./config/config_hpt.yaml\n",
    "\n",
    "# python3\n",
    "# ==============================================================================\n",
    "# Copyright 2020 Google LLC. This software is provided as-is, without warranty\n",
    "# or representation for any use or purpose. Your use of it is subject to your\n",
    "# agreement with Google.\n",
    "# ==============================================================================\n",
    "\n",
    "\n",
    "# hptuning_config.yaml\n",
    "trainingInput:\n",
    "  scaleTier: STANDARD-1 \n",
    "  hyperparameters:\n",
    "    goal: MAXIMIZE\n",
    "    maxTrials: 5\n",
    "    maxParallelTrials: 5\n",
    "    hyperparameterMetricTag: roc_auc\n",
    "    enableTrialEarlyStopping: TRUE\n",
    "    params:\n",
    "      - parameterName: max_depth\n",
    "        type: INTEGER\n",
    "        minValue: 3\n",
    "        maxValue: 8\n",
    "      - parameterName: n_estimators\n",
    "        type: INTEGER\n",
    "        minValue: 50\n",
    "        maxValue: 200\n",
    "      - parameterName: booster\n",
    "        type: CATEGORICAL\n",
    "        categoricalValues: [\n",
    "          \"gbtree\",\n",
    "          \"gblinear\",\n",
    "          \"dart\"\n",
    "        ]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "environment": {
   "name": "tf2-gpu.2-1.m71",
   "type": "gcloud",
   "uri": "gcr.io/deeplearning-platform-release/tf2-gpu.2-1:m71"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
