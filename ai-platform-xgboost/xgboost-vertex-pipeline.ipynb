{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ============================================================================== \\\n",
    " Copyright 2020 Google LLC. This software is provided as-is, without warranty \\\n",
    " or representation for any use or purpose. Your use of it is subject to your \\\n",
    " agreement with Google. \\\n",
    " ============================================================================== \n",
    " \n",
    " Author: Elvin Zhu, Chanchal Chatterjee \\\n",
    " Email: elvinzhu@google.com \\\n",
    "<img src=\"img/google-cloud-icon.jpg\" alt=\"Drawing\" style=\"width: 200px;\"/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: kfp==1.6.2 in /opt/conda/lib/python3.7/site-packages (1.6.2)\n",
      "Requirement already satisfied: google-cloud-storage<2,>=1.20.0 in /opt/conda/lib/python3.7/site-packages (from kfp==1.6.2) (1.32.0)\n",
      "Requirement already satisfied: google-api-python-client<2,>=1.7.8 in /home/jupyter/.local/lib/python3.7/site-packages (from kfp==1.6.2) (1.7.11)\n",
      "Requirement already satisfied: google-auth<2,>=1.6.1 in /opt/conda/lib/python3.7/site-packages (from kfp==1.6.2) (1.24.0)\n",
      "Requirement already satisfied: docstring-parser<1,>=0.7.3 in /opt/conda/lib/python3.7/site-packages (from kfp==1.6.2) (0.7.3)\n",
      "Requirement already satisfied: kfp-pipeline-spec<0.2.0,>=0.1.7 in /opt/conda/lib/python3.7/site-packages (from kfp==1.6.2) (0.1.8)\n",
      "Requirement already satisfied: kfp-server-api<2.0.0,>=1.1.2 in /opt/conda/lib/python3.7/site-packages (from kfp==1.6.2) (1.3.0)\n",
      "Requirement already satisfied: click<8,>=7.1.1 in /opt/conda/lib/python3.7/site-packages (from kfp==1.6.2) (7.1.2)\n",
      "Requirement already satisfied: PyYAML<6,>=5.3 in /opt/conda/lib/python3.7/site-packages (from kfp==1.6.2) (5.3.1)\n",
      "Requirement already satisfied: absl-py<=0.11,>=0.9 in /opt/conda/lib/python3.7/site-packages (from kfp==1.6.2) (0.11.0)\n",
      "Requirement already satisfied: Deprecated<2,>=1.2.7 in /opt/conda/lib/python3.7/site-packages (from kfp==1.6.2) (1.2.11)\n",
      "Requirement already satisfied: protobuf<4,>=3.13.0 in /opt/conda/lib/python3.7/site-packages (from kfp==1.6.2) (3.14.0)\n",
      "Requirement already satisfied: strip-hints<1,>=0.1.8 in /opt/conda/lib/python3.7/site-packages (from kfp==1.6.2) (0.1.9)\n",
      "Requirement already satisfied: kubernetes<13,>=8.0.0 in /opt/conda/lib/python3.7/site-packages (from kfp==1.6.2) (11.0.0)\n",
      "Requirement already satisfied: cloudpickle<2,>=1.3.0 in /opt/conda/lib/python3.7/site-packages (from kfp==1.6.2) (1.6.0)\n",
      "Requirement already satisfied: requests-toolbelt<1,>=0.8.0 in /opt/conda/lib/python3.7/site-packages (from kfp==1.6.2) (0.9.1)\n",
      "Requirement already satisfied: jsonschema<4,>=3.0.1 in /opt/conda/lib/python3.7/site-packages (from kfp==1.6.2) (3.2.0)\n",
      "Requirement already satisfied: fire<1,>=0.3.1 in /opt/conda/lib/python3.7/site-packages (from kfp==1.6.2) (0.4.0)\n",
      "Requirement already satisfied: tabulate<1,>=0.8.6 in /opt/conda/lib/python3.7/site-packages (from kfp==1.6.2) (0.8.7)\n",
      "Requirement already satisfied: six in /opt/conda/lib/python3.7/site-packages (from absl-py<=0.11,>=0.9->kfp==1.6.2) (1.15.0)\n",
      "Requirement already satisfied: wrapt<2,>=1.10 in /opt/conda/lib/python3.7/site-packages (from Deprecated<2,>=1.2.7->kfp==1.6.2) (1.12.1)\n",
      "Requirement already satisfied: termcolor in /opt/conda/lib/python3.7/site-packages (from fire<1,>=0.3.1->kfp==1.6.2) (1.1.0)\n",
      "Requirement already satisfied: uritemplate<4dev,>=3.0.0 in /opt/conda/lib/python3.7/site-packages (from google-api-python-client<2,>=1.7.8->kfp==1.6.2) (3.0.1)\n",
      "Requirement already satisfied: google-auth-httplib2>=0.0.3 in /opt/conda/lib/python3.7/site-packages (from google-api-python-client<2,>=1.7.8->kfp==1.6.2) (0.0.4)\n",
      "Requirement already satisfied: httplib2<1dev,>=0.9.2 in /opt/conda/lib/python3.7/site-packages (from google-api-python-client<2,>=1.7.8->kfp==1.6.2) (0.18.1)\n",
      "Requirement already satisfied: setuptools>=40.3.0 in /opt/conda/lib/python3.7/site-packages (from google-auth<2,>=1.6.1->kfp==1.6.2) (51.1.0)\n",
      "Requirement already satisfied: rsa<5,>=3.1.4 in /opt/conda/lib/python3.7/site-packages (from google-auth<2,>=1.6.1->kfp==1.6.2) (4.6)\n",
      "Requirement already satisfied: cachetools<5.0,>=2.0.0 in /opt/conda/lib/python3.7/site-packages (from google-auth<2,>=1.6.1->kfp==1.6.2) (4.1.1)\n",
      "Requirement already satisfied: pyasn1-modules>=0.2.1 in /opt/conda/lib/python3.7/site-packages (from google-auth<2,>=1.6.1->kfp==1.6.2) (0.2.8)\n",
      "Requirement already satisfied: google-resumable-media<2.0dev,>=1.0.0 in /opt/conda/lib/python3.7/site-packages (from google-cloud-storage<2,>=1.20.0->kfp==1.6.2) (1.3.0)\n",
      "Requirement already satisfied: requests<3.0.0dev,>=2.18.0 in /opt/conda/lib/python3.7/site-packages (from google-cloud-storage<2,>=1.20.0->kfp==1.6.2) (2.25.1)\n",
      "Requirement already satisfied: google-cloud-core<2.0dev,>=1.4.1 in /opt/conda/lib/python3.7/site-packages (from google-cloud-storage<2,>=1.20.0->kfp==1.6.2) (1.6.0)\n",
      "Requirement already satisfied: google-api-core<2.0.0dev,>=1.21.0 in /opt/conda/lib/python3.7/site-packages (from google-cloud-core<2.0dev,>=1.4.1->google-cloud-storage<2,>=1.20.0->kfp==1.6.2) (1.22.4)\n",
      "Requirement already satisfied: googleapis-common-protos<2.0dev,>=1.6.0 in /opt/conda/lib/python3.7/site-packages (from google-api-core<2.0.0dev,>=1.21.0->google-cloud-core<2.0dev,>=1.4.1->google-cloud-storage<2,>=1.20.0->kfp==1.6.2) (1.52.0)\n",
      "Requirement already satisfied: pytz in /opt/conda/lib/python3.7/site-packages (from google-api-core<2.0.0dev,>=1.21.0->google-cloud-core<2.0dev,>=1.4.1->google-cloud-storage<2,>=1.20.0->kfp==1.6.2) (2020.5)\n",
      "Requirement already satisfied: google-crc32c<2.0dev,>=1.0 in /opt/conda/lib/python3.7/site-packages (from google-resumable-media<2.0dev,>=1.0.0->google-cloud-storage<2,>=1.20.0->kfp==1.6.2) (1.1.0)\n",
      "Requirement already satisfied: cffi>=1.0.0 in /opt/conda/lib/python3.7/site-packages (from google-crc32c<2.0dev,>=1.0->google-resumable-media<2.0dev,>=1.0.0->google-cloud-storage<2,>=1.20.0->kfp==1.6.2) (1.14.4)\n",
      "Requirement already satisfied: pycparser in /opt/conda/lib/python3.7/site-packages (from cffi>=1.0.0->google-crc32c<2.0dev,>=1.0->google-resumable-media<2.0dev,>=1.0.0->google-cloud-storage<2,>=1.20.0->kfp==1.6.2) (2.20)\n",
      "Requirement already satisfied: importlib-metadata in /opt/conda/lib/python3.7/site-packages (from jsonschema<4,>=3.0.1->kfp==1.6.2) (3.3.0)\n",
      "Requirement already satisfied: pyrsistent>=0.14.0 in /opt/conda/lib/python3.7/site-packages (from jsonschema<4,>=3.0.1->kfp==1.6.2) (0.17.3)\n",
      "Requirement already satisfied: attrs>=17.4.0 in /opt/conda/lib/python3.7/site-packages (from jsonschema<4,>=3.0.1->kfp==1.6.2) (20.3.0)\n",
      "Requirement already satisfied: python-dateutil in /opt/conda/lib/python3.7/site-packages (from kfp-server-api<2.0.0,>=1.1.2->kfp==1.6.2) (2.8.1)\n",
      "Requirement already satisfied: urllib3>=1.15 in /opt/conda/lib/python3.7/site-packages (from kfp-server-api<2.0.0,>=1.1.2->kfp==1.6.2) (1.26.2)\n",
      "Requirement already satisfied: certifi in /opt/conda/lib/python3.7/site-packages (from kfp-server-api<2.0.0,>=1.1.2->kfp==1.6.2) (2020.12.5)\n",
      "Requirement already satisfied: websocket-client!=0.40.0,!=0.41.*,!=0.42.*,>=0.32.0 in /opt/conda/lib/python3.7/site-packages (from kubernetes<13,>=8.0.0->kfp==1.6.2) (0.57.0)\n",
      "Requirement already satisfied: requests-oauthlib in /opt/conda/lib/python3.7/site-packages (from kubernetes<13,>=8.0.0->kfp==1.6.2) (1.3.0)\n",
      "Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /opt/conda/lib/python3.7/site-packages (from pyasn1-modules>=0.2.1->google-auth<2,>=1.6.1->kfp==1.6.2) (0.4.8)\n",
      "Requirement already satisfied: chardet<5,>=3.0.2 in /opt/conda/lib/python3.7/site-packages (from requests<3.0.0dev,>=2.18.0->google-cloud-storage<2,>=1.20.0->kfp==1.6.2) (4.0.0)\n",
      "Requirement already satisfied: idna<3,>=2.5 in /opt/conda/lib/python3.7/site-packages (from requests<3.0.0dev,>=2.18.0->google-cloud-storage<2,>=1.20.0->kfp==1.6.2) (2.10)\n",
      "Requirement already satisfied: wheel in /opt/conda/lib/python3.7/site-packages (from strip-hints<1,>=0.1.8->kfp==1.6.2) (0.36.2)\n",
      "Requirement already satisfied: zipp>=0.5 in /opt/conda/lib/python3.7/site-packages (from importlib-metadata->jsonschema<4,>=3.0.1->kfp==1.6.2) (3.4.0)\n",
      "Requirement already satisfied: typing-extensions>=3.6.4 in /opt/conda/lib/python3.7/site-packages (from importlib-metadata->jsonschema<4,>=3.0.1->kfp==1.6.2) (3.7.4.3)\n",
      "Requirement already satisfied: oauthlib>=3.0.0 in /opt/conda/lib/python3.7/site-packages (from requests-oauthlib->kubernetes<13,>=8.0.0->kfp==1.6.2) (3.1.0)\n"
     ]
    }
   ],
   "source": [
    "!python3 -m pip install kfp==1.6.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import kfp\n",
    "import kfp.components as comp\n",
    "import kfp.dsl as dsl\n",
    "from typing import NamedTuple\n",
    "from kfp.v2 import compiler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_preprocess(\n",
    "    bucket_name: str,\n",
    "    input_file: str,\n",
    "    target_column: str,\n",
    "    ) -> NamedTuple('PreprocessOutput', \n",
    "              [\n",
    "                  ('x_train_name', str),\n",
    "                  ('x_test_name', str),\n",
    "                  ('y_train_name', str),\n",
    "                  ('y_test_name', str),\n",
    "                  ('n_classes', int),\n",
    "              ]):\n",
    "    \n",
    "    from collections import namedtuple\n",
    "    from sklearn.model_selection import train_test_split\n",
    "    import pandas as pd\n",
    "    import os\n",
    "    import logging\n",
    "\n",
    "    logging.info(\"Loading {}\".format(input_file))\n",
    "    dataset = pd.read_csv(input_file)\n",
    "    # drop unique id column which is not useful for ML\n",
    "    dataset.drop(['LOAN_SEQUENCE_NUMBER'], axis=1, inplace=True)\n",
    "\n",
    "    # Convert categorical columns into one-hot encodings\n",
    "    str_cols = [col for col in dataset.columns if dataset[col].dtype == 'object']\n",
    "    dataset = pd.get_dummies(dataset, columns=str_cols)\n",
    "    n_classes = dataset[target_column].nunique()\n",
    "    logging.info(\"No. of Classes: {}\".format(n_classes))\n",
    "\n",
    "    # Split with a small test size so as to allow our model to train on more data\n",
    "    x_train, x_test, y_train, y_test = train_test_split(\n",
    "        dataset.drop(target_column, axis=1), \n",
    "        dataset[target_column], \n",
    "        test_size=0.1,\n",
    "        random_state=1,\n",
    "        shuffle=True, \n",
    "        stratify=dataset[target_column], \n",
    "        )\n",
    "\n",
    "    logging.info(\"x_train shape = {}\".format(x_train.shape))\n",
    "    logging.info(\"x_test shape = {}\".format(x_test.shape))\n",
    "    logging.info(\"y_train shape = {}\".format(y_train.shape))\n",
    "    logging.info(\"y_test shape = {}\".format(y_test.shape))\n",
    "\n",
    "    base_file_name = os.path.basename(input_file)\n",
    "    base_name, ext_name = os.path.splitext(base_file_name)\n",
    "    x_train_name = \"{}_x_train{}\".format(base_name, ext_name)\n",
    "    x_test_name = \"{}_x_test{}\".format(base_name, ext_name)\n",
    "    y_train_name = \"{}_y_train{}\".format(base_name, ext_name)\n",
    "    y_test_name = \"{}_y_test{}\".format(base_name, ext_name)\n",
    "    \n",
    "    x_train_name = os.path.join(\"gs://\", bucket_name, \"data_split_xgb\", x_train_name)\n",
    "    x_test_name = os.path.join(\"gs://\", bucket_name, \"data_split_xgb\", x_test_name)\n",
    "    y_train_name = os.path.join(\"gs://\", bucket_name, \"data_split_xgb\", y_train_name)\n",
    "    y_test_name = os.path.join(\"gs://\", bucket_name, \"data_split_xgb\", y_test_name)\n",
    "    \n",
    "    x_train.to_csv(x_train_name, index=False)\n",
    "    x_test.to_csv(x_test_name, index=False)\n",
    "    y_train.to_csv(y_train_name, index=False)\n",
    "    y_test.to_csv(y_test_name, index=False)\n",
    "\n",
    "    logging.info(\"x_train saved to {}\".format(x_train_name))\n",
    "    logging.info(\"x_test saved to {}\".format(x_test_name))\n",
    "    logging.info(\"y_train saved to {}\".format(y_train_name))\n",
    "    logging.info(\"y_test saved to {}\".format(y_test_name))\n",
    "    logging.info(\"finished\")\n",
    "    \n",
    "    PreprocessOutput = namedtuple('PreprocessOutput', \n",
    "        ['x_train_name', 'x_test_name', 'y_train_name', 'y_test_name', 'n_classes'])\n",
    "    return PreprocessOutput(\n",
    "        x_train_name=x_train_name,\n",
    "        x_test_name=x_test_name,\n",
    "        y_train_name=y_train_name,\n",
    "        y_test_name=y_test_name,\n",
    "        n_classes=n_classes,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def hypertune(\n",
    "        project_id: str,\n",
    "        region: str,\n",
    "        job_name: str,\n",
    "        bucket_name: str,\n",
    "        job_folder_name: str,\n",
    "        train_feature_path: str,\n",
    "        train_label_path: str,\n",
    "        val_feature_path: str,\n",
    "        val_label_path: str,\n",
    "        n_classes: int,\n",
    "        metric_id: str,\n",
    "        max_trial_count: int,\n",
    "        parallel_trial_count: int,\n",
    "        package_uri: str,\n",
    "        executor_image_uri: str = 'us-docker.pkg.dev/vertex-ai/training/tf-cpu.2-2:latest',\n",
    "        python_module: str = \"trainer.train\",\n",
    "        api_endpoint: str = \"us-central1-aiplatform.googleapis.com\",\n",
    "        machine_type: str = \"n1-standard-4\",\n",
    "    ) -> NamedTuple('TrainOutput', \n",
    "              [('response', str), ('job_name', str)]):\n",
    "    from collections import namedtuple\n",
    "    from google.cloud import aiplatform\n",
    "    import subprocess\n",
    "    import logging\n",
    "\n",
    "    job_name = job_name + \"_hpt\"\n",
    "    job_dir = 'gs://{}/{}/{}'.format(\n",
    "        bucket_name,\n",
    "        job_folder_name,\n",
    "        job_name,\n",
    "        )\n",
    "\n",
    "    # The AI Platform services require regional API endpoints.\n",
    "    client_options = {\"api_endpoint\": api_endpoint}\n",
    "    # Initialize client that will be used to create and send requests.\n",
    "    # This client only needs to be created once, and can be reused for multiple requests.\n",
    "    client = aiplatform.gapic.JobServiceClient(client_options=client_options)\n",
    "\n",
    "    # study_spec\n",
    "    metric = {\n",
    "        \"metric_id\": metric_id,\n",
    "        \"goal\": aiplatform.gapic.StudySpec.MetricSpec.GoalType.MAXIMIZE,\n",
    "    }\n",
    "\n",
    "    max_depth = {\n",
    "            \"parameter_id\": \"max_depth\",\n",
    "            \"integer_value_spec\": {\"min_value\": 2, \"max_value\": 20},\n",
    "            \"scale_type\": aiplatform.gapic.StudySpec.ParameterSpec.ScaleType.UNIT_LINEAR_SCALE,\n",
    "    }\n",
    "    n_estimators = {\n",
    "            \"parameter_id\": \"n_estimators\",\n",
    "            \"integer_value_spec\": {\"min_value\": 10, \"max_value\": 200},\n",
    "            \"scale_type\": aiplatform.gapic.StudySpec.ParameterSpec.ScaleType.UNIT_LINEAR_SCALE,\n",
    "    }\n",
    "    booster = {\n",
    "        \"parameter_id\": \"booster\",\n",
    "        \"categorical_value_spec\": {\"values\": [\"gbtree\",\"gblinear\",\"dart\"]},\n",
    "    }\n",
    "\n",
    "    # trial_job_spec\n",
    "    machine_spec = {\n",
    "        \"machine_type\": machine_type,\n",
    "    }\n",
    "    worker_pool_spec = {\n",
    "        \"machine_spec\": machine_spec,\n",
    "        \"replica_count\": 1,\n",
    "        \"python_package_spec\": {\n",
    "            \"executor_image_uri\": executor_image_uri,\n",
    "            \"package_uris\": [package_uri],\n",
    "            \"python_module\": python_module,\n",
    "            \"args\": [\n",
    "                '--job-dir',\n",
    "                job_dir,\n",
    "                '--train_feature_name',\n",
    "                train_feature_path,\n",
    "                '--train_label_name',\n",
    "                train_label_path,\n",
    "                '--val_feature_name',\n",
    "                val_feature_path,\n",
    "                '--val_label_name',\n",
    "                val_label_path,\n",
    "                '--no_classes',\n",
    "                n_classes,\n",
    "            ],\n",
    "        },\n",
    "    }\n",
    "\n",
    "    # hyperparameter_tuning_job\n",
    "    hyperparameter_tuning_job = {\n",
    "        \"display_name\": job_name,\n",
    "        \"max_trial_count\": max_trial_count,\n",
    "        \"parallel_trial_count\": parallel_trial_count,\n",
    "        \"study_spec\": {\n",
    "            \"metrics\": [metric],\n",
    "            \"parameters\": [max_depth, n_estimators, booster],\n",
    "        },\n",
    "        \"trial_job_spec\": {\"worker_pool_specs\": [worker_pool_spec]},\n",
    "    }\n",
    "    parent = f\"projects/{project_id}/locations/{region}\"\n",
    "    response = client.create_hyperparameter_tuning_job(\n",
    "        parent=parent, hyperparameter_tuning_job=hyperparameter_tuning_job\n",
    "    )\n",
    "    logging.info(f\"response: {response}\")\n",
    "    hpt_job_name = response.name.split('/')[-1]\n",
    "        \n",
    "    TrainOutput = namedtuple('TrainOutput',['response', 'job_name'])\n",
    "    return TrainOutput(response=response, job_name=hpt_job_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_hpt_job_status(\n",
    "        project_id: str,\n",
    "        region: str,\n",
    "        hpt_job_name: str,\n",
    "        api_endpoint: str = \"us-central1-aiplatform.googleapis.com\",\n",
    "        time_out: int = 9000, # timeout after 2.5 hours by default\n",
    "        time_sleep: int = 60, # check status every minute by default\n",
    "    ) -> NamedTuple('Ghp_Output', \n",
    "              [('booster', str), ('max_depth', int), ('n_estimators', int)]):\n",
    "    \n",
    "    from collections import namedtuple\n",
    "    from google.cloud import aiplatform\n",
    "\n",
    "    import time\n",
    "    import logging\n",
    "\n",
    "    time0 = time.time()\n",
    "    status = False\n",
    "    \n",
    "    while time.time() - time0 < time_out:    \n",
    "        client_options = {\"api_endpoint\": api_endpoint}\n",
    "        client = aiplatform.gapic.JobServiceClient(client_options=client_options)\n",
    "        name = client.hyperparameter_tuning_job_path(\n",
    "            project=project_id,\n",
    "            location=region,\n",
    "            hyperparameter_tuning_job=hpt_job_name,\n",
    "        )\n",
    "        response = client.get_hyperparameter_tuning_job(name=name)\n",
    "        logging.info(f\"response: {response}\")\n",
    "        \n",
    "        if 'state' in response and \"JobState.JOB_STATE_SUCCEEDED\" == str(response.state):\n",
    "            status = True\n",
    "            break\n",
    "        else:\n",
    "            logging.info(\"Checking status ...\")\n",
    "            logging.info(response)\n",
    "            time.sleep(time_sleep)\n",
    "            \n",
    "    if not status:\n",
    "        raise TimeoutError(\"No successful job found. Timeout after {} seconds\".format(time_out))\n",
    "\n",
    "    max_ind = 0\n",
    "    max_val = 0\n",
    "    for ind, trials in enumerate(response.trials):\n",
    "        value = trials.final_measurement.metrics[0].value\n",
    "        logging.info(f\"Metrics Value (larger is better): {value}\")\n",
    "        if value > max_val:\n",
    "            max_val = value\n",
    "            max_ind = ind\n",
    "\n",
    "    param_dict = {}\n",
    "    for params in response.trials[max_ind].parameters:\n",
    "        param_dict[params.parameter_id] = params.value\n",
    "\n",
    "    booster=param_dict['booster']\n",
    "    max_depth=str(int(param_dict['max_depth']))\n",
    "    n_estimators=str(int(param_dict['n_estimators']))\n",
    "\n",
    "    logging.info(f\"booster {booster}\")\n",
    "    logging.info(f\"max_depth {max_depth}\")\n",
    "    logging.info(f\"n_estimators {n_estimators}\")\n",
    "        \n",
    "    Ghp_Output = namedtuple('Ghp_Output',['booster', 'max_depth', 'n_estimators'])\n",
    "    return Ghp_Output(booster=booster, max_depth=max_depth, n_estimators=n_estimators )        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(\n",
    "        project_id: str,\n",
    "        region: str,\n",
    "        job_name: str,\n",
    "        bucket_name: str,\n",
    "        job_folder_name: str,\n",
    "        train_feature_path: str,\n",
    "        train_label_path: str,\n",
    "        n_classes: int,\n",
    "        n_estimators: int,\n",
    "        max_depth: int,\n",
    "        booster: str,\n",
    "        package_uri: str,\n",
    "        executor_image_uri: str = 'us-docker.pkg.dev/vertex-ai/training/tf-cpu.2-2:latest',\n",
    "        python_module: str = \"trainer.train\",\n",
    "        api_endpoint: str = \"us-central1-aiplatform.googleapis.com\",\n",
    "        machine_type: str = \"n1-standard-4\",\n",
    "    ) -> NamedTuple('TrainOutput', \n",
    "              [('response', str), ('job_name', str)]):\n",
    "    \n",
    "    from collections import namedtuple\n",
    "    from google.cloud import aiplatform\n",
    "    import logging\n",
    "\n",
    "    job_dir = 'gs://{}/{}/{}'.format(\n",
    "        bucket_name,\n",
    "        job_folder_name,\n",
    "        job_name,\n",
    "        )\n",
    "    \n",
    "    # The AI Platform services require regional API endpoints.\n",
    "    client_options = {\"api_endpoint\": api_endpoint}\n",
    "    # Initialize client that will be used to create and send requests.\n",
    "    # This client only needs to be created once, and can be reused for multiple requests.\n",
    "    client = aiplatform.gapic.JobServiceClient(client_options=client_options)\n",
    "    custom_job = {\n",
    "        \"display_name\": job_name,\n",
    "        \"job_spec\": {\n",
    "            \"worker_pool_specs\": [\n",
    "                {\n",
    "                    \"machine_spec\": {\n",
    "                        \"machine_type\": machine_type,\n",
    "                    },\n",
    "                    \"replica_count\": 1,\n",
    "                    \"python_package_spec\": {\n",
    "                        \"executor_image_uri\": executor_image_uri,\n",
    "                        \"package_uris\": [package_uri],\n",
    "                        \"python_module\": python_module,\n",
    "                        \"args\": [\n",
    "                          '--job-dir',\n",
    "                          job_dir,\n",
    "                          '--train_feature_name',\n",
    "                          train_feature_path,\n",
    "                          '--train_label_name',\n",
    "                          train_label_path,\n",
    "                          '--no_classes',\n",
    "                          n_classes,\n",
    "                          '--n_estimators',\n",
    "                          n_estimators,\n",
    "                          '--max_depth',\n",
    "                          max_depth,\n",
    "                          '--booster',\n",
    "                          booster\n",
    "                        ],\n",
    "                    },\n",
    "                }\n",
    "            ]\n",
    "        },\n",
    "    }\n",
    "    parent = f\"projects/{project_id}/locations/{regino}\"\n",
    "    response = client.create_custom_job(parent=parent, custom_job=custom_job)\n",
    "    logging.info(f\"response: {response}\")\n",
    "    training_job_id = response.name.split('/')[-1]\n",
    "    \n",
    "    TrainOutput = namedtuple('TrainOutput',['response', 'job_name'])\n",
    "    return TrainOutput(response=response, job_name=training_job_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_job_status(\n",
    "        project_id: str,\n",
    "        region: str,\n",
    "        job_name: str,\n",
    "        api_endpoint: str = \"us-central1-aiplatform.googleapis.com\",\n",
    "        time_out: int = 9000, # timeout after 2.5 hours by default\n",
    "        time_sleep: int = 60, # check status every minute by default\n",
    "    ) -> NamedTuple('Gct_Output', \n",
    "              [('response', str), ('status', bool)]):\n",
    "    \n",
    "    from collections import namedtuple\n",
    "    from google.cloud import aiplatform\n",
    "\n",
    "    import time\n",
    "    import logging\n",
    "\n",
    "    time0 = time.time()\n",
    "    status = False\n",
    "    \n",
    "    while time.time() - time0 < time_out:    \n",
    "        client_options = {\"api_endpoint\": api_endpoint}\n",
    "        client = aiplatform.gapic.JobServiceClient(client_options=client_options)\n",
    "        name = client.custom_job_path(\n",
    "            project=project_id,\n",
    "            location=region,\n",
    "            custom_job=job_name,\n",
    "        )\n",
    "        response = client.get_custom_job(name=name)\n",
    "        logging.info(f\"response: {response}\")\n",
    "        \n",
    "        if 'state' in response and \"JobState.JOB_STATE_SUCCEEDED\" == str(response.state):\n",
    "            status = True\n",
    "            break\n",
    "        else:\n",
    "            logging.info(\"Checking status ...\")\n",
    "            logging.info(response)\n",
    "            time.sleep(time_sleep)\n",
    "            \n",
    "    if not status:\n",
    "        raise TimeoutError(\"No successful job found. Timeout after {} seconds\".format(time_out))\n",
    "       \n",
    "    Gct_Output = namedtuple('Gct_Output',['response', 'status'])\n",
    "    return Gct_Output(response=response, status=status)        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def import_model(\n",
    "    bucket_name: str,\n",
    "    job_folder_name: str,\n",
    "    job_name: str,\n",
    "    model_display_name: str,\n",
    "    serving_container_image_uri: str = 'us-docker.pkg.dev/vertex-ai/prediction/xgboost-cpu.1-3:latest',\n",
    "    ) -> NamedTuple('ImportModelOutput', \n",
    "              [('model_id', str)]): \n",
    "    \n",
    "    from google.cloud import aiplatform\n",
    "    from collections import namedtuple\n",
    "    import logging\n",
    "       \n",
    "    latest_model_dir = \"gs://{}/{}/{}\".format(bucket_name, job_folder_name, job_name)\n",
    "    \n",
    "    response = aiplatform.Model.upload(\n",
    "        display_name = model_display_name,\n",
    "        serving_container_image_uri = serving_container_image_uri,\n",
    "        artifact_uri = latest_model_dir\n",
    "    )\n",
    "    model_id = response.name.split('/')[-1]\n",
    "    \n",
    "    ImportModelOutput = namedtuple('ImportModelOutput',['model_id'])\n",
    "    return ImportModelOutput(model_id=model_id)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compile python functions to components"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "component_dir = \"./components\"\n",
    "base_image = \"us-docker.pkg.dev/vertex-ai/training/tf-cpu.2-2:latest\"\n",
    "\n",
    "yaml_name = '{}/preprocess.yaml'.format(component_dir)\n",
    "preprocess_op = comp.func_to_container_op(\n",
    "    data_preprocess, \n",
    "    output_component_file=yaml_name,\n",
    "    base_image=base_image)\n",
    "\n",
    "yaml_name = '{}/train_hpt.yaml'.format(component_dir)\n",
    "hypertune_op = comp.func_to_container_op(\n",
    "    hypertune, \n",
    "    output_component_file=yaml_name,\n",
    "    base_image=base_image)\n",
    "\n",
    "yaml_name = '{}/ghp.yaml'.format(component_dir) # Get hypertune\n",
    "ghp_op = comp.func_to_container_op(\n",
    "    get_hpt_job_status, \n",
    "    output_component_file=yaml_name,\n",
    "    base_image=base_image)\n",
    "\n",
    "yaml_name = '{}/train.yaml'.format(component_dir)\n",
    "train_op = comp.func_to_container_op(\n",
    "    train, \n",
    "    output_component_file=yaml_name,\n",
    "    base_image=base_image)\n",
    "\n",
    "yaml_name = '{}/gct.yaml'.format(component_dir) # Get custom train\n",
    "gct_op = comp.func_to_container_op(\n",
    "    get_job_status, \n",
    "    output_component_file=yaml_name,\n",
    "    base_image=base_image)\n",
    "\n",
    "yaml_name = '{}/import_model.yaml'.format(component_dir)\n",
    "import_model_op = comp.func_to_container_op(\n",
    "    import_model, \n",
    "    output_component_file=yaml_name,\n",
    "    base_image=base_image)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compile KFP pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "@dsl.pipeline(\n",
    "   name='vertex-training-pipeline',\n",
    "   description='A example of vertex training pipeline with custom xgboost model.'\n",
    ")\n",
    "def train_pipeline(\n",
    "    job_name: str,\n",
    "    project_id: str,\n",
    "    region: str,\n",
    "    user_name: str,\n",
    "    bucket_name: str,\n",
    "    input_file: str,\n",
    "    job_folder_name: str,\n",
    "    target_column: str,\n",
    "    package_uri: str,\n",
    "    metric_id: str,\n",
    "    max_trial_count: int,\n",
    "    parallel_trial_count: int,\n",
    "    model_display_name: str):\n",
    "    \n",
    "    preprocess_task = preprocess_op(\n",
    "        bucket_name = bucket_name,\n",
    "        input_file = input_file,\n",
    "        target_column = target_column,\n",
    "    )\n",
    "    \n",
    "    hpt_task = hypertune_op(\n",
    "        project_id = project_id,\n",
    "        region = region,\n",
    "        job_name = job_name,\n",
    "        bucket_name = bucket_name,\n",
    "        job_folder_name = job_folder_name,\n",
    "        train_feature_path = preprocess_task.outputs['x_train_name'],\n",
    "        train_label_path   = preprocess_task.outputs['y_train_name'],\n",
    "        val_feature_path   = preprocess_task.outputs['x_test_name'],\n",
    "        val_label_path     = preprocess_task.outputs['y_test_name'],\n",
    "        n_classes = preprocess_task.outputs['n_classes'],\n",
    "        metric_id = metric_id,\n",
    "        max_trial_count = max_trial_count,\n",
    "        parallel_trial_count = parallel_trial_count,\n",
    "        package_uri = package_uri,\n",
    "    )\n",
    "    \n",
    "    ghp_task = ghp_op(\n",
    "        project_id = project_id,\n",
    "        region = region,\n",
    "        hpt_job_name = hpt_task.outputs['job_name'],\n",
    "    )\n",
    "    \n",
    "    train_task = train_op(\n",
    "        project_id = project_id,\n",
    "        region = region, \n",
    "        job_name = job_name,\n",
    "        bucket_name = bucket_name,\n",
    "        job_folder_name = job_folder_name,\n",
    "        train_feature_path = preprocess_task.outputs['x_train_name'],\n",
    "        train_label_path   = preprocess_task.outputs['y_train_name'],\n",
    "        n_classes = preprocess_task.outputs['n_classes'],\n",
    "        n_estimators = ghp_task.outputs['n_estimators'],\n",
    "        max_depth = ghp_task.outputs['max_depth'],\n",
    "        booster = ghp_task.outputs['booster'],\n",
    "        package_uri = package_uri,\n",
    "    )\n",
    "    \n",
    "    gct_task = gct_op(\n",
    "        project_id = project_id,\n",
    "        region = region,\n",
    "        job_name = train_task.outputs['job_name']\n",
    "    )\n",
    "    \n",
    "#     deploy_task = deploy_op(\n",
    "#         status = lro_task_2.outputs['status'],\n",
    "#         bucket_name = bucket_name,\n",
    "#         job_folder_name = job_folder_name,\n",
    "#         job_name = train_task.outputs['job_name'],\n",
    "#         region = 'global',\n",
    "#         model_framework = 'XGBOOST',\n",
    "#         model_name = deployed_model_name,\n",
    "#         model_version = deployed_model_version,\n",
    "#         model_description = deployed_model_description,\n",
    "#     )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline_pkg_path=\"./train_pipeline.json\"\n",
    "pipeline_root = \"gs://tuti_job/pipeline_root\"\n",
    "compiler.Compiler().compile(\n",
    "    pipeline_func=train_pipeline, \n",
    "    package_path=pipeline_pkg_path,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run KFP pipeline on AI Platform hosted Kubernetes cluster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "See the Pipeline job <a href=\"https://console.cloud.google.com/vertex-ai/locations/us-central1/pipelines/runs/vertex-training-pipeline-20210615053129?project=img-seg-3d\" target=\"_blank\" >here</a>."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "## ============== Uncomment to run the pipeline ==============\n",
    "from kfp.v2.google.client import AIPlatformClient\n",
    "from datetime import datetime\n",
    "from pytz import timezone\n",
    "\n",
    "PROJECT = 'img-seg-3d'\n",
    "REGION = 'us-central1'\n",
    "my_timezone = 'US/Pacific'\n",
    "        \n",
    "# Define pipeline input\n",
    "pipeline_params = {\n",
    "    \"job_name\": 'xgb_train_elvinzhu_{}'.format(\n",
    "        datetime.now(timezone(my_timezone)).strftime(\"%m%d%y_%H%M\")\n",
    "        ),\n",
    "    \"project_id\": PROJECT,\n",
    "    \"region\": REGION,\n",
    "    \"user_name\": 'elvinzhu',\n",
    "    \"bucket_name\": 'tuti_job',\n",
    "    \"job_folder_name\": 'xgb_train_job',\n",
    "    \"input_file\": 'gs://tuti_asset/datasets/mortgage_structured.csv',\n",
    "    \"target_column\": 'TARGET',\n",
    "    \"package_uri\": \"gs://vapit_job/trainer/xgboost/trainer-0.1.tar.gz\",\n",
    "    \"max_trial_count\": 4,\n",
    "    \"parallel_trial_count\": 1,\n",
    "    \"metric_id\": \"roc_auc\",\n",
    "}\n",
    "\n",
    "# Create pipeline client\n",
    "api_client = AIPlatformClient(\n",
    "    project_id=PROJECT,\n",
    "    region=REGION,\n",
    ")\n",
    "response = api_client.create_run_from_job_spec(\n",
    "    job_spec_path=pipeline_pkg_path,\n",
    "    pipeline_root=pipeline_root,\n",
    "    parameter_values=pipeline_params\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "environment": {
   "name": "tf2-gpu.2-1.m61",
   "type": "gcloud",
   "uri": "gcr.io/deeplearning-platform-release/tf2-gpu.2-1:m61"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
