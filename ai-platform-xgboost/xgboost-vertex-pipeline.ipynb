{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ============================================================================== \\\n",
    " Copyright 2020 Google LLC. This software is provided as-is, without warranty \\\n",
    " or representation for any use or purpose. Your use of it is subject to your \\\n",
    " agreement with Google. \\\n",
    " ============================================================================== \n",
    " \n",
    " Author: Elvin Zhu, Chanchal Chatterjee \\\n",
    " Email: elvinzhu@google.com \\\n",
    "<img src=\"img/google-cloud-icon.jpg\" alt=\"Drawing\" style=\"width: 200px;\"/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !python3 -m pip install kfp==1.6.2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compile your own docker image for component base image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !cd /home/jupyter/vapit/docker\n",
    "# !sh build_image.sh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !cd /home/jupyter/vapit/ai-platform-xgboost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import kfp\n",
    "import kfp.components as comp\n",
    "import kfp.dsl as dsl\n",
    "from typing import NamedTuple"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_preprocess(\n",
    "    bucket_name: str,\n",
    "    input_file: str,\n",
    "    target_column: str,\n",
    "    ) -> NamedTuple('PreprocessOutput', \n",
    "              [\n",
    "                  ('x_train_name', str),\n",
    "                  ('x_test_name', str),\n",
    "                  ('y_train_name', str),\n",
    "                  ('y_test_name', str),\n",
    "                  ('n_classes', str),\n",
    "              ]):\n",
    "    \n",
    "    from collections import namedtuple\n",
    "    from sklearn.model_selection import train_test_split\n",
    "    import pandas as pd\n",
    "    import os\n",
    "    import logging\n",
    "\n",
    "    logging.info(\"Loading {}\".format(input_file))\n",
    "    dataset = pd.read_csv(input_file)\n",
    "    # drop unique id column which is not useful for ML\n",
    "    dataset.drop(['LOAN_SEQUENCE_NUMBER'], axis=1, inplace=True)\n",
    "\n",
    "    # Convert categorical columns into one-hot encodings\n",
    "    str_cols = [col for col in dataset.columns if dataset[col].dtype == 'object']\n",
    "    dataset = pd.get_dummies(dataset, columns=str_cols)\n",
    "    n_classes = dataset[target_column].nunique()\n",
    "    logging.info(\"No. of Classes: {}\".format(n_classes))\n",
    "\n",
    "    # Split with a small test size so as to allow our model to train on more data\n",
    "    x_train, x_test, y_train, y_test = train_test_split(\n",
    "        dataset.drop(target_column, axis=1), \n",
    "        dataset[target_column], \n",
    "        test_size=0.1,\n",
    "        random_state=1,\n",
    "        shuffle=True, \n",
    "        stratify=dataset[target_column], \n",
    "        )\n",
    "\n",
    "    logging.info(\"x_train shape = {}\".format(x_train.shape))\n",
    "    logging.info(\"x_test shape = {}\".format(x_test.shape))\n",
    "    logging.info(\"y_train shape = {}\".format(y_train.shape))\n",
    "    logging.info(\"y_test shape = {}\".format(y_test.shape))\n",
    "\n",
    "    base_file_name = os.path.basename(input_file)\n",
    "    base_name, ext_name = os.path.splitext(base_file_name)\n",
    "    x_train_name = \"{}_x_train{}\".format(base_name, ext_name)\n",
    "    x_test_name = \"{}_x_test{}\".format(base_name, ext_name)\n",
    "    y_train_name = \"{}_y_train{}\".format(base_name, ext_name)\n",
    "    y_test_name = \"{}_y_test{}\".format(base_name, ext_name)\n",
    "    \n",
    "    x_train_name = os.path.join(\"gs://\", bucket_name, \"data_split_xgb\", x_train_name)\n",
    "    x_test_name = os.path.join(\"gs://\", bucket_name, \"data_split_xgb\", x_test_name)\n",
    "    y_train_name = os.path.join(\"gs://\", bucket_name, \"data_split_xgb\", y_train_name)\n",
    "    y_test_name = os.path.join(\"gs://\", bucket_name, \"data_split_xgb\", y_test_name)\n",
    "    \n",
    "    x_train.to_csv(x_train_name, index=False)\n",
    "    x_test.to_csv(x_test_name, index=False)\n",
    "    y_train.to_csv(y_train_name, index=False)\n",
    "    y_test.to_csv(y_test_name, index=False)\n",
    "\n",
    "    logging.info(\"x_train saved to {}\".format(x_train_name))\n",
    "    logging.info(\"x_test saved to {}\".format(x_test_name))\n",
    "    logging.info(\"y_train saved to {}\".format(y_train_name))\n",
    "    logging.info(\"y_test saved to {}\".format(y_test_name))\n",
    "    logging.info(\"finished\")\n",
    "    \n",
    "    PreprocessOutput = namedtuple('PreprocessOutput', \n",
    "        ['x_train_name', 'x_test_name', 'y_train_name', 'y_test_name', 'n_classes'])\n",
    "    return PreprocessOutput(\n",
    "        x_train_name=x_train_name,\n",
    "        x_test_name=x_test_name,\n",
    "        y_train_name=y_train_name,\n",
    "        y_test_name=y_test_name,\n",
    "        n_classes=str(n_classes),\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def hypertune(\n",
    "        project_id: str,\n",
    "        region: str,\n",
    "        job_name: str,\n",
    "        bucket_name: str,\n",
    "        job_folder_name: str,\n",
    "        train_feature_path: str,\n",
    "        train_label_path: str,\n",
    "        val_feature_path: str,\n",
    "        val_label_path: str,\n",
    "        n_classes: str,\n",
    "        metric_id: str,\n",
    "        max_trial_count: int,\n",
    "        parallel_trial_count: int,\n",
    "        package_uri: str,\n",
    "        executor_image_uri: str = 'us-docker.pkg.dev/vertex-ai/training/tf-cpu.2-2:latest',\n",
    "        python_module: str = \"trainer.train_hpt\",\n",
    "        api_endpoint: str = \"us-central1-aiplatform.googleapis.com\",\n",
    "        machine_type: str = \"n1-standard-4\",\n",
    "    ) -> NamedTuple('TrainOutput', \n",
    "              [('job_name', str)]):\n",
    "    from collections import namedtuple\n",
    "    from google.cloud import aiplatform\n",
    "    import subprocess\n",
    "    import logging\n",
    "\n",
    "    job_name = job_name + \"_hpt\"\n",
    "    job_dir = 'gs://{}/{}/{}'.format(\n",
    "        bucket_name,\n",
    "        job_folder_name,\n",
    "        job_name,\n",
    "        )\n",
    "\n",
    "    # The AI Platform services require regional API endpoints.\n",
    "    client_options = {\"api_endpoint\": api_endpoint}\n",
    "    # Initialize client that will be used to create and send requests.\n",
    "    # This client only needs to be created once, and can be reused for multiple requests.\n",
    "    client = aiplatform.gapic.JobServiceClient(client_options=client_options)\n",
    "    print(client)\n",
    "    \n",
    "    # study_spec\n",
    "    metric = {\n",
    "        \"metric_id\": metric_id,\n",
    "        \"goal\": aiplatform.gapic.StudySpec.MetricSpec.GoalType.MAXIMIZE,\n",
    "    }\n",
    "    print(metric)\n",
    "\n",
    "    max_depth = {\n",
    "            \"parameter_id\": \"max_depth\",\n",
    "            \"integer_value_spec\": {\"min_value\": 2, \"max_value\": 20},\n",
    "            \"scale_type\": aiplatform.gapic.StudySpec.ParameterSpec.ScaleType.UNIT_LINEAR_SCALE,\n",
    "    }\n",
    "    n_estimators = {\n",
    "            \"parameter_id\": \"n_estimators\",\n",
    "            \"integer_value_spec\": {\"min_value\": 10, \"max_value\": 200},\n",
    "            \"scale_type\": aiplatform.gapic.StudySpec.ParameterSpec.ScaleType.UNIT_LINEAR_SCALE,\n",
    "    }\n",
    "    booster = {\n",
    "        \"parameter_id\": \"booster\",\n",
    "        \"categorical_value_spec\": {\"values\": [\"gbtree\",\"gblinear\",\"dart\"]},\n",
    "    }\n",
    "\n",
    "    # trial_job_spec\n",
    "    machine_spec = {\n",
    "        \"machine_type\": machine_type,\n",
    "    }\n",
    "    worker_pool_spec = {\n",
    "        \"machine_spec\": machine_spec,\n",
    "        \"replica_count\": 1,\n",
    "        \"python_package_spec\": {\n",
    "            \"executor_image_uri\": executor_image_uri,\n",
    "            \"package_uris\": [package_uri],\n",
    "            \"python_module\": python_module,\n",
    "            \"args\": [\n",
    "                '--job-dir',\n",
    "                job_dir,\n",
    "                '--train_feature_name',\n",
    "                train_feature_path,\n",
    "                '--train_label_name',\n",
    "                train_label_path,\n",
    "                '--val_feature_name',\n",
    "                val_feature_path,\n",
    "                '--val_label_name',\n",
    "                val_label_path,\n",
    "                '--no_classes',\n",
    "                str(n_classes),\n",
    "            ],\n",
    "        },\n",
    "    }\n",
    "\n",
    "    # hyperparameter_tuning_job\n",
    "    hyperparameter_tuning_job = {\n",
    "        \"display_name\": job_name,\n",
    "        \"max_trial_count\": max_trial_count,\n",
    "        \"parallel_trial_count\": parallel_trial_count,\n",
    "        \"study_spec\": {\n",
    "            \"metrics\": [metric],\n",
    "            \"parameters\": [max_depth, n_estimators, booster],\n",
    "        },\n",
    "        \"trial_job_spec\": {\"worker_pool_specs\": [worker_pool_spec]},\n",
    "    }\n",
    "    print(hyperparameter_tuning_job)\n",
    "    \n",
    "    parent = f\"projects/{project_id}/locations/{region}\"\n",
    "    response = client.create_hyperparameter_tuning_job(\n",
    "        parent=parent, hyperparameter_tuning_job=hyperparameter_tuning_job\n",
    "    )\n",
    "    print(response)\n",
    "    logging.info(f\"response: {response}\")\n",
    "    hpt_job_name = response.name.split('/')[-1]\n",
    "        \n",
    "    TrainOutput = namedtuple('TrainOutput',['job_name'])\n",
    "    return TrainOutput(job_name=hpt_job_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_hpt_job_status(\n",
    "        project_id: str,\n",
    "        region: str,\n",
    "        hpt_job_name: str,\n",
    "        api_endpoint: str = \"us-central1-aiplatform.googleapis.com\",\n",
    "        time_out: int = 9000, # timeout after 2.5 hours by default\n",
    "        time_sleep: int = 60, # check status every minute by default\n",
    "    ) -> NamedTuple('Ghp_Output', \n",
    "              [('booster', str), ('max_depth', str), ('n_estimators', str)]):\n",
    "    \n",
    "    from collections import namedtuple\n",
    "    from google.cloud import aiplatform\n",
    "\n",
    "    import time\n",
    "    import logging\n",
    "\n",
    "    time0 = time.time()\n",
    "    status = False\n",
    "    \n",
    "    while time.time() - time0 < time_out:    \n",
    "        client_options = {\"api_endpoint\": api_endpoint}\n",
    "        client = aiplatform.gapic.JobServiceClient(client_options=client_options)\n",
    "        name = client.hyperparameter_tuning_job_path(\n",
    "            project=project_id,\n",
    "            location=region,\n",
    "            hyperparameter_tuning_job=hpt_job_name,\n",
    "        )\n",
    "        response = client.get_hyperparameter_tuning_job(name=name)\n",
    "        logging.info(f\"response: {response}\")\n",
    "        \n",
    "        if 'state' in response and \"JobState.JOB_STATE_SUCCEEDED\" == str(response.state):\n",
    "            status = True\n",
    "            break\n",
    "        else:\n",
    "            logging.info(\"Checking status ...\")\n",
    "            logging.info(response)\n",
    "            time.sleep(time_sleep)\n",
    "            \n",
    "    if not status:\n",
    "        raise TimeoutError(\"No successful job found. Timeout after {} seconds\".format(time_out))\n",
    "\n",
    "    max_ind = 0\n",
    "    max_val = 0\n",
    "    for ind, trials in enumerate(response.trials):\n",
    "        value = trials.final_measurement.metrics[0].value\n",
    "        logging.info(f\"Metrics Value (larger is better): {value}\")\n",
    "        if value > max_val:\n",
    "            max_val = value\n",
    "            max_ind = ind\n",
    "\n",
    "    param_dict = {}\n",
    "    for params in response.trials[max_ind].parameters:\n",
    "        param_dict[params.parameter_id] = params.value\n",
    "\n",
    "    booster=param_dict['booster']\n",
    "    max_depth=str(int(param_dict['max_depth']))\n",
    "    n_estimators=str(int(param_dict['n_estimators']))\n",
    "\n",
    "    logging.info(f\"booster {booster}\")\n",
    "    logging.info(f\"max_depth {max_depth}\")\n",
    "    logging.info(f\"n_estimators {n_estimators}\")\n",
    "        \n",
    "    Ghp_Output = namedtuple('Ghp_Output',['booster', 'max_depth', 'n_estimators'])\n",
    "    return Ghp_Output(booster=str(booster), max_depth=str(max_depth), n_estimators=str(n_estimators) )        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(\n",
    "        project_id: str,\n",
    "        region: str,\n",
    "        job_name: str,\n",
    "        bucket_name: str,\n",
    "        job_folder_name: str,\n",
    "        train_feature_path: str,\n",
    "        train_label_path: str,\n",
    "        n_classes: str,\n",
    "        n_estimators: str,\n",
    "        max_depth: str,\n",
    "        booster: str,\n",
    "        package_uri: str,\n",
    "        executor_image_uri: str = 'us-docker.pkg.dev/vertex-ai/training/tf-cpu.2-2:latest',\n",
    "        python_module: str = \"trainer.train\",\n",
    "        api_endpoint: str = \"us-central1-aiplatform.googleapis.com\",\n",
    "        machine_type: str = \"n1-standard-4\",\n",
    "    ) -> NamedTuple('TrainOutput', \n",
    "              [('ct_job_name', str)]):\n",
    "    \n",
    "    from collections import namedtuple\n",
    "    from google.cloud import aiplatform\n",
    "    import logging\n",
    "\n",
    "    job_dir = 'gs://{}/{}/{}'.format(\n",
    "        bucket_name,\n",
    "        job_folder_name,\n",
    "        job_name,\n",
    "        )\n",
    "    \n",
    "    # The AI Platform services require regional API endpoints.\n",
    "    client_options = {\"api_endpoint\": api_endpoint}\n",
    "    # Initialize client that will be used to create and send requests.\n",
    "    # This client only needs to be created once, and can be reused for multiple requests.\n",
    "    client = aiplatform.gapic.JobServiceClient(client_options=client_options)\n",
    "    custom_job = {\n",
    "        \"display_name\": job_name,\n",
    "        \"job_spec\": {\n",
    "            \"worker_pool_specs\": [\n",
    "                {\n",
    "                    \"machine_spec\": {\n",
    "                        \"machine_type\": machine_type,\n",
    "                    },\n",
    "                    \"replica_count\": 1,\n",
    "                    \"python_package_spec\": {\n",
    "                        \"executor_image_uri\": executor_image_uri,\n",
    "                        \"package_uris\": [package_uri],\n",
    "                        \"python_module\": python_module,\n",
    "                        \"args\": [\n",
    "                          '--job-dir',\n",
    "                          job_dir,\n",
    "                          '--train_feature_name',\n",
    "                          train_feature_path,\n",
    "                          '--train_label_name',\n",
    "                          train_label_path,\n",
    "                          '--no_classes',\n",
    "                          str(n_classes),\n",
    "                          '--n_estimators',\n",
    "                          str(n_estimators),\n",
    "                          '--max_depth',\n",
    "                          str(max_depth),\n",
    "                          '--booster',\n",
    "                          str(booster)\n",
    "                        ],\n",
    "                    },\n",
    "                }\n",
    "            ]\n",
    "        },\n",
    "    }\n",
    "    parent = f\"projects/{project_id}/locations/{region}\"\n",
    "    response = client.create_custom_job(parent=parent, custom_job=custom_job)\n",
    "    logging.info(f\"response: {response}\")\n",
    "    training_job_id = response.name.split('/')[-1]\n",
    "    \n",
    "    TrainOutput = namedtuple('TrainOutput',['ct_job_name'])\n",
    "    return TrainOutput(ct_job_name=training_job_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_job_status(\n",
    "        project_id: str,\n",
    "        region: str,\n",
    "        ct_job_name: str,\n",
    "        api_endpoint: str = \"us-central1-aiplatform.googleapis.com\",\n",
    "        time_out: int = 9000, # timeout after 2.5 hours by default\n",
    "        time_sleep: int = 60, # check status every minute by default\n",
    "    ) -> NamedTuple('Gct_Output', \n",
    "              [('status', bool)]):\n",
    "    \n",
    "    from collections import namedtuple\n",
    "    from google.cloud import aiplatform\n",
    "\n",
    "    import time\n",
    "    import logging\n",
    "\n",
    "    time0 = time.time()\n",
    "    status = False\n",
    "    \n",
    "    while time.time() - time0 < time_out:    \n",
    "        client_options = {\"api_endpoint\": api_endpoint}\n",
    "        client = aiplatform.gapic.JobServiceClient(client_options=client_options)\n",
    "        name = client.custom_job_path(\n",
    "            project=project_id,\n",
    "            location=region,\n",
    "            custom_job=ct_job_name,\n",
    "        )\n",
    "        response = client.get_custom_job(name=name)\n",
    "        logging.info(f\"response: {response}\")\n",
    "        \n",
    "        if 'state' in response and \"JobState.JOB_STATE_SUCCEEDED\" == str(response.state):\n",
    "            status = True\n",
    "            break\n",
    "        else:\n",
    "            logging.info(\"Checking status ...\")\n",
    "            logging.info(response)\n",
    "            time.sleep(time_sleep)\n",
    "            \n",
    "    if not status:\n",
    "        raise TimeoutError(\"No successful job found. Timeout after {} seconds\".format(time_out))\n",
    "       \n",
    "    Gct_Output = namedtuple('Gct_Output',['status'])\n",
    "    return Gct_Output(status=status)        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def import_model(\n",
    "    project_id: str,\n",
    "    status: bool,\n",
    "    bucket_name: str,\n",
    "    job_name: str,\n",
    "    job_folder_name: str,\n",
    "    model_display_name: str,\n",
    "    serving_container_image_uri: str = 'us-docker.pkg.dev/vertex-ai/prediction/xgboost-cpu.1-3:latest',\n",
    "    ) -> NamedTuple('ImportModelOutput', \n",
    "              [('model_id', str)]): \n",
    "    \n",
    "    from google.cloud import aiplatform\n",
    "    from collections import namedtuple\n",
    "    import logging\n",
    "       \n",
    "    latest_model_dir = \"gs://{}/{}/{}\".format(bucket_name, job_folder_name, job_name)\n",
    "    \n",
    "    response = aiplatform.Model.upload(\n",
    "        display_name = model_display_name,\n",
    "        serving_container_image_uri = serving_container_image_uri,\n",
    "        artifact_uri = latest_model_dir,\n",
    "        project = project_id,\n",
    "    )\n",
    "    model_id = response.name.split('/')[-1]\n",
    "    \n",
    "    ImportModelOutput = namedtuple('ImportModelOutput',['model_id'])\n",
    "    return ImportModelOutput(model_id=model_id)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compile python functions to components"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "component_dir = \"./components\"\n",
    "base_image = \"gcr.io/img-seg-3d/vertex_base@sha256:2862b83c8b1fd32afddd6be49934f43586ed32c08988a57bb3cef65f5afed750\"\n",
    "\n",
    "yaml_name = '{}/preprocess.yaml'.format(component_dir)\n",
    "preprocess_op = comp.func_to_container_op(\n",
    "    data_preprocess, \n",
    "    output_component_file=yaml_name,\n",
    "    base_image=base_image)\n",
    "\n",
    "yaml_name = '{}/train_hpt.yaml'.format(component_dir)\n",
    "hypertune_op = comp.func_to_container_op(\n",
    "    hypertune, \n",
    "    output_component_file=yaml_name,\n",
    "    base_image=base_image)\n",
    "\n",
    "yaml_name = '{}/ghp.yaml'.format(component_dir) # Get hypertune\n",
    "ghp_op = comp.func_to_container_op(\n",
    "    get_hpt_job_status, \n",
    "    output_component_file=yaml_name,\n",
    "    base_image=base_image)\n",
    "\n",
    "yaml_name = '{}/train.yaml'.format(component_dir)\n",
    "train_op = comp.func_to_container_op(\n",
    "    train, \n",
    "    output_component_file=yaml_name,\n",
    "    base_image=base_image)\n",
    "\n",
    "yaml_name = '{}/gct.yaml'.format(component_dir) # Get custom train\n",
    "gct_op = comp.func_to_container_op(\n",
    "    get_job_status, \n",
    "    output_component_file=yaml_name,\n",
    "    base_image=base_image)\n",
    "\n",
    "yaml_name = '{}/import_model.yaml'.format(component_dir)\n",
    "import_model_op = comp.func_to_container_op(\n",
    "    import_model, \n",
    "    output_component_file=yaml_name,\n",
    "    base_image=base_image)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compile KFP pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "@dsl.pipeline(\n",
    "   name='vertex-training-pipeline',\n",
    "   description='A example of vertex training pipeline with custom xgboost model.'\n",
    ")\n",
    "def train_pipeline(\n",
    "    job_name: str,\n",
    "    project_id: str,\n",
    "    region: str,\n",
    "    user_name: str,\n",
    "    bucket_name: str,\n",
    "    input_file: str,\n",
    "    job_folder_name: str,\n",
    "    target_column: str,\n",
    "    package_uri: str,\n",
    "    metric_id: str,\n",
    "    max_trial_count: int,\n",
    "    parallel_trial_count: int,\n",
    "    model_display_name: str):\n",
    "    \n",
    "    preprocess_task = preprocess_op(\n",
    "        bucket_name = bucket_name,\n",
    "        input_file = input_file,\n",
    "        target_column = target_column,\n",
    "    )\n",
    "    \n",
    "    hpt_task = hypertune_op(\n",
    "        project_id = project_id,\n",
    "        region = region,\n",
    "        job_name = job_name,\n",
    "        bucket_name = bucket_name,\n",
    "        job_folder_name = job_folder_name,\n",
    "        train_feature_path = preprocess_task.outputs['x_train_name'],\n",
    "        train_label_path   = preprocess_task.outputs['y_train_name'],\n",
    "        val_feature_path   = preprocess_task.outputs['x_test_name'],\n",
    "        val_label_path     = preprocess_task.outputs['y_test_name'],\n",
    "        n_classes = preprocess_task.outputs['n_classes'],\n",
    "        metric_id = metric_id,\n",
    "        max_trial_count = max_trial_count,\n",
    "        parallel_trial_count = parallel_trial_count,\n",
    "        package_uri = package_uri,\n",
    "    )\n",
    "    \n",
    "    ghp_task = ghp_op(\n",
    "        project_id = project_id,\n",
    "        region = region,\n",
    "        hpt_job_name = hpt_task.outputs['job_name'],\n",
    "    )\n",
    "    \n",
    "    train_task = train_op(\n",
    "        project_id = project_id,\n",
    "        region = region, \n",
    "        job_name = job_name,\n",
    "        bucket_name = bucket_name,\n",
    "        job_folder_name = job_folder_name,\n",
    "        train_feature_path = preprocess_task.outputs['x_train_name'],\n",
    "        train_label_path   = preprocess_task.outputs['y_train_name'],\n",
    "        n_classes = preprocess_task.outputs['n_classes'],\n",
    "        n_estimators = ghp_task.outputs['n_estimators'],\n",
    "        max_depth = ghp_task.outputs['max_depth'],\n",
    "        booster = ghp_task.outputs['booster'],\n",
    "        package_uri = package_uri,\n",
    "    )\n",
    "    \n",
    "    gct_task = gct_op(\n",
    "        project_id = project_id,\n",
    "        region = region,\n",
    "        ct_job_name = train_task.outputs['ct_job_name']\n",
    "    )\n",
    "    import_model_task = import_model_op(\n",
    "        project_id = project_id,\n",
    "        bucket_name = bucket_name,\n",
    "        job_name = job_name,\n",
    "        job_folder_name = job_folder_name,\n",
    "        model_display_name = model_display_name,\n",
    "        status = gct_task.outputs['status'],\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run KFP pipeline on AI Platform hosted Kubernetes cluster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ## ============== Uncomment to compile and run the pipeline ==============\n",
    "# from kfp.compiler import compiler\n",
    "# from datetime import datetime\n",
    "# from pytz import timezone\n",
    "\n",
    "# PROJECT = 'img-seg-3d' ## <== TODO: Change to your gcp project id\n",
    "# USER_NAME = 'elvinzhu' ## <== TODO: Change to your user name\n",
    "# BUCKET_NAME = 'tuti_job' ## <== TODO: Change to your bucket name\n",
    "# PACKAGE_URI = f\"gs://{BUCKET_NAME}/trainer/xgboost/trainer-0.1.tar.gz\"  ## <== TODO: Change to your package uri \n",
    "# KFP_HOST_NAME = 'https://6ff530db99970db2-dot-us-central2.pipelines.googleusercontent.com' ## <== TODO: Change to your host name\n",
    "\n",
    "# # Compile pipeline \n",
    "# pipeline_pkg_path=\"./train_pipeline.tar.gz\"\n",
    "# pipeline_root = f\"gs://{BUCKET_NAME}/pipeline_root\"\n",
    "\n",
    "# compiler.Compiler().compile(\n",
    "#     pipeline_func=train_pipeline, \n",
    "#     package_path=pipeline_pkg_path,\n",
    "# )\n",
    "\n",
    "# # Run pipeline\n",
    "# JOB_FOLDER_NAME = 'xgb_train_job'\n",
    "# REGION = 'us-central1'\n",
    "# my_timezone = 'US/Pacific'\n",
    "        \n",
    "# # Define pipeline input\n",
    "# pipeline_params = {\n",
    "#     \"job_name\": 'xgb_train_{}_{}'.format(\n",
    "#         USER_NAME, datetime.now(timezone(my_timezone)).strftime(\"%m%d%y_%H%M\")\n",
    "#         ),\n",
    "#     \"project_id\": PROJECT,\n",
    "#     \"region\": REGION,\n",
    "#     \"user_name\": USER_NAME,\n",
    "#     \"bucket_name\": BUCKET_NAME,\n",
    "#     \"job_folder_name\": JOB_FOLDER_NAME,\n",
    "#     \"input_file\": 'gs://tuti_asset/datasets/mortgage_structured.csv',\n",
    "#     \"target_column\": 'TARGET',\n",
    "#     \"package_uri\": PACKAGE_URI,\n",
    "#     \"max_trial_count\": 4,\n",
    "#     \"parallel_trial_count\": 2,\n",
    "#     \"metric_id\": \"roc_auc\",\n",
    "#     \"model_display_name\": \"vertex_pipeline_xgboost_model\"\n",
    "# }\n",
    "\n",
    "# kfp_exp_name = 'xgboost_ai_platform'\n",
    "# kfp_run_name = 'demo_xgboost'\n",
    "\n",
    "# client = kfp.Client(host=KFP_HOST_NAME) \n",
    "# # Create Experiment GROUP\n",
    "# exp = client.create_experiment(name = kfp_exp_name)\n",
    "# # Create Experiment RUN\n",
    "# run = client.run_pipeline(exp.id, kfp_run_name, pipeline_pkg_path, params=pipeline_params)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run KFP pipeline on Vertex AI Pipeline (Fully managed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "See the Pipeline job <a href=\"https://console.cloud.google.com/vertex-ai/locations/us-central1/pipelines/runs/vertex-training-pipeline-20210615203300?project=img-seg-3d\" target=\"_blank\" >here</a>."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "## ============== Uncomment to compile and run the pipeline ==============\n",
    "from kfp.v2 import compiler\n",
    "from kfp.v2.google.client import AIPlatformClient\n",
    "from datetime import datetime\n",
    "from pytz import timezone\n",
    "\n",
    "PROJECT = 'img-seg-3d' ## <== TODO: Change to your gcp project id\n",
    "USER_NAME = 'elvinzhu' ## <== TODO: Change to your user name\n",
    "BUCKET_NAME = 'vapit_job' ## <== TODO: Change to your bucket name\n",
    "PACKAGE_URI = f\"gs://{BUCKET_NAME}/trainer/xgboost/trainer-0.1.tar.gz\"  ## <== TODO: Change to your package uri \n",
    "\n",
    "# Compile pipeline \n",
    "pipeline_pkg_path=\"./train_pipeline.json\"\n",
    "pipeline_root = f\"gs://{BUCKET_NAME}/pipeline_root\"\n",
    "\n",
    "compiler.Compiler().compile(\n",
    "    pipeline_func=train_pipeline, \n",
    "    package_path=pipeline_pkg_path,\n",
    ")\n",
    "\n",
    "# Run pipeline\n",
    "JOB_FOLDER_NAME = 'xgb_train_job'\n",
    "REGION = 'us-central1'\n",
    "my_timezone = 'US/Pacific'\n",
    "        \n",
    "# Define pipeline input\n",
    "pipeline_params = {\n",
    "    \"job_name\": 'xgb_train_{}_{}'.format(\n",
    "        USER_NAME, datetime.now(timezone(my_timezone)).strftime(\"%m%d%y_%H%M\")\n",
    "        ),\n",
    "    \"project_id\": PROJECT,\n",
    "    \"region\": REGION,\n",
    "    \"user_name\": USER_NAME,\n",
    "    \"bucket_name\": BUCKET_NAME,\n",
    "    \"job_folder_name\": JOB_FOLDER_NAME,\n",
    "    \"input_file\": 'gs://tuti_asset/datasets/mortgage_structured.csv',\n",
    "    \"target_column\": 'TARGET',\n",
    "    \"package_uri\": PACKAGE_URI,\n",
    "    \"max_trial_count\": 4,\n",
    "    \"parallel_trial_count\": 2,\n",
    "    \"metric_id\": \"roc_auc\",\n",
    "    \"model_display_name\": \"vertex_pipeline_xgboost_model\"\n",
    "}\n",
    "\n",
    "# Create pipeline client\n",
    "api_client = AIPlatformClient(\n",
    "    project_id=PROJECT,\n",
    "    region=REGION,\n",
    ")\n",
    "response = api_client.create_run_from_job_spec(\n",
    "    job_spec_path=pipeline_pkg_path,\n",
    "    pipeline_root=pipeline_root,\n",
    "    parameter_values=pipeline_params\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "environment": {
   "name": "tf2-gpu.2-1.m71",
   "type": "gcloud",
   "uri": "gcr.io/deeplearning-platform-release/tf2-gpu.2-1:m71"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
