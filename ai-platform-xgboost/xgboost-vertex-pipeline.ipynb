{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ============================================================================== \\\n",
    " Copyright 2020 Google LLC. This software is provided as-is, without warranty \\\n",
    " or representation for any use or purpose. Your use of it is subject to your \\\n",
    " agreement with Google. \\\n",
    " ============================================================================== \n",
    " \n",
    " Author: Elvin Zhu, Chanchal Chatterjee \\\n",
    " Email: elvinzhu@google.com \\\n",
    "<img src=\"img/google-cloud-icon.jpg\" alt=\"Drawing\" style=\"width: 200px;\"/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !python3 -m pip install kfp==1.6.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cloudml-hypertune==0.1.0.dev6\n",
      "gcsfs==0.6.1\n",
      "google-api-python-client==1.7.11\n",
      "google-cloud==0.34.0\n",
      "matplotlib==3.2.1\n",
      "numpy==1.18.0\n",
      "pandas==1.2.1\n",
      "scikit-learn==0.22\n",
      "scipy==1.4.1\n",
      "tensorflow==2.1.0\n"
     ]
    }
   ],
   "source": [
    "!python3 -m pip freeze --user"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import kfp\n",
    "import kfp.components as comp\n",
    "import kfp.dsl as dsl\n",
    "from typing import NamedTuple\n",
    "from kfp.compiler import compiler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_preprocess(\n",
    "    bucket_name: str,\n",
    "    input_file: str,\n",
    "    target_column: str,\n",
    "    ) -> NamedTuple('PreprocessOutput', \n",
    "              [\n",
    "                  ('x_train_name', str),\n",
    "                  ('x_test_name', str),\n",
    "                  ('y_train_name', str),\n",
    "                  ('y_test_name', str),\n",
    "                  ('n_classes', str),\n",
    "              ]):\n",
    "    \n",
    "    from collections import namedtuple\n",
    "    from sklearn.model_selection import train_test_split\n",
    "    import pandas as pd\n",
    "    import os\n",
    "    import logging\n",
    "\n",
    "    logging.info(\"Loading {}\".format(input_file))\n",
    "    dataset = pd.read_csv(input_file)\n",
    "    # drop unique id column which is not useful for ML\n",
    "    dataset.drop(['LOAN_SEQUENCE_NUMBER'], axis=1, inplace=True)\n",
    "\n",
    "    # Convert categorical columns into one-hot encodings\n",
    "    str_cols = [col for col in dataset.columns if dataset[col].dtype == 'object']\n",
    "    dataset = pd.get_dummies(dataset, columns=str_cols)\n",
    "    n_classes = dataset[target_column].nunique()\n",
    "    logging.info(\"No. of Classes: {}\".format(n_classes))\n",
    "\n",
    "    # Split with a small test size so as to allow our model to train on more data\n",
    "    x_train, x_test, y_train, y_test = train_test_split(\n",
    "        dataset.drop(target_column, axis=1), \n",
    "        dataset[target_column], \n",
    "        test_size=0.1,\n",
    "        random_state=1,\n",
    "        shuffle=True, \n",
    "        stratify=dataset[target_column], \n",
    "        )\n",
    "\n",
    "    logging.info(\"x_train shape = {}\".format(x_train.shape))\n",
    "    logging.info(\"x_test shape = {}\".format(x_test.shape))\n",
    "    logging.info(\"y_train shape = {}\".format(y_train.shape))\n",
    "    logging.info(\"y_test shape = {}\".format(y_test.shape))\n",
    "\n",
    "    base_file_name = os.path.basename(input_file)\n",
    "    base_name, ext_name = os.path.splitext(base_file_name)\n",
    "    x_train_name = \"{}_x_train{}\".format(base_name, ext_name)\n",
    "    x_test_name = \"{}_x_test{}\".format(base_name, ext_name)\n",
    "    y_train_name = \"{}_y_train{}\".format(base_name, ext_name)\n",
    "    y_test_name = \"{}_y_test{}\".format(base_name, ext_name)\n",
    "    \n",
    "    x_train_name = os.path.join(\"gs://\", bucket_name, \"data_split_xgb\", x_train_name)\n",
    "    x_test_name = os.path.join(\"gs://\", bucket_name, \"data_split_xgb\", x_test_name)\n",
    "    y_train_name = os.path.join(\"gs://\", bucket_name, \"data_split_xgb\", y_train_name)\n",
    "    y_test_name = os.path.join(\"gs://\", bucket_name, \"data_split_xgb\", y_test_name)\n",
    "    \n",
    "    x_train.to_csv(x_train_name, index=False)\n",
    "    x_test.to_csv(x_test_name, index=False)\n",
    "    y_train.to_csv(y_train_name, index=False)\n",
    "    y_test.to_csv(y_test_name, index=False)\n",
    "\n",
    "    logging.info(\"x_train saved to {}\".format(x_train_name))\n",
    "    logging.info(\"x_test saved to {}\".format(x_test_name))\n",
    "    logging.info(\"y_train saved to {}\".format(y_train_name))\n",
    "    logging.info(\"y_test saved to {}\".format(y_test_name))\n",
    "    logging.info(\"finished\")\n",
    "    \n",
    "    PreprocessOutput = namedtuple('PreprocessOutput', \n",
    "        ['x_train_name', 'x_test_name', 'y_train_name', 'y_test_name', 'n_classes'])\n",
    "    return PreprocessOutput(\n",
    "        x_train_name=x_train_name,\n",
    "        x_test_name=x_test_name,\n",
    "        y_train_name=y_train_name,\n",
    "        y_test_name=y_test_name,\n",
    "        n_classes=str(n_classes),\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def hypertune(\n",
    "        project_id: str,\n",
    "        region: str,\n",
    "        job_name: str,\n",
    "        bucket_name: str,\n",
    "        job_folder_name: str,\n",
    "        train_feature_path: str,\n",
    "        train_label_path: str,\n",
    "        val_feature_path: str,\n",
    "        val_label_path: str,\n",
    "        n_classes: str,\n",
    "        metric_id: str,\n",
    "        max_trial_count: int,\n",
    "        parallel_trial_count: int,\n",
    "        package_uri: str,\n",
    "        executor_image_uri: str = 'us-docker.pkg.dev/vertex-ai/training/tf-cpu.2-2:latest',\n",
    "        python_module: str = \"trainer.train\",\n",
    "        api_endpoint: str = \"us-central1-aiplatform.googleapis.com\",\n",
    "        machine_type: str = \"n1-standard-4\",\n",
    "    ) -> NamedTuple('TrainOutput', \n",
    "              [('response', str), ('job_name', str)]):\n",
    "    from collections import namedtuple\n",
    "    from google.cloud import aiplatform\n",
    "    import subprocess\n",
    "    import logging\n",
    "\n",
    "    job_name = job_name + \"_hpt\"\n",
    "    job_dir = 'gs://{}/{}/{}'.format(\n",
    "        bucket_name,\n",
    "        job_folder_name,\n",
    "        job_name,\n",
    "        )\n",
    "\n",
    "    # The AI Platform services require regional API endpoints.\n",
    "    client_options = {\"api_endpoint\": api_endpoint}\n",
    "    # Initialize client that will be used to create and send requests.\n",
    "    # This client only needs to be created once, and can be reused for multiple requests.\n",
    "    client = aiplatform.gapic.JobServiceClient(client_options=client_options)\n",
    "    print(client)\n",
    "    \n",
    "    # study_spec\n",
    "    metric = {\n",
    "        \"metric_id\": metric_id,\n",
    "        \"goal\": aiplatform.gapic.StudySpec.MetricSpec.GoalType.MAXIMIZE,\n",
    "    }\n",
    "    print(metric)\n",
    "\n",
    "    max_depth = {\n",
    "            \"parameter_id\": \"max_depth\",\n",
    "            \"integer_value_spec\": {\"min_value\": 2, \"max_value\": 20},\n",
    "            \"scale_type\": aiplatform.gapic.StudySpec.ParameterSpec.ScaleType.UNIT_LINEAR_SCALE,\n",
    "    }\n",
    "    n_estimators = {\n",
    "            \"parameter_id\": \"n_estimators\",\n",
    "            \"integer_value_spec\": {\"min_value\": 10, \"max_value\": 200},\n",
    "            \"scale_type\": aiplatform.gapic.StudySpec.ParameterSpec.ScaleType.UNIT_LINEAR_SCALE,\n",
    "    }\n",
    "    booster = {\n",
    "        \"parameter_id\": \"booster\",\n",
    "        \"categorical_value_spec\": {\"values\": [\"gbtree\",\"gblinear\",\"dart\"]},\n",
    "    }\n",
    "\n",
    "    # trial_job_spec\n",
    "    machine_spec = {\n",
    "        \"machine_type\": machine_type,\n",
    "    }\n",
    "    worker_pool_spec = {\n",
    "        \"machine_spec\": machine_spec,\n",
    "        \"replica_count\": 1,\n",
    "        \"python_package_spec\": {\n",
    "            \"executor_image_uri\": executor_image_uri,\n",
    "            \"package_uris\": [package_uri],\n",
    "            \"python_module\": python_module,\n",
    "            \"args\": [\n",
    "                '--job-dir',\n",
    "                job_dir,\n",
    "                '--train_feature_name',\n",
    "                train_feature_path,\n",
    "                '--train_label_name',\n",
    "                train_label_path,\n",
    "                '--val_feature_name',\n",
    "                val_feature_path,\n",
    "                '--val_label_name',\n",
    "                val_label_path,\n",
    "                '--no_classes',\n",
    "                str(n_classes),\n",
    "            ],\n",
    "        },\n",
    "    }\n",
    "\n",
    "    # hyperparameter_tuning_job\n",
    "    hyperparameter_tuning_job = {\n",
    "        \"display_name\": job_name,\n",
    "        \"max_trial_count\": max_trial_count,\n",
    "        \"parallel_trial_count\": parallel_trial_count,\n",
    "        \"study_spec\": {\n",
    "            \"metrics\": [metric],\n",
    "            \"parameters\": [max_depth, n_estimators, booster],\n",
    "        },\n",
    "        \"trial_job_spec\": {\"worker_pool_specs\": [worker_pool_spec]},\n",
    "    }\n",
    "    print(hyperparameter_tuning_job)\n",
    "    \n",
    "    parent = f\"projects/{project_id}/locations/{region}\"\n",
    "    response = client.create_hyperparameter_tuning_job(\n",
    "        parent=parent, hyperparameter_tuning_job=hyperparameter_tuning_job\n",
    "    )\n",
    "    print(response)\n",
    "    logging.info(f\"response: {response}\")\n",
    "    hpt_job_name = response.name.split('/')[-1]\n",
    "        \n",
    "    TrainOutput = namedtuple('TrainOutput',['response', 'job_name'])\n",
    "    return TrainOutput(response=response, job_name=hpt_job_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# project_id = 'img-seg-3d'\n",
    "# region = 'us-central1'\n",
    "# job_name = 'xgb_train_elvinzhu_061421_2259'\n",
    "# bucket_name = 'tuti_job'\n",
    "# job_folder_name = 'xgb_train_job'\n",
    "# train_feature_path = \"gs://tuti_job/data_split_xgb/mortgage_structured_x_train.csv\"\n",
    "# train_label_path = \"gs://tuti_job/data_split_xgb/mortgage_structured_y_train.csv\"\n",
    "# val_feature_path = \"gs://tuti_job/data_split_xgb/mortgage_structured_x_test.csv\"\n",
    "# val_label_path = \"gs://tuti_job/data_split_xgb/mortgage_structured_y_test.csv\"\n",
    "# n_classes = '4'\n",
    "# metric_id = 'roc_auc'\n",
    "# max_trial_count = 4\n",
    "# parallel_trial_count = 1\n",
    "# package_uri = \"gs://vapit_job/trainer/xgboost/trainer-0.1.tar.gz\"\n",
    "# executor_image_uri = 'us-docker.pkg.dev/vertex-ai/training/tf-cpu.2-2:latest'\n",
    "# python_module = \"trainer.train\"\n",
    "# api_endpoint = \"us-central1-aiplatform.googleapis.com\"\n",
    "# machine_type = \"n1-standard-4\"\n",
    "\n",
    "# from collections import namedtuple\n",
    "# from google.cloud import aiplatform\n",
    "# import subprocess\n",
    "# import logging\n",
    "\n",
    "# job_name = job_name + \"_hpt\"\n",
    "# job_dir = 'gs://{}/{}/{}'.format(\n",
    "#     bucket_name,\n",
    "#     job_folder_name,\n",
    "#     job_name,\n",
    "#     )\n",
    "\n",
    "# # The AI Platform services require regional API endpoints.\n",
    "# client_options = {\"api_endpoint\": api_endpoint}\n",
    "# # Initialize client that will be used to create and send requests.\n",
    "# # This client only needs to be created once, and can be reused for multiple requests.\n",
    "# client = aiplatform.gapic.JobServiceClient(client_options=client_options)\n",
    "\n",
    "# # study_spec\n",
    "# metric = {\n",
    "#     \"metric_id\": metric_id,\n",
    "#     \"goal\": aiplatform.gapic.StudySpec.MetricSpec.GoalType.MAXIMIZE,\n",
    "# }\n",
    "\n",
    "# max_depth = {\n",
    "#         \"parameter_id\": \"max_depth\",\n",
    "#         \"integer_value_spec\": {\"min_value\": 2, \"max_value\": 20},\n",
    "#         \"scale_type\": aiplatform.gapic.StudySpec.ParameterSpec.ScaleType.UNIT_LINEAR_SCALE,\n",
    "# }\n",
    "# n_estimators = {\n",
    "#         \"parameter_id\": \"n_estimators\",\n",
    "#         \"integer_value_spec\": {\"min_value\": 10, \"max_value\": 200},\n",
    "#         \"scale_type\": aiplatform.gapic.StudySpec.ParameterSpec.ScaleType.UNIT_LINEAR_SCALE,\n",
    "# }\n",
    "# booster = {\n",
    "#     \"parameter_id\": \"booster\",\n",
    "#     \"categorical_value_spec\": {\"values\": [\"gbtree\",\"gblinear\",\"dart\"]},\n",
    "# }\n",
    "\n",
    "# # trial_job_spec\n",
    "# machine_spec = {\n",
    "#     \"machine_type\": machine_type,\n",
    "# }\n",
    "# worker_pool_spec = {\n",
    "#     \"machine_spec\": machine_spec,\n",
    "#     \"replica_count\": 1,\n",
    "#     \"python_package_spec\": {\n",
    "#         \"executor_image_uri\": executor_image_uri,\n",
    "#         \"package_uris\": [package_uri],\n",
    "#         \"python_module\": python_module,\n",
    "#         \"args\": [\n",
    "#             '--job-dir',\n",
    "#             job_dir,\n",
    "#             '--train_feature_name',\n",
    "#             train_feature_path,\n",
    "#             '--train_label_name',\n",
    "#             train_label_path,\n",
    "#             '--val_feature_name',\n",
    "#             val_feature_path,\n",
    "#             '--val_label_name',\n",
    "#             val_label_path,\n",
    "#             '--no_classes',\n",
    "#             n_classes,\n",
    "#         ],\n",
    "#     },\n",
    "# }\n",
    "\n",
    "# # hyperparameter_tuning_job\n",
    "# hyperparameter_tuning_job = {\n",
    "#     \"display_name\": job_name,\n",
    "#     \"max_trial_count\": max_trial_count,\n",
    "#     \"parallel_trial_count\": parallel_trial_count,\n",
    "#     \"study_spec\": {\n",
    "#         \"metrics\": [metric],\n",
    "#         \"parameters\": [max_depth, n_estimators, booster],\n",
    "#     },\n",
    "#     \"trial_job_spec\": {\"worker_pool_specs\": [worker_pool_spec]},\n",
    "# }\n",
    "# parent = f\"projects/{project_id}/locations/{region}\"\n",
    "# response = client.create_hyperparameter_tuning_job(\n",
    "#     parent=parent, hyperparameter_tuning_job=hyperparameter_tuning_job\n",
    "# )\n",
    "# logging.info(f\"response: {response}\")\n",
    "# hpt_job_name = response.name.split('/')[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_hpt_job_status(\n",
    "        project_id: str,\n",
    "        region: str,\n",
    "        hpt_job_name: str,\n",
    "        api_endpoint: str = \"us-central1-aiplatform.googleapis.com\",\n",
    "        time_out: int = 9000, # timeout after 2.5 hours by default\n",
    "        time_sleep: int = 60, # check status every minute by default\n",
    "    ) -> NamedTuple('Ghp_Output', \n",
    "              [('booster', str), ('max_depth', str), ('n_estimators', str)]):\n",
    "    \n",
    "    from collections import namedtuple\n",
    "    from google.cloud import aiplatform\n",
    "\n",
    "    import time\n",
    "    import logging\n",
    "\n",
    "    time0 = time.time()\n",
    "    status = False\n",
    "    \n",
    "    while time.time() - time0 < time_out:    \n",
    "        client_options = {\"api_endpoint\": api_endpoint}\n",
    "        client = aiplatform.gapic.JobServiceClient(client_options=client_options)\n",
    "        name = client.hyperparameter_tuning_job_path(\n",
    "            project=project_id,\n",
    "            location=region,\n",
    "            hyperparameter_tuning_job=hpt_job_name,\n",
    "        )\n",
    "        response = client.get_hyperparameter_tuning_job(name=name)\n",
    "        logging.info(f\"response: {response}\")\n",
    "        \n",
    "        if 'state' in response and \"JobState.JOB_STATE_SUCCEEDED\" == str(response.state):\n",
    "            status = True\n",
    "            break\n",
    "        else:\n",
    "            logging.info(\"Checking status ...\")\n",
    "            logging.info(response)\n",
    "            time.sleep(time_sleep)\n",
    "            \n",
    "    if not status:\n",
    "        raise TimeoutError(\"No successful job found. Timeout after {} seconds\".format(time_out))\n",
    "\n",
    "    max_ind = 0\n",
    "    max_val = 0\n",
    "    for ind, trials in enumerate(response.trials):\n",
    "        value = trials.final_measurement.metrics[0].value\n",
    "        logging.info(f\"Metrics Value (larger is better): {value}\")\n",
    "        if value > max_val:\n",
    "            max_val = value\n",
    "            max_ind = ind\n",
    "\n",
    "    param_dict = {}\n",
    "    for params in response.trials[max_ind].parameters:\n",
    "        param_dict[params.parameter_id] = params.value\n",
    "\n",
    "    booster=param_dict['booster']\n",
    "    max_depth=str(int(param_dict['max_depth']))\n",
    "    n_estimators=str(int(param_dict['n_estimators']))\n",
    "\n",
    "    logging.info(f\"booster {booster}\")\n",
    "    logging.info(f\"max_depth {max_depth}\")\n",
    "    logging.info(f\"n_estimators {n_estimators}\")\n",
    "        \n",
    "    Ghp_Output = namedtuple('Ghp_Output',['booster', 'max_depth', 'n_estimators'])\n",
    "    return Ghp_Output(booster=str(booster), max_depth=str(max_depth), n_estimators=str(n_estimators) )        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(\n",
    "        project_id: str,\n",
    "        region: str,\n",
    "        job_name: str,\n",
    "        bucket_name: str,\n",
    "        job_folder_name: str,\n",
    "        train_feature_path: str,\n",
    "        train_label_path: str,\n",
    "        n_classes: str,\n",
    "        n_estimators: str,\n",
    "        max_depth: str,\n",
    "        booster: str,\n",
    "        package_uri: str,\n",
    "        executor_image_uri: str = 'us-docker.pkg.dev/vertex-ai/training/tf-cpu.2-2:latest',\n",
    "        python_module: str = \"trainer.train\",\n",
    "        api_endpoint: str = \"us-central1-aiplatform.googleapis.com\",\n",
    "        machine_type: str = \"n1-standard-4\",\n",
    "    ) -> NamedTuple('TrainOutput', \n",
    "              [('response', str), ('job_name', str)]):\n",
    "    \n",
    "    from collections import namedtuple\n",
    "    from google.cloud import aiplatform\n",
    "    import logging\n",
    "\n",
    "    job_dir = 'gs://{}/{}/{}'.format(\n",
    "        bucket_name,\n",
    "        job_folder_name,\n",
    "        job_name,\n",
    "        )\n",
    "    \n",
    "    # The AI Platform services require regional API endpoints.\n",
    "    client_options = {\"api_endpoint\": api_endpoint}\n",
    "    # Initialize client that will be used to create and send requests.\n",
    "    # This client only needs to be created once, and can be reused for multiple requests.\n",
    "    client = aiplatform.gapic.JobServiceClient(client_options=client_options)\n",
    "    custom_job = {\n",
    "        \"display_name\": job_name,\n",
    "        \"job_spec\": {\n",
    "            \"worker_pool_specs\": [\n",
    "                {\n",
    "                    \"machine_spec\": {\n",
    "                        \"machine_type\": machine_type,\n",
    "                    },\n",
    "                    \"replica_count\": 1,\n",
    "                    \"python_package_spec\": {\n",
    "                        \"executor_image_uri\": executor_image_uri,\n",
    "                        \"package_uris\": [package_uri],\n",
    "                        \"python_module\": python_module,\n",
    "                        \"args\": [\n",
    "                          '--job-dir',\n",
    "                          job_dir,\n",
    "                          '--train_feature_name',\n",
    "                          train_feature_path,\n",
    "                          '--train_label_name',\n",
    "                          train_label_path,\n",
    "                          '--no_classes',\n",
    "                          str(n_classes),\n",
    "                          '--n_estimators',\n",
    "                          str(n_estimators),\n",
    "                          '--max_depth',\n",
    "                          str(max_depth),\n",
    "                          '--booster',\n",
    "                          str(booster)\n",
    "                        ],\n",
    "                    },\n",
    "                }\n",
    "            ]\n",
    "        },\n",
    "    }\n",
    "    parent = f\"projects/{project_id}/locations/{regino}\"\n",
    "    response = client.create_custom_job(parent=parent, custom_job=custom_job)\n",
    "    logging.info(f\"response: {response}\")\n",
    "    training_job_id = response.name.split('/')[-1]\n",
    "    \n",
    "    TrainOutput = namedtuple('TrainOutput',['response', 'job_name'])\n",
    "    return TrainOutput(response=response, job_name=training_job_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_job_status(\n",
    "        project_id: str,\n",
    "        region: str,\n",
    "        job_name: str,\n",
    "        api_endpoint: str = \"us-central1-aiplatform.googleapis.com\",\n",
    "        time_out: int = 9000, # timeout after 2.5 hours by default\n",
    "        time_sleep: int = 60, # check status every minute by default\n",
    "    ) -> NamedTuple('Gct_Output', \n",
    "              [('response', str), ('status', bool)]):\n",
    "    \n",
    "    from collections import namedtuple\n",
    "    from google.cloud import aiplatform\n",
    "\n",
    "    import time\n",
    "    import logging\n",
    "\n",
    "    time0 = time.time()\n",
    "    status = False\n",
    "    \n",
    "    while time.time() - time0 < time_out:    \n",
    "        client_options = {\"api_endpoint\": api_endpoint}\n",
    "        client = aiplatform.gapic.JobServiceClient(client_options=client_options)\n",
    "        name = client.custom_job_path(\n",
    "            project=project_id,\n",
    "            location=region,\n",
    "            custom_job=job_name,\n",
    "        )\n",
    "        response = client.get_custom_job(name=name)\n",
    "        logging.info(f\"response: {response}\")\n",
    "        \n",
    "        if 'state' in response and \"JobState.JOB_STATE_SUCCEEDED\" == str(response.state):\n",
    "            status = True\n",
    "            break\n",
    "        else:\n",
    "            logging.info(\"Checking status ...\")\n",
    "            logging.info(response)\n",
    "            time.sleep(time_sleep)\n",
    "            \n",
    "    if not status:\n",
    "        raise TimeoutError(\"No successful job found. Timeout after {} seconds\".format(time_out))\n",
    "       \n",
    "    Gct_Output = namedtuple('Gct_Output',['response', 'status'])\n",
    "    return Gct_Output(response=response, status=status)        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def import_model(\n",
    "    bucket_name: str,\n",
    "    job_folder_name: str,\n",
    "    job_name: str,\n",
    "    model_display_name: str,\n",
    "    serving_container_image_uri: str = 'us-docker.pkg.dev/vertex-ai/prediction/xgboost-cpu.1-3:latest',\n",
    "    ) -> NamedTuple('ImportModelOutput', \n",
    "              [('model_id', str)]): \n",
    "    \n",
    "    from google.cloud import aiplatform\n",
    "    from collections import namedtuple\n",
    "    import logging\n",
    "       \n",
    "    latest_model_dir = \"gs://{}/{}/{}\".format(bucket_name, job_folder_name, job_name)\n",
    "    \n",
    "    response = aiplatform.Model.upload(\n",
    "        display_name = model_display_name,\n",
    "        serving_container_image_uri = serving_container_image_uri,\n",
    "        artifact_uri = latest_model_dir\n",
    "    )\n",
    "    model_id = response.name.split('/')[-1]\n",
    "    \n",
    "    ImportModelOutput = namedtuple('ImportModelOutput',['model_id'])\n",
    "    return ImportModelOutput(model_id=model_id)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compile python functions to components"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "component_dir = \"./components\"\n",
    "# base_image = \"us-docker.pkg.dev/vertex-ai/training/tf-cpu.2-2:latest\"\n",
    "# base_image = \"gcr.io/deeplearning-platform-release/tf2-gpu.2-1\"\n",
    "base_image = \"gcr.io/img-seg-3d/vertex_base@sha256:3609fa8d584cbf97d071de3028b3b92d87c901726927e8a4ebb6ff5d9fff87a3\"\n",
    "\n",
    "yaml_name = '{}/preprocess.yaml'.format(component_dir)\n",
    "preprocess_op = comp.func_to_container_op(\n",
    "    data_preprocess, \n",
    "    output_component_file=yaml_name,\n",
    "    base_image=base_image)\n",
    "\n",
    "yaml_name = '{}/train_hpt.yaml'.format(component_dir)\n",
    "hypertune_op = comp.func_to_container_op(\n",
    "    hypertune, \n",
    "    output_component_file=yaml_name,\n",
    "    base_image=base_image)\n",
    "\n",
    "yaml_name = '{}/ghp.yaml'.format(component_dir) # Get hypertune\n",
    "ghp_op = comp.func_to_container_op(\n",
    "    get_hpt_job_status, \n",
    "    output_component_file=yaml_name,\n",
    "    base_image=base_image)\n",
    "\n",
    "yaml_name = '{}/train.yaml'.format(component_dir)\n",
    "train_op = comp.func_to_container_op(\n",
    "    train, \n",
    "    output_component_file=yaml_name,\n",
    "    base_image=base_image)\n",
    "\n",
    "yaml_name = '{}/gct.yaml'.format(component_dir) # Get custom train\n",
    "gct_op = comp.func_to_container_op(\n",
    "    get_job_status, \n",
    "    output_component_file=yaml_name,\n",
    "    base_image=base_image)\n",
    "\n",
    "yaml_name = '{}/import_model.yaml'.format(component_dir)\n",
    "import_model_op = comp.func_to_container_op(\n",
    "    import_model, \n",
    "    output_component_file=yaml_name,\n",
    "    base_image=base_image)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compile KFP pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "@dsl.pipeline(\n",
    "   name='vertex-training-pipeline',\n",
    "   description='A example of vertex training pipeline with custom xgboost model.'\n",
    ")\n",
    "def train_pipeline(\n",
    "    job_name: str,\n",
    "    project_id: str,\n",
    "    region: str,\n",
    "    user_name: str,\n",
    "    bucket_name: str,\n",
    "    input_file: str,\n",
    "    job_folder_name: str,\n",
    "    target_column: str,\n",
    "    package_uri: str,\n",
    "    metric_id: str,\n",
    "    max_trial_count: int,\n",
    "    parallel_trial_count: int,\n",
    "    model_display_name: str):\n",
    "    \n",
    "    preprocess_task = preprocess_op(\n",
    "        bucket_name = bucket_name,\n",
    "        input_file = input_file,\n",
    "        target_column = target_column,\n",
    "    )\n",
    "    \n",
    "    hpt_task = hypertune_op(\n",
    "        project_id = project_id,\n",
    "        region = region,\n",
    "        job_name = job_name,\n",
    "        bucket_name = bucket_name,\n",
    "        job_folder_name = job_folder_name,\n",
    "        train_feature_path = preprocess_task.outputs['x_train_name'],\n",
    "        train_label_path   = preprocess_task.outputs['y_train_name'],\n",
    "        val_feature_path   = preprocess_task.outputs['x_test_name'],\n",
    "        val_label_path     = preprocess_task.outputs['y_test_name'],\n",
    "        n_classes = preprocess_task.outputs['n_classes'],\n",
    "        metric_id = metric_id,\n",
    "        max_trial_count = max_trial_count,\n",
    "        parallel_trial_count = parallel_trial_count,\n",
    "        package_uri = package_uri,\n",
    "    )\n",
    "    \n",
    "    ghp_task = ghp_op(\n",
    "        project_id = project_id,\n",
    "        region = region,\n",
    "        hpt_job_name = hpt_task.outputs['job_name'],\n",
    "    )\n",
    "    \n",
    "    train_task = train_op(\n",
    "        project_id = project_id,\n",
    "        region = region, \n",
    "        job_name = job_name,\n",
    "        bucket_name = bucket_name,\n",
    "        job_folder_name = job_folder_name,\n",
    "        train_feature_path = preprocess_task.outputs['x_train_name'],\n",
    "        train_label_path   = preprocess_task.outputs['y_train_name'],\n",
    "        n_classes = preprocess_task.outputs['n_classes'],\n",
    "        n_estimators = ghp_task.outputs['n_estimators'],\n",
    "        max_depth = ghp_task.outputs['max_depth'],\n",
    "        booster = ghp_task.outputs['booster'],\n",
    "        package_uri = package_uri,\n",
    "    )\n",
    "    \n",
    "    gct_task = gct_op(\n",
    "        project_id = project_id,\n",
    "        region = region,\n",
    "        job_name = train_task.outputs['job_name']\n",
    "    )\n",
    "    \n",
    "#     deploy_task = deploy_op(\n",
    "#         status = lro_task_2.outputs['status'],\n",
    "#         bucket_name = bucket_name,\n",
    "#         job_folder_name = job_folder_name,\n",
    "#         job_name = train_task.outputs['job_name'],\n",
    "#         region = 'global',\n",
    "#         model_framework = 'XGBOOST',\n",
    "#         model_name = deployed_model_name,\n",
    "#         model_version = deployed_model_version,\n",
    "#         model_description = deployed_model_description,\n",
    "#     )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline_pkg_path=\"./train_pipeline.tar.gz\"\n",
    "pipeline_root = \"gs://tuti_job/pipeline_root\"\n",
    "\n",
    "compiler.Compiler().compile(\n",
    "    pipeline_func=train_pipeline, \n",
    "    package_path=pipeline_pkg_path,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run KFP pipeline on AI Platform hosted Kubernetes cluster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<a href=\"https://6ff530db99970db2-dot-us-central2.pipelines.googleusercontent.com/#/experiments/details/6f7fbd64-4ef1-405b-8f4c-7a1f1c8e952c\" target=\"_blank\" >Experiment details</a>."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<a href=\"https://6ff530db99970db2-dot-us-central2.pipelines.googleusercontent.com/#/runs/details/d7cbfc89-7825-45c5-9287-fa082b64ba81\" target=\"_blank\" >Run details</a>."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "## ============== Uncomment to run the pipeline ==============\n",
    "from datetime import datetime\n",
    "from pytz import timezone\n",
    "\n",
    "PROJECT = 'img-seg-3d'\n",
    "REGION = 'us-central1'\n",
    "my_timezone = 'US/Pacific'\n",
    "        \n",
    "# Define pipeline input\n",
    "pipeline_params = {\n",
    "    \"job_name\": 'xgb_train_elvinzhu_{}'.format(\n",
    "        datetime.now(timezone(my_timezone)).strftime(\"%m%d%y_%H%M\")\n",
    "        ),\n",
    "    \"project_id\": PROJECT,\n",
    "    \"region\": REGION,\n",
    "    \"user_name\": 'elvinzhu',\n",
    "    \"bucket_name\": 'tuti_job',\n",
    "    \"job_folder_name\": 'xgb_train_job',\n",
    "    \"input_file\": 'gs://tuti_asset/datasets/mortgage_structured.csv',\n",
    "    \"target_column\": 'TARGET',\n",
    "    \"package_uri\": \"gs://vapit_job/trainer/xgboost/trainer-0.1.tar.gz\",\n",
    "    \"max_trial_count\": 4,\n",
    "    \"parallel_trial_count\": 1,\n",
    "    \"metric_id\": \"roc_auc\",\n",
    "    \"model_display_name\": \"vertex_pipeline_xgboost_model\"\n",
    "}\n",
    "\n",
    "kfp_host_name = 'https://6ff530db99970db2-dot-us-central2.pipelines.googleusercontent.com'\n",
    "kfp_exp_name = 'xgboost_ai_platform'\n",
    "kfp_run_name = 'demo_xgboost'\n",
    "\n",
    "client = kfp.Client(host=kfp_host_name) \n",
    "# Create Experiment GROUP\n",
    "exp = client.create_experiment(name = kfp_exp_name)\n",
    "# Create Experiment RUN\n",
    "run = client.run_pipeline(exp.id, kfp_run_name, pipeline_pkg_path, params=pipeline_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "absl-py==0.11.0\n",
      "adal @ file:///home/conda/feedstock_root/build_artifacts/adal_1603322199803/work\n",
      "aiohttp @ file:///home/conda/feedstock_root/build_artifacts/aiohttp_1607974765015/work\n",
      "ansiwrap==0.8.4\n",
      "apache-beam==2.17.0\n",
      "appdirs @ file:///home/conda/feedstock_root/build_artifacts/appdirs_1603108395799/work\n",
      "argon2-cffi @ file:///home/conda/feedstock_root/build_artifacts/argon2-cffi_1605217006346/work\n",
      "arrow @ file:///home/conda/feedstock_root/build_artifacts/arrow_1602526723127/work\n",
      "asn1crypto @ file:///home/conda/feedstock_root/build_artifacts/asn1crypto_1595949944546/work\n",
      "astor==0.8.1\n",
      "astropy @ file:///home/conda/feedstock_root/build_artifacts/astropy_1606674796685/work\n",
      "async-generator==1.10\n",
      "async-timeout==3.0.1\n",
      "attrs @ file:///home/conda/feedstock_root/build_artifacts/attrs_1605083924122/work\n",
      "avro-python3==1.10.1\n",
      "backcall @ file:///home/conda/feedstock_root/build_artifacts/backcall_1592338393461/work\n",
      "backports.functools-lru-cache==1.6.1\n",
      "binaryornot==0.4.4\n",
      "black @ file:///home/conda/feedstock_root/build_artifacts/black-recipe_1599478779128/work\n",
      "bleach @ file:///home/conda/feedstock_root/build_artifacts/bleach_1600454382015/work\n",
      "blinker==1.4\n",
      "Bottleneck @ file:///home/conda/feedstock_root/build_artifacts/bottleneck_1602497275686/work\n",
      "brotlipy==0.7.0\n",
      "build==0.4.0\n",
      "cached-property==1.5.2\n",
      "cachetools @ file:///home/conda/feedstock_root/build_artifacts/cachetools_1593420445823/work\n",
      "caip-notebooks-serverextension @ file:///opt/conda/conda-bld/dlenv-base_1609194679764/work/packages/server_extension\n",
      "certifi==2020.12.5\n",
      "cffi @ file:///home/conda/feedstock_root/build_artifacts/cffi_1606601125223/work\n",
      "chardet==4.0.0\n",
      "click==7.1.2\n",
      "cloudml-hypertune==0.1.0.dev6\n",
      "cloudpickle @ file:///home/conda/feedstock_root/build_artifacts/cloudpickle_1598400192773/work\n",
      "colorama @ file:///home/conda/feedstock_root/build_artifacts/colorama_1602866480661/work\n",
      "conda==4.9.2\n",
      "conda-package-handling @ file:///home/conda/feedstock_root/build_artifacts/conda-package-handling_1602876795648/work\n",
      "confuse @ file:///home/conda/feedstock_root/build_artifacts/confuse_1605988134315/work\n",
      "cookiecutter==1.7.2\n",
      "crcmod==1.7\n",
      "cryptography @ file:///home/conda/feedstock_root/build_artifacts/cryptography_1607606133162/work\n",
      "cycler==0.10.0\n",
      "decorator==4.4.2\n",
      "defusedxml==0.6.0\n",
      "Deprecated==1.2.11\n",
      "dill==0.3.0\n",
      "docker @ file:///home/conda/feedstock_root/build_artifacts/docker-py_1608677399996/work\n",
      "docker-pycreds==0.4.0\n",
      "docopt==0.6.2\n",
      "docstring-parser==0.7.3\n",
      "entrypoints @ file:///home/conda/feedstock_root/build_artifacts/entrypoints_1605121927639/work/dist/entrypoints-0.3-py2.py3-none-any.whl\n",
      "explainable-ai-sdk @ file:///opt/conda/conda-bld/dlenv-tf-2-1-cpu_1609196001920/work/explainable_ai_sdk-1.0-py3-none-any.whl\n",
      "explainers @ file:///opt/conda/conda-bld/dlenv-tf-2-1-cpu_1609196001920/work/explainers-0.1-cp37-cp37m-linux_x86_64.whl\n",
      "fastavro==0.21.24\n",
      "fasteners==0.16\n",
      "fire==0.4.0\n",
      "fsspec @ file:///home/conda/feedstock_root/build_artifacts/fsspec_1608050797851/work\n",
      "future==0.18.2\n",
      "gast==0.2.2\n",
      "gcsfs==0.6.1\n",
      "gitdb @ file:///home/conda/feedstock_root/build_artifacts/gitdb_1588651542737/work\n",
      "GitPython @ file:///home/conda/feedstock_root/build_artifacts/gitpython_1603460643560/work\n",
      "google-api-core @ file:///Users/runner/miniforge3/conda-bld/google-api-core-split_1601998551628/work\n",
      "google-api-python-client==1.7.11\n",
      "google-apitools==0.5.28\n",
      "google-auth @ file:///home/conda/feedstock_root/build_artifacts/google-auth_1608136875028/work\n",
      "google-auth-httplib2 @ file:///home/conda/feedstock_root/build_artifacts/google-auth-httplib2_1601483325475/work\n",
      "google-auth-oauthlib @ file:///home/conda/feedstock_root/build_artifacts/google-auth-oauthlib_1603996258953/work\n",
      "google-cloud==0.34.0\n",
      "google-cloud-aiplatform==1.0.1\n",
      "google-cloud-automl==2.3.0\n",
      "google-cloud-bigquery==1.26.1\n",
      "google-cloud-bigquery-storage==1.0.0\n",
      "google-cloud-bigtable==1.4.0\n",
      "google-cloud-core==1.6.0\n",
      "google-cloud-dataproc==1.1.1\n",
      "google-cloud-datastore==1.12.0\n",
      "google-cloud-firestore==1.8.1\n",
      "google-cloud-kms==1.4.0\n",
      "google-cloud-language==1.3.0\n",
      "google-cloud-logging==1.15.1\n",
      "google-cloud-monitoring==1.1.0\n",
      "google-cloud-pubsub==1.7.0\n",
      "google-cloud-scheduler==1.3.0\n",
      "google-cloud-spanner==1.17.1\n",
      "google-cloud-speech==1.3.2\n",
      "google-cloud-storage==1.32.0\n",
      "google-cloud-tasks==1.5.0\n",
      "google-cloud-translate==2.0.2\n",
      "google-cloud-videointelligence==1.15.0\n",
      "google-cloud-vision==1.0.0\n",
      "google-crc32c==1.1.0\n",
      "google-pasta==0.2.0\n",
      "google-resumable-media==1.3.0\n",
      "googleapis-common-protos @ file:///home/conda/feedstock_root/build_artifacts/googleapis-common-protos-feedstock_1604414770577/work\n",
      "grpc-google-iam-v1==0.12.3\n",
      "grpcio @ file:///home/conda/feedstock_root/build_artifacts/grpcio_1607005037559/work\n",
      "grpcio-gcp @ file:///home/conda/feedstock_root/build_artifacts/grpcio-gcp_1604414757985/work\n",
      "h5py==3.1.0\n",
      "hdfs==2.5.8\n",
      "htmlmin==0.1.12\n",
      "httplib2 @ file:///home/conda/feedstock_root/build_artifacts/httplib2_1590007784658/work\n",
      "idna @ file:///tmp/build/80754af9/idna_1593446292537/work\n",
      "ImageHash @ file:///home/conda/feedstock_root/build_artifacts/imagehash_1605809872811/work\n",
      "importlib-metadata @ file:///home/conda/feedstock_root/build_artifacts/importlib-metadata_1607975937419/work\n",
      "iniconfig==1.1.1\n",
      "ipykernel @ file:///home/conda/feedstock_root/build_artifacts/ipykernel_1605455367082/work/dist/ipykernel-5.3.4-py3-none-any.whl\n",
      "ipython @ file:///home/conda/feedstock_root/build_artifacts/ipython_1604159552583/work\n",
      "ipython-genutils==0.2.0\n",
      "ipython-sql @ file:///home/conda/feedstock_root/build_artifacts/ipython-sql_1602667927030/work\n",
      "ipywidgets @ file:///home/conda/feedstock_root/build_artifacts/ipywidgets_1609038995114/work\n",
      "jedi @ file:///home/conda/feedstock_root/build_artifacts/jedi_1609162397074/work\n",
      "Jinja2==2.11.2\n",
      "jinja2-time==0.2.0\n",
      "joblib @ file:///home/conda/feedstock_root/build_artifacts/joblib_1607956439537/work\n",
      "json5 @ file:///home/conda/feedstock_root/build_artifacts/json5_1600692310011/work\n",
      "jsonschema @ file:///home/conda/feedstock_root/build_artifacts/jsonschema_1602551949684/work\n",
      "jupyter==1.0.0\n",
      "jupyter-aihub-deploy-extension @ file:///opt/conda/conda-bld/dlenv-base_1609194679764/work/packages/aihub/jupyter_aihub_deploy_extension\n",
      "jupyter-client @ file:///home/conda/feedstock_root/build_artifacts/jupyter_client_1598486169312/work\n",
      "jupyter-console==6.2.0\n",
      "jupyter-core @ file:///home/conda/feedstock_root/build_artifacts/jupyter_core_1605735018888/work\n",
      "jupyter-http-over-ws==0.0.8\n",
      "jupyterlab==1.2.16\n",
      "jupyterlab-git @ file:///home/conda/feedstock_root/build_artifacts/jupyterlab-git_1598805204722/work\n",
      "jupyterlab-pygments @ file:///home/conda/feedstock_root/build_artifacts/jupyterlab_pygments_1601375948261/work\n",
      "jupyterlab-server @ file:///home/conda/feedstock_root/build_artifacts/jupyterlab_server_1593951277307/work\n",
      "jupyterlab-widgets @ file:///home/conda/feedstock_root/build_artifacts/jupyterlab_widgets_1609173350931/work\n",
      "Keras==2.3.1\n",
      "Keras-Applications==1.0.8\n",
      "Keras-Preprocessing==1.1.0\n",
      "keras-tuner==1.0.2\n",
      "kfp==1.6.2\n",
      "kfp-pipeline-spec==0.1.8\n",
      "kfp-server-api==1.3.0\n",
      "kiwisolver @ file:///home/conda/feedstock_root/build_artifacts/kiwisolver_1604322297326/work\n",
      "kubernetes==11.0.0\n",
      "llvmlite==0.35.0\n",
      "Markdown @ file:///home/conda/feedstock_root/build_artifacts/markdown_1603668500820/work\n",
      "MarkupSafe @ file:///home/conda/feedstock_root/build_artifacts/markupsafe_1602267316845/work\n",
      "matplotlib==3.2.1\n",
      "missingno==0.4.2\n",
      "mistune @ file:///home/conda/feedstock_root/build_artifacts/mistune_1605115652256/work\n",
      "ml-metadata==0.21.2\n",
      "mock==2.0.0\n",
      "multidict @ file:///home/conda/feedstock_root/build_artifacts/multidict_1607973093237/work\n",
      "multimethod @ file:///home/conda/feedstock_root/build_artifacts/multimethod_1603129052241/work\n",
      "mypy-extensions @ file:///home/conda/feedstock_root/build_artifacts/mypy_extensions_1607883174369/work\n",
      "nb-conda @ file:///home/conda/feedstock_root/build_artifacts/nb_conda_1603265693244/work\n",
      "nb-conda-kernels @ file:///home/conda/feedstock_root/build_artifacts/nb_conda_kernels_1606762466752/work\n",
      "nbclient @ file:///home/conda/feedstock_root/build_artifacts/nbclient_1602859080374/work\n",
      "nbconvert @ file:///home/conda/feedstock_root/build_artifacts/nbconvert_1605401831777/work\n",
      "nbdime @ file:///home/conda/feedstock_root/build_artifacts/nbdime_1601645814261/work\n",
      "nbformat @ file:///home/conda/feedstock_root/build_artifacts/nbformat_1602732862338/work\n",
      "nest-asyncio @ file:///home/conda/feedstock_root/build_artifacts/nest-asyncio_1605195931949/work\n",
      "networkx @ file:///home/conda/feedstock_root/build_artifacts/networkx_1598210780226/work\n",
      "notebook @ file:///home/conda/feedstock_root/build_artifacts/notebook_1608772658455/work\n",
      "notebook-executor @ file:///opt/conda/conda-bld/dlenv-base_1609194679764/work/packages/notebook_executor\n",
      "numba @ file:///home/conda/feedstock_root/build_artifacts/numba_1607010268229/work\n",
      "numpy==1.18.0\n",
      "oauth2client==3.0.0\n",
      "oauthlib==3.1.0\n",
      "olefile @ file:///home/conda/feedstock_root/build_artifacts/olefile_1602866521163/work\n",
      "opt-einsum==3.3.0\n",
      "packaging @ file:///home/conda/feedstock_root/build_artifacts/packaging_1607785313469/work\n",
      "pandas==1.2.1\n",
      "pandas-profiling @ file:///home/conda/feedstock_root/build_artifacts/pandas-profiling_1589366436905/work\n",
      "pandocfilters==1.4.2\n",
      "papermill @ file:///home/conda/feedstock_root/build_artifacts/papermill_1604950649566/work\n",
      "parso @ file:///home/conda/feedstock_root/build_artifacts/parso_1607618318316/work\n",
      "pathspec @ file:///home/conda/feedstock_root/build_artifacts/pathspec_1605120834673/work\n",
      "patsy==0.5.1\n",
      "pbr==5.5.1\n",
      "pep517==0.10.0\n",
      "pexpect @ file:///home/conda/feedstock_root/build_artifacts/pexpect_1602535608087/work\n",
      "phik @ file:///home/conda/feedstock_root/build_artifacts/phik_1590331950347/work\n",
      "pickleshare @ file:///home/conda/feedstock_root/build_artifacts/pickleshare_1602536217715/work\n",
      "Pillow @ file:///home/conda/feedstock_root/build_artifacts/pillow_1604748693076/work\n",
      "pluggy==0.13.1\n",
      "poyo==0.5.0\n",
      "prettytable @ file:///home/conda/feedstock_root/build_artifacts/prettytable_1605348622459/work\n",
      "prometheus-client @ file:///home/conda/feedstock_root/build_artifacts/prometheus_client_1605543085815/work\n",
      "promise==2.3\n",
      "prompt-toolkit @ file:///home/conda/feedstock_root/build_artifacts/prompt-toolkit_1605053337398/work\n",
      "proto-plus==1.18.1\n",
      "protobuf==3.14.0\n",
      "psutil==5.8.0\n",
      "ptyprocess==0.6.0\n",
      "py==1.10.0\n",
      "pyarrow==2.0.0\n",
      "pyasn1==0.4.8\n",
      "pyasn1-modules==0.2.8\n",
      "pycosat @ file:///home/conda/feedstock_root/build_artifacts/pycosat_1602277852533/work\n",
      "pycparser @ file:///tmp/build/80754af9/pycparser_1594388511720/work\n",
      "pydot==1.4.1\n",
      "pyerfa @ file:///home/conda/feedstock_root/build_artifacts/pyerfa_1606600955735/work\n",
      "Pygments @ file:///home/conda/feedstock_root/build_artifacts/pygments_1607352093725/work\n",
      "PyJWT==1.7.1\n",
      "pymongo==3.11.2\n",
      "pyOpenSSL @ file:///home/conda/feedstock_root/build_artifacts/pyopenssl_1608055815057/work\n",
      "pyparsing==2.4.7\n",
      "pyrsistent @ file:///home/conda/feedstock_root/build_artifacts/pyrsistent_1605115584602/work\n",
      "PySocks @ file:///home/conda/feedstock_root/build_artifacts/pysocks_1602326924965/work\n",
      "pytest==6.2.4\n",
      "python-dateutil==2.8.1\n",
      "python-helpers==0.3.0\n",
      "python-slugify @ file:///home/conda/feedstock_root/build_artifacts/python-slugify_1593573453419/work\n",
      "pytz @ file:///home/conda/feedstock_root/build_artifacts/pytz_1608904108784/work\n",
      "PyWavelets @ file:///home/conda/feedstock_root/build_artifacts/pywavelets_1607290802651/work\n",
      "PyYAML==5.3.1\n",
      "pyzmq==20.0.0\n",
      "qtconsole==5.0.1\n",
      "QtPy==1.9.0\n",
      "regex @ file:///home/conda/feedstock_root/build_artifacts/regex_1608556687710/work\n",
      "requests @ file:///home/conda/feedstock_root/build_artifacts/requests_1608156231189/work\n",
      "requests-oauthlib @ file:///home/conda/feedstock_root/build_artifacts/requests-oauthlib_1595492159598/work\n",
      "requests-toolbelt==0.9.1\n",
      "retrying==1.3.3\n",
      "rsa @ file:///home/conda/feedstock_root/build_artifacts/rsa_1591996208734/work\n",
      "ruamel-yaml-conda @ file:///home/conda/feedstock_root/build_artifacts/ruamel_yaml_1602430330381/work\n",
      "scikit-learn==0.22\n",
      "scipy==1.4.1\n",
      "seaborn @ file:///home/conda/feedstock_root/build_artifacts/seaborn-base_1608544589436/work\n",
      "Send2Trash==1.5.0\n",
      "simplejson @ file:///home/conda/feedstock_root/build_artifacts/simplejson_1607639732453/work\n",
      "six @ file:///home/conda/feedstock_root/build_artifacts/six_1590081179328/work\n",
      "smmap @ file:///home/conda/feedstock_root/build_artifacts/smmap_1588651577140/work\n",
      "SQLAlchemy @ file:///home/conda/feedstock_root/build_artifacts/sqlalchemy_1608366799098/work\n",
      "sqlparse @ file:///home/conda/feedstock_root/build_artifacts/sqlparse_1602142927465/work\n",
      "statsmodels @ file:///home/conda/feedstock_root/build_artifacts/statsmodels_1604226971477/work\n",
      "strip-hints==0.1.9\n",
      "tabulate==0.8.7\n",
      "tangled-up-in-unicode @ file:///home/conda/feedstock_root/build_artifacts/tangled-up-in-unicode_1589363771888/work\n",
      "tenacity @ file:///home/conda/feedstock_root/build_artifacts/tenacity_1608153789284/work\n",
      "tensorboard==2.1.1\n",
      "tensorflow==2.1.0\n",
      "tensorflow-cloud==0.1.7\n",
      "tensorflow-data-validation==0.21.5\n",
      "tensorflow-datasets==2.0.0\n",
      "tensorflow-estimator==2.1.0\n",
      "tensorflow-hub==0.7.0\n",
      "tensorflow-io==0.11.0\n",
      "tensorflow-metadata==0.21.2\n",
      "tensorflow-model-analysis==0.21.6\n",
      "tensorflow-probability==0.9.0\n",
      "tensorflow-serving-api==2.1.0\n",
      "tensorflow-transform==0.21.2\n",
      "termcolor==1.1.0\n",
      "terminado @ file:///home/conda/feedstock_root/build_artifacts/terminado_1605116895887/work\n",
      "terminaltables==3.1.0\n",
      "testpath==0.4.4\n",
      "text-unidecode==1.3\n",
      "textwrap3==0.9.2\n",
      "tfx==0.21.4\n",
      "tfx-bsl==0.21.4\n",
      "threadpoolctl @ file:///tmp/tmp79xdzxkt/threadpoolctl-2.1.0-py3-none-any.whl\n",
      "toml @ file:///home/conda/feedstock_root/build_artifacts/toml_1604308577558/work\n",
      "tornado @ file:///home/conda/feedstock_root/build_artifacts/tornado_1604105014127/work\n",
      "tqdm @ file:///home/conda/feedstock_root/build_artifacts/tqdm_1608900042843/work\n",
      "traitlets @ file:///home/conda/feedstock_root/build_artifacts/traitlets_1602771532708/work\n",
      "typed-ast @ file:///home/conda/feedstock_root/build_artifacts/typed-ast_1608563088362/work\n",
      "typing-extensions @ file:///home/conda/feedstock_root/build_artifacts/typing_extensions_1602702424206/work\n",
      "Unidecode @ file:///home/conda/feedstock_root/build_artifacts/unidecode_1608471621084/work\n",
      "uritemplate==3.0.1\n",
      "urllib3 @ file:///home/conda/feedstock_root/build_artifacts/urllib3_1605225765842/work\n",
      "visions @ file:///home/conda/feedstock_root/build_artifacts/visions_1603150922716/work\n",
      "wcwidth @ file:///home/conda/feedstock_root/build_artifacts/wcwidth_1600965781394/work\n",
      "webencodings==0.5.1\n",
      "websocket-client @ file:///home/conda/feedstock_root/build_artifacts/websocket-client_1607981760650/work\n",
      "Werkzeug==1.0.1\n",
      "whichcraft==0.6.1\n",
      "widgetsnbextension @ file:///home/conda/feedstock_root/build_artifacts/widgetsnbextension_1605475530234/work\n",
      "witwidget==1.7.0\n",
      "wrapt @ file:///home/conda/feedstock_root/build_artifacts/wrapt_1606141202967/work\n",
      "xgboost==1.3.3\n",
      "yarl @ file:///home/conda/feedstock_root/build_artifacts/yarl_1607968711036/work\n",
      "zipp @ file:///home/conda/feedstock_root/build_artifacts/zipp_1603668650351/work\n"
     ]
    }
   ],
   "source": [
    "# !python3 -m pip freeze"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "environment": {
   "name": "tf2-gpu.2-1.m61",
   "type": "gcloud",
   "uri": "gcr.io/deeplearning-platform-release/tf2-gpu.2-1:m61"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
