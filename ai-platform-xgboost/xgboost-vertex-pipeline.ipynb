{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ============================================================================== \\\n",
    " Copyright 2020 Google LLC. This software is provided as-is, without warranty \\\n",
    " or representation for any use or purpose. Your use of it is subject to your \\\n",
    " agreement with Google. \\\n",
    " ============================================================================== \n",
    " \n",
    " Author: Elvin Zhu, Chanchal Chatterjee \\\n",
    " Email: elvinzhu@google.com \\\n",
    "<img src=\"img/google-cloud-icon.jpg\" alt=\"Drawing\" style=\"width: 200px;\"/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !python3 -m pip install kfp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import kfp\n",
    "import yaml\n",
    "import kfp.components as comp\n",
    "import kfp.dsl as dsl\n",
    "from typing import NamedTuple\n",
    "# from kfp.compiler import compiler\n",
    "from kfp.v2 import compiler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_preprocess(\n",
    "    bucket_name: str,\n",
    "    input_file: str,\n",
    "    target_column: str,\n",
    "    ) -> NamedTuple('PreprocessOutput', \n",
    "              [\n",
    "                  ('x_train_name', str),\n",
    "                  ('x_test_name', str),\n",
    "                  ('y_train_name', str),\n",
    "                  ('y_test_name', str),\n",
    "                  ('n_classes', int),\n",
    "              ]):\n",
    "    \n",
    "    from collections import namedtuple\n",
    "    from sklearn.model_selection import train_test_split\n",
    "    import pandas as pd\n",
    "    import os\n",
    "    import logging\n",
    "\n",
    "    logging.info(\"Loading {}\".format(input_file))\n",
    "    dataset = pd.read_csv(input_file)\n",
    "    # drop unique id column which is not useful for ML\n",
    "    dataset.drop(['LOAN_SEQUENCE_NUMBER'], axis=1, inplace=True)\n",
    "\n",
    "    # Convert categorical columns into one-hot encodings\n",
    "    str_cols = [col for col in dataset.columns if dataset[col].dtype == 'object']\n",
    "    dataset = pd.get_dummies(dataset, columns=str_cols)\n",
    "    n_classes = dataset[target_column].nunique()\n",
    "    logging.info(\"No. of Classes: {}\".format(n_classes))\n",
    "\n",
    "    # Split with a small test size so as to allow our model to train on more data\n",
    "    x_train, x_test, y_train, y_test = train_test_split(\n",
    "        dataset.drop(target_column, axis=1), \n",
    "        dataset[target_column], \n",
    "        test_size=0.1,\n",
    "        random_state=1,\n",
    "        shuffle=True, \n",
    "        stratify=dataset[target_column], \n",
    "        )\n",
    "\n",
    "    logging.info(\"x_train shape = {}\".format(x_train.shape))\n",
    "    logging.info(\"x_test shape = {}\".format(x_test.shape))\n",
    "    logging.info(\"y_train shape = {}\".format(y_train.shape))\n",
    "    logging.info(\"y_test shape = {}\".format(y_test.shape))\n",
    "\n",
    "    base_file_name = os.path.basename(input_file)\n",
    "    base_name, ext_name = os.path.splitext(base_file_name)\n",
    "    x_train_name = \"{}_x_train{}\".format(base_name, ext_name)\n",
    "    x_test_name = \"{}_x_test{}\".format(base_name, ext_name)\n",
    "    y_train_name = \"{}_y_train{}\".format(base_name, ext_name)\n",
    "    y_test_name = \"{}_y_test{}\".format(base_name, ext_name)\n",
    "    \n",
    "    x_train_name = os.path.join(\"gs://\", bucket_name, \"data_split_xgb\", x_train_name)\n",
    "    x_test_name = os.path.join(\"gs://\", bucket_name, \"data_split_xgb\", x_test_name)\n",
    "    y_train_name = os.path.join(\"gs://\", bucket_name, \"data_split_xgb\", y_train_name)\n",
    "    y_test_name = os.path.join(\"gs://\", bucket_name, \"data_split_xgb\", y_test_name)\n",
    "    \n",
    "    x_train.to_csv(x_train_name, index=False)\n",
    "    x_test.to_csv(x_test_name, index=False)\n",
    "    y_train.to_csv(y_train_name, index=False)\n",
    "    y_test.to_csv(y_test_name, index=False)\n",
    "\n",
    "    logging.info(\"x_train saved to {}\".format(x_train_name))\n",
    "    logging.info(\"x_test saved to {}\".format(x_test_name))\n",
    "    logging.info(\"y_train saved to {}\".format(y_train_name))\n",
    "    logging.info(\"y_test saved to {}\".format(y_test_name))\n",
    "    logging.info(\"finished\")\n",
    "    \n",
    "    PreprocessOutput = namedtuple('PreprocessOutput', \n",
    "        ['x_train_name', 'x_test_name', 'y_train_name', 'y_test_name', 'n_classes'])\n",
    "    return PreprocessOutput(\n",
    "        x_train_name=x_train_name,\n",
    "        x_test_name=x_test_name,\n",
    "        y_train_name=y_train_name,\n",
    "        y_test_name=y_test_name,\n",
    "        n_classes=n_classes,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def hypertune(\n",
    "        project_id: str,\n",
    "        region: str,\n",
    "        job_name: str,\n",
    "        bucket_name: str,\n",
    "        job_folder_name: str,\n",
    "        train_feature_path: str,\n",
    "        train_label_path: str,\n",
    "        val_feature_path: str,\n",
    "        val_label_path: str,\n",
    "        n_classes: int,\n",
    "        metric_id: str,\n",
    "        max_trial_count: int,\n",
    "        parallel_trial_count: int,\n",
    "        package_uri: str,\n",
    "        executor_image_uri: str = 'us-docker.pkg.dev/vertex-ai/training/tf-cpu.2-2:latest',\n",
    "        python_module: str = \"trainer.train\",\n",
    "        api_endpoint: str = \"us-central1-aiplatform.googleapis.com\",\n",
    "        machine_type: str = \"n1-standard-4\",\n",
    "    ) -> NamedTuple('TrainOutput', \n",
    "              [('response', str), ('job_name', str)]):\n",
    "    from collections import namedtuple\n",
    "    import subprocess\n",
    "    import logging\n",
    "\n",
    "    job_name = job_name + \"_hpt\"\n",
    "    job_dir = 'gs://{}/{}/{}'.format(\n",
    "        bucket_name,\n",
    "        job_folder_name,\n",
    "        job_name,\n",
    "        )\n",
    "\n",
    "    # The AI Platform services require regional API endpoints.\n",
    "    client_options = {\"api_endpoint\": api_endpoint}\n",
    "    # Initialize client that will be used to create and send requests.\n",
    "    # This client only needs to be created once, and can be reused for multiple requests.\n",
    "    client = aiplatform.gapic.JobServiceClient(client_options=client_options)\n",
    "\n",
    "    # study_spec\n",
    "    metric = {\n",
    "        \"metric_id\": metric_id,\n",
    "        \"goal\": aiplatform.gapic.StudySpec.MetricSpec.GoalType.MAXIMIZE,\n",
    "    }\n",
    "\n",
    "    max_depth = {\n",
    "            \"parameter_id\": \"max_depth\",\n",
    "            \"integer_value_spec\": {\"min_value\": 2, \"max_value\": 20},\n",
    "            \"scale_type\": aiplatform.gapic.StudySpec.ParameterSpec.ScaleType.UNIT_LINEAR_SCALE,\n",
    "    }\n",
    "    n_estimators = {\n",
    "            \"parameter_id\": \"n_estimators\",\n",
    "            \"integer_value_spec\": {\"min_value\": 10, \"max_value\": 200},\n",
    "            \"scale_type\": aiplatform.gapic.StudySpec.ParameterSpec.ScaleType.UNIT_LINEAR_SCALE,\n",
    "    }\n",
    "    booster = {\n",
    "        \"parameter_id\": \"booster\",\n",
    "        \"categorical_value_spec\": {\"values\": [\"gbtree\",\"gblinear\",\"dart\"]},\n",
    "    }\n",
    "\n",
    "    # trial_job_spec\n",
    "    machine_spec = {\n",
    "        \"machine_type\": machine_type,\n",
    "    }\n",
    "    worker_pool_spec = {\n",
    "        \"machine_spec\": machine_spec,\n",
    "        \"replica_count\": 1,\n",
    "        \"python_package_spec\": {\n",
    "            \"executor_image_uri\": executor_image_uri,\n",
    "            \"package_uris\": [package_uri],\n",
    "            \"python_module\": python_module,\n",
    "            \"args\": [\n",
    "                '--job-dir',\n",
    "                job_dir,\n",
    "                '--train_feature_name',\n",
    "                train_feature_path,\n",
    "                '--train_label_name',\n",
    "                train_label_path,\n",
    "                '--val_feature_name',\n",
    "                val_feature_path,\n",
    "                '--val_label_name',\n",
    "                val_label_path,\n",
    "                '--no_classes',\n",
    "                n_classes,\n",
    "            ],\n",
    "        },\n",
    "    }\n",
    "\n",
    "    # hyperparameter_tuning_job\n",
    "    hyperparameter_tuning_job = {\n",
    "        \"display_name\": job_name,\n",
    "        \"max_trial_count\": max_trial_count,\n",
    "        \"parallel_trial_count\": parallel_trial_count,\n",
    "        \"study_spec\": {\n",
    "            \"metrics\": [metric],\n",
    "            \"parameters\": [max_depth, n_estimators, booster],\n",
    "        },\n",
    "        \"trial_job_spec\": {\"worker_pool_specs\": [worker_pool_spec]},\n",
    "    }\n",
    "    parent = f\"projects/{project_id}/locations/{region}\"\n",
    "    response = client.create_hyperparameter_tuning_job(\n",
    "        parent=parent, hyperparameter_tuning_job=hyperparameter_tuning_job\n",
    "    )\n",
    "    logging.info(f\"response: {response}\")\n",
    "    hpt_job_name = response.name.split('/')[-1]\n",
    "        \n",
    "    TrainOutput = namedtuple('TrainOutput',['response', 'job_name'])\n",
    "    return TrainOutput(response=response, job_name=hpt_job_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_hpt_job_status(\n",
    "        project_id: str,\n",
    "        region: str,\n",
    "        hpt_job_name: str,\n",
    "        api_endpoint: str = \"us-central1-aiplatform.googleapis.com\",\n",
    "        time_out: int = 9000, # timeout after 2.5 hours by default\n",
    "        time_sleep: int = 60, # check status every minute by default\n",
    "    ) -> NamedTuple('Ghp_Output', \n",
    "              [('booster', str), ('max_depth', int), ('n_estimators', int)]):\n",
    "    \n",
    "    from collections import namedtuple\n",
    "    from google.cloud import aiplatform\n",
    "\n",
    "    import time\n",
    "    import logging\n",
    "\n",
    "    time0 = time.time()\n",
    "    status = False\n",
    "    \n",
    "    while time.time() - time0 < time_out:    \n",
    "        client_options = {\"api_endpoint\": api_endpoint}\n",
    "        client = aiplatform.gapic.JobServiceClient(client_options=client_options)\n",
    "        name = client.hyperparameter_tuning_job_path(\n",
    "            project=project_id,\n",
    "            location=region,\n",
    "            hyperparameter_tuning_job=hpt_job_name,\n",
    "        )\n",
    "        response = client.get_hyperparameter_tuning_job(name=name)\n",
    "        logging.info(f\"response: {response}\")\n",
    "        \n",
    "        if 'state' in response and \"JobState.JOB_STATE_SUCCEEDED\" == str(response.state):\n",
    "            status = True\n",
    "            break\n",
    "        else:\n",
    "            logging.info(\"Checking status ...\")\n",
    "            logging.info(response)\n",
    "            time.sleep(time_sleep)\n",
    "            \n",
    "    if not status:\n",
    "        raise TimeoutError(\"No successful job found. Timeout after {} seconds\".format(time_out))\n",
    "\n",
    "    max_ind = 0\n",
    "    max_val = 0\n",
    "    for ind, trials in enumerate(response.trials):\n",
    "        value = trials.final_measurement.metrics[0].value\n",
    "        logging.info(f\"Metrics Value (larger is better): {value}\")\n",
    "        if value > max_val:\n",
    "            max_val = value\n",
    "            max_ind = ind\n",
    "\n",
    "    param_dict = {}\n",
    "    for params in response.trials[max_ind].parameters:\n",
    "        param_dict[params.parameter_id] = params.value\n",
    "\n",
    "    booster=param_dict['booster']\n",
    "    max_depth=str(int(param_dict['max_depth']))\n",
    "    n_estimators=str(int(param_dict['n_estimators']))\n",
    "\n",
    "    logging.info(f\"booster {booster}\")\n",
    "    logging.info(f\"max_depth {max_depth}\")\n",
    "    logging.info(f\"n_estimators {n_estimators}\")\n",
    "        \n",
    "    Ghp_Output = namedtuple('Ghp_Output',['booster', 'max_depth', 'n_estimators'])\n",
    "    return Ghp_Output(booster=booster, max_depth=max_depth, n_estimators=n_estimators )        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(\n",
    "        project_id: str,\n",
    "        region: str,\n",
    "        job_name: str,\n",
    "        bucket_name: str,\n",
    "        job_folder_name: str,\n",
    "        train_feature_path: str,\n",
    "        train_label_path: str,\n",
    "        n_classes: int,\n",
    "        n_estimators: int,\n",
    "        max_depth: int,\n",
    "        booster: str,\n",
    "        package_uri: str,\n",
    "        executor_image_uri: str = 'us-docker.pkg.dev/vertex-ai/training/tf-cpu.2-2:latest',\n",
    "        python_module: str = \"trainer.train\",\n",
    "        api_endpoint: str = \"us-central1-aiplatform.googleapis.com\",\n",
    "        machine_type: str = \"n1-standard-4\",\n",
    "    ) -> NamedTuple('TrainOutput', \n",
    "              [('response', str), ('job_name', str)]):\n",
    "    \n",
    "    from collections import namedtuple\n",
    "    from google.cloud import aiplatform\n",
    "    import logging\n",
    "\n",
    "    job_dir = 'gs://{}/{}/{}'.format(\n",
    "        bucket_name,\n",
    "        job_folder_name,\n",
    "        job_name,\n",
    "        )\n",
    "    \n",
    "    # The AI Platform services require regional API endpoints.\n",
    "    client_options = {\"api_endpoint\": api_endpoint}\n",
    "    # Initialize client that will be used to create and send requests.\n",
    "    # This client only needs to be created once, and can be reused for multiple requests.\n",
    "    client = aiplatform.gapic.JobServiceClient(client_options=client_options)\n",
    "    custom_job = {\n",
    "        \"display_name\": job_name,\n",
    "        \"job_spec\": {\n",
    "            \"worker_pool_specs\": [\n",
    "                {\n",
    "                    \"machine_spec\": {\n",
    "                        \"machine_type\": machine_type,\n",
    "                    },\n",
    "                    \"replica_count\": 1,\n",
    "                    \"python_package_spec\": {\n",
    "                        \"executor_image_uri\": executor_image_uri,\n",
    "                        \"package_uris\": [package_uri],\n",
    "                        \"python_module\": python_module,\n",
    "                        \"args\": [\n",
    "                          '--job-dir',\n",
    "                          job_dir,\n",
    "                          '--train_feature_name',\n",
    "                          train_feature_path,\n",
    "                          '--train_label_name',\n",
    "                          train_label_path,\n",
    "                          '--no_classes',\n",
    "                          n_classes,\n",
    "                          '--n_estimators',\n",
    "                          n_estimators,\n",
    "                          '--max_depth',\n",
    "                          max_depth,\n",
    "                          '--booster',\n",
    "                          booster\n",
    "                        ],\n",
    "                    },\n",
    "                }\n",
    "            ]\n",
    "        },\n",
    "    }\n",
    "    parent = f\"projects/{project_id}/locations/{regino}\"\n",
    "    response = client.create_custom_job(parent=parent, custom_job=custom_job)\n",
    "    logging.info(f\"response: {response}\")\n",
    "    training_job_id = response.name.split('/')[-1]\n",
    "    \n",
    "    TrainOutput = namedtuple('TrainOutput',['response', 'job_name'])\n",
    "    return TrainOutput(response=response, job_name=training_job_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_job_status(\n",
    "        project_id: str,\n",
    "        region: str,\n",
    "        job_name: str,\n",
    "        api_endpoint: str = \"us-central1-aiplatform.googleapis.com\",\n",
    "        time_out: int = 9000, # timeout after 2.5 hours by default\n",
    "        time_sleep: int = 60, # check status every minute by default\n",
    "    ) -> NamedTuple('Gct_Output', \n",
    "              [('response', str), ('status', bool)]):\n",
    "    \n",
    "    from collections import namedtuple\n",
    "    from google.cloud import aiplatform\n",
    "\n",
    "    import time\n",
    "    import logging\n",
    "\n",
    "    time0 = time.time()\n",
    "    status = False\n",
    "    \n",
    "    while time.time() - time0 < time_out:    \n",
    "        client_options = {\"api_endpoint\": api_endpoint}\n",
    "        client = aiplatform.gapic.JobServiceClient(client_options=client_options)\n",
    "        name = client.custom_job_path(\n",
    "            project=project_id,\n",
    "            location=region,\n",
    "            custom_job=job_name,\n",
    "        )\n",
    "        response = client.get_custom_job(name=name)\n",
    "        logging.info(f\"response: {response}\")\n",
    "        \n",
    "        if 'state' in response and \"JobState.JOB_STATE_SUCCEEDED\" == str(response.state):\n",
    "            status = True\n",
    "            break\n",
    "        else:\n",
    "            logging.info(\"Checking status ...\")\n",
    "            logging.info(response)\n",
    "            time.sleep(time_sleep)\n",
    "            \n",
    "    if not status:\n",
    "        raise TimeoutError(\"No successful job found. Timeout after {} seconds\".format(time_out))\n",
    "       \n",
    "    Gct_Output = namedtuple('Gct_Output',['response', 'status'])\n",
    "    return Gct_Output(response=response, status=status)        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def deploy(\n",
    "#     bucket_name: str,\n",
    "#     job_folder_name: str,\n",
    "#     job_name: str,\n",
    "#     model_name: str,\n",
    "#     model_version: str,\n",
    "#     region:str,\n",
    "#     model_framework:str,\n",
    "#     model_description: str,\n",
    "#     status: bool,\n",
    "#     ):\n",
    "#     from collections import namedtuple\n",
    "#     import subprocess\n",
    "#     import logging\n",
    "#     import re\n",
    "       \n",
    "#     latest_model_dir = \"gs://{}/{}/{}\".format(bucket_name, job_folder_name, job_name)\n",
    "    \n",
    "#     # Check if model exists:\n",
    "#     response = subprocess.run([\n",
    "#             \"gcloud\", \"ai-platform\", \"models\", \"list\",\n",
    "#             \"--region\", \"global\",\n",
    "#         ], stdout=subprocess.PIPE)\n",
    "#     response = response.stdout.decode().split(\"\\n\")[1:]\n",
    "#     list_of_models = [re.sub(\" +\", \" \", x).split(\" \")[0] for x in response]\n",
    "\n",
    "#     # create model if not exists\n",
    "#     if not model_name in list_of_models:\n",
    "#         # create model\n",
    "#         response = subprocess.run([\n",
    "#             \"gcloud\", \"ai-platform\", \"models\", \"create\",\n",
    "#             model_name,\n",
    "#             \"--region\", region,\n",
    "#             \"--enable-logging\",\n",
    "#         ], stdout=subprocess.PIPE)\n",
    "    \n",
    "#     # create model version\n",
    "#     response = subprocess.run([\n",
    "#         \"gcloud\",\"beta\", \"ai-platform\", \"versions\", \"create\",\n",
    "#         model_version,\n",
    "#         \"--model\", model_name,\n",
    "#         \"--origin\", latest_model_dir,\n",
    "#         \"--region\", \"global\",\n",
    "#         \"--python-version\", \"3.7\",\n",
    "#         \"--runtime-version\", \"2.2\",\n",
    "#         \"--framework\", model_framework,\n",
    "#         \"--description\", model_description,\n",
    "#     ], stdout=subprocess.PIPE)\n",
    "    \n",
    "#     DeployOutput = namedtuple('DeployOutput',['response'])\n",
    "        \n",
    "#     return DeployOutput(response=response.stdout.decode())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compile python functions to components"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "component_dir = \"./components\"\n",
    "base_image = \"us-docker.pkg.dev/vertex-ai/training/tf-cpu.2-2:latest\"\n",
    "\n",
    "yaml_name = '{}/preprocess.yaml'.format(component_dir)\n",
    "preprocess_op = comp.func_to_container_op(\n",
    "    data_preprocess, \n",
    "    output_component_file=yaml_name,\n",
    "    base_image=base_image)\n",
    "\n",
    "yaml_name = '{}/train_hpt.yaml'.format(component_dir)\n",
    "hypertune_op = comp.func_to_container_op(\n",
    "    hypertune, \n",
    "    output_component_file=yaml_name,\n",
    "    base_image=base_image)\n",
    "\n",
    "yaml_name = '{}/ghp.yaml'.format(component_dir) # Get hypertune\n",
    "ghp_op = comp.func_to_container_op(\n",
    "    get_hpt_job_status, \n",
    "    output_component_file=yaml_name,\n",
    "    base_image=base_image)\n",
    "\n",
    "yaml_name = '{}/train.yaml'.format(component_dir)\n",
    "train_op = comp.func_to_container_op(\n",
    "    train, \n",
    "    output_component_file=yaml_name,\n",
    "    base_image=base_image)\n",
    "\n",
    "yaml_name = '{}/gct.yaml'.format(component_dir) # Get custom train\n",
    "gct_op = comp.func_to_container_op(\n",
    "    get_job_status, \n",
    "    output_component_file=yaml_name,\n",
    "    base_image=base_image)\n",
    "\n",
    "# yaml_name = '{}/deploy.yaml'.format(component_dir)\n",
    "# deploy_op = comp.func_to_container_op(\n",
    "#     deploy, \n",
    "#     output_component_file=yaml_name,\n",
    "#     base_image=base_image)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compile KFP pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "@dsl.pipeline(\n",
    "   name='vertex-training-pipeline',\n",
    "   description='A example of vertex training pipeline with custom xgboost model.'\n",
    ")\n",
    "def train_pipeline(\n",
    "    job_name: str,\n",
    "    project_id: str,\n",
    "    region: str,\n",
    "    user_name: str,\n",
    "    bucket_name: str,\n",
    "    input_file: str,\n",
    "    job_folder_name: str,\n",
    "    target_column: str,\n",
    "    package_uri: str,\n",
    "    metric_id: str,\n",
    "    max_trial_count: int,\n",
    "    parallel_trial_count: int,\n",
    "#     deployed_model_name: str,\n",
    "#     deployed_model_version: str,\n",
    "#     deployed_model_description: str,\n",
    "\n",
    "#     config_yaml_hpt: str,\n",
    "#     config_yaml: str,\n",
    "\n",
    "    ):\n",
    "    preprocess_task = preprocess_op(\n",
    "        bucket_name = bucket_name,\n",
    "        input_file = input_file,\n",
    "        target_column = target_column,\n",
    "    )\n",
    "    \n",
    "    hpt_task = hypertune_op(\n",
    "        project_id = project_id,\n",
    "        region = region,\n",
    "        job_name = job_name,\n",
    "        bucket_name = bucket_name,\n",
    "        job_folder_name = job_folder_name,\n",
    "        train_feature_path = preprocess_task.outputs['x_train_name'],\n",
    "        train_label_path   = preprocess_task.outputs['y_train_name'],\n",
    "        val_feature_path   = preprocess_task.outputs['x_test_name'],\n",
    "        val_label_path     = preprocess_task.outputs['y_test_name'],\n",
    "        n_classes = preprocess_task.outputs['n_classes'],\n",
    "        metric_id = metric_id,\n",
    "        max_trial_count = max_trial_count,\n",
    "        parallel_trial_count = parallel_trial_count,\n",
    "        package_uri = package_uri,\n",
    "    )\n",
    "    \n",
    "    ghp_task = ghp_op(\n",
    "        project_id = project_id,\n",
    "        region = region,\n",
    "        hpt_job_name = hpt_task.outputs['job_name'],\n",
    "    )\n",
    "    \n",
    "    train_task = train_op(\n",
    "        project_id = project_id,\n",
    "        region = region, \n",
    "        job_name = job_name,\n",
    "        bucket_name = bucket_name,\n",
    "        job_folder_name = job_folder_name,\n",
    "        train_feature_path = preprocess_task.outputs['x_train_name'],\n",
    "        train_label_path   = preprocess_task.outputs['y_train_name'],\n",
    "        n_classes = preprocess_task.outputs['n_classes'],\n",
    "        n_estimators = ghp_task.outputs['n_estimators'],\n",
    "        max_depth = ghp_task.outputs['max_depth'],\n",
    "        booster = ghp_task.outputs['booster'],\n",
    "        package_uri = package_uri,\n",
    "    )\n",
    "    \n",
    "    gct_task = gct_op(\n",
    "        project_id = project_id,\n",
    "        region = region,\n",
    "        job_name = train_task.outputs['job_name']\n",
    "    )\n",
    "    \n",
    "#     deploy_task = deploy_op(\n",
    "#         status = lro_task_2.outputs['status'],\n",
    "#         bucket_name = bucket_name,\n",
    "#         job_folder_name = job_folder_name,\n",
    "#         job_name = train_task.outputs['job_name'],\n",
    "#         region = 'global',\n",
    "#         model_framework = 'XGBOOST',\n",
    "#         model_name = deployed_model_name,\n",
    "#         model_version = deployed_model_version,\n",
    "#         model_description = deployed_model_description,\n",
    "#     )\n",
    "    \n",
    "# pipeline_pkg_path=\"./train_pipeline.json\"\n",
    "# compiler.Compiler().compile(train_pipeline, package_path=pipeline_pkg_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline_pkg_path=\"./train_pipeline.json\"\n",
    "pipeline_root = \"gs://tuti_job/pipeline_root\"\n",
    "compiler.Compiler().compile(\n",
    "    pipeline_func=train_pipeline, \n",
    "    pipeline_root=pipeline_root,\n",
    "    output_path=pipeline_pkg_path,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run KFP pipeline on AI Platform hosted Kubernetes cluster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'root'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-43-49787c67df21>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     33\u001b[0m     \u001b[0mjob_spec_path\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpipeline_pkg_path\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     34\u001b[0m     \u001b[0mpipeline_root\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpipeline_root\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 35\u001b[0;31m     \u001b[0mparameter_values\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpipeline_params\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     36\u001b[0m )\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/kfp/v2/google/client/client.py\u001b[0m in \u001b[0;36mcreate_run_from_job_spec\u001b[0;34m(self, job_spec_path, job_id, pipeline_root, parameter_values, enable_caching, cmek, service_account, network, labels)\u001b[0m\n\u001b[1;32m    311\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    312\u001b[0m     builder = runtime_config_builder.RuntimeConfigBuilder.from_job_spec_json(\n\u001b[0;32m--> 313\u001b[0;31m         job_spec)\n\u001b[0m\u001b[1;32m    314\u001b[0m     \u001b[0mbuilder\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate_pipeline_root\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpipeline_root\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    315\u001b[0m     \u001b[0mbuilder\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate_runtime_parameters\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparameter_values\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/kfp/v2/google/client/runtime_config_builder.py\u001b[0m in \u001b[0;36mfrom_job_spec_json\u001b[0;34m(cls, job_spec)\u001b[0m\n\u001b[1;32m     54\u001b[0m     \u001b[0mruntime_config_spec\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mjob_spec\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'runtimeConfig'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     55\u001b[0m     \u001b[0mparameter_types\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 56\u001b[0;31m     parameter_input_definitions = job_spec['pipelineSpec']['root'].get(\n\u001b[0m\u001b[1;32m     57\u001b[0m         'inputDefinitions', {}).get('parameters', {})\n\u001b[1;32m     58\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mk\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mv\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mparameter_input_definitions\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyError\u001b[0m: 'root'"
     ]
    }
   ],
   "source": [
    "## ============== Uncomment to run the pipeline ==============\n",
    "from kfp.v2.google.client import AIPlatformClient\n",
    "from datetime import datetime\n",
    "from pytz import timezone\n",
    "\n",
    "PROJECT = 'img-seg-3d'\n",
    "REGION = 'us-central1'\n",
    "my_timezone = 'US/Pacific'\n",
    "        \n",
    "# Define pipeline input\n",
    "pipeline_params = {\n",
    "    \"job_name\": 'xgb_train_elvinzhu_{}'.format(\n",
    "        datetime.now(timezone(my_timezone)).strftime(\"%m%d%y_%H%M\")\n",
    "        ),\n",
    "    \"project_id\": PROJECT,\n",
    "    \"region\": REGION,\n",
    "    \"user_name\": 'elvinzhu',\n",
    "    \"bucket_name\": 'tuti_job',\n",
    "    \"job_folder_name\": 'xgb_train_job',\n",
    "    \"input_file\": 'gs://tuti_asset/datasets/mortgage_structured.csv',\n",
    "    \"target_column\": 'TARGET',\n",
    "    \"package_uri\": \"gs://vapit_job/trainer/xgboost/trainer-0.1.tar.gz\",\n",
    "    \"max_trial_count\": 4,\n",
    "    \"parallel_trial_count\": 1,\n",
    "}\n",
    "\n",
    "# Create pipeline client\n",
    "api_client = AIPlatformClient(\n",
    "    project_id=PROJECT,\n",
    "    region=REGION,\n",
    ")\n",
    "response = api_client.create_run_from_job_spec(\n",
    "    job_spec_path=pipeline_pkg_path,\n",
    "    pipeline_root=pipeline_root,\n",
    "    parameter_values=pipeline_params\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting kfp==1.6.2\n",
      "  Downloading kfp-1.6.2.tar.gz (222 kB)\n",
      "\u001b[K     |████████████████████████████████| 222 kB 9.8 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: PyYAML<6,>=5.3 in /opt/conda/lib/python3.7/site-packages (from kfp==1.6.2) (5.3.1)\n",
      "Requirement already satisfied: google-cloud-storage<2,>=1.20.0 in /opt/conda/lib/python3.7/site-packages (from kfp==1.6.2) (1.32.0)\n",
      "Requirement already satisfied: kubernetes<13,>=8.0.0 in /opt/conda/lib/python3.7/site-packages (from kfp==1.6.2) (11.0.0)\n",
      "Requirement already satisfied: google-api-python-client<2,>=1.7.8 in /home/jupyter/.local/lib/python3.7/site-packages (from kfp==1.6.2) (1.7.11)\n",
      "Requirement already satisfied: google-auth<2,>=1.6.1 in /opt/conda/lib/python3.7/site-packages (from kfp==1.6.2) (1.24.0)\n",
      "Requirement already satisfied: requests-toolbelt<1,>=0.8.0 in /opt/conda/lib/python3.7/site-packages (from kfp==1.6.2) (0.9.1)\n",
      "Requirement already satisfied: cloudpickle<2,>=1.3.0 in /opt/conda/lib/python3.7/site-packages (from kfp==1.6.2) (1.6.0)\n",
      "Requirement already satisfied: kfp-server-api<2.0.0,>=1.1.2 in /opt/conda/lib/python3.7/site-packages (from kfp==1.6.2) (1.3.0)\n",
      "Requirement already satisfied: jsonschema<4,>=3.0.1 in /opt/conda/lib/python3.7/site-packages (from kfp==1.6.2) (3.2.0)\n",
      "Requirement already satisfied: tabulate<1,>=0.8.6 in /opt/conda/lib/python3.7/site-packages (from kfp==1.6.2) (0.8.7)\n",
      "Requirement already satisfied: click<8,>=7.1.1 in /opt/conda/lib/python3.7/site-packages (from kfp==1.6.2) (7.1.2)\n",
      "Requirement already satisfied: Deprecated<2,>=1.2.7 in /opt/conda/lib/python3.7/site-packages (from kfp==1.6.2) (1.2.11)\n",
      "Requirement already satisfied: strip-hints<1,>=0.1.8 in /opt/conda/lib/python3.7/site-packages (from kfp==1.6.2) (0.1.9)\n",
      "Requirement already satisfied: docstring-parser<1,>=0.7.3 in /opt/conda/lib/python3.7/site-packages (from kfp==1.6.2) (0.7.3)\n",
      "Requirement already satisfied: fire<1,>=0.3.1 in /opt/conda/lib/python3.7/site-packages (from kfp==1.6.2) (0.4.0)\n",
      "Requirement already satisfied: protobuf<4,>=3.13.0 in /opt/conda/lib/python3.7/site-packages (from kfp==1.6.2) (3.14.0)\n",
      "Collecting absl-py<=0.11,>=0.9\n",
      "  Downloading absl_py-0.11.0-py3-none-any.whl (127 kB)\n",
      "\u001b[K     |████████████████████████████████| 127 kB 12.0 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: six in /opt/conda/lib/python3.7/site-packages (from absl-py<=0.11,>=0.9->kfp==1.6.2) (1.15.0)\n",
      "Requirement already satisfied: wrapt<2,>=1.10 in /opt/conda/lib/python3.7/site-packages (from Deprecated<2,>=1.2.7->kfp==1.6.2) (1.12.1)\n",
      "Requirement already satisfied: termcolor in /opt/conda/lib/python3.7/site-packages (from fire<1,>=0.3.1->kfp==1.6.2) (1.1.0)\n",
      "Requirement already satisfied: google-auth-httplib2>=0.0.3 in /opt/conda/lib/python3.7/site-packages (from google-api-python-client<2,>=1.7.8->kfp==1.6.2) (0.0.4)\n",
      "Requirement already satisfied: uritemplate<4dev,>=3.0.0 in /opt/conda/lib/python3.7/site-packages (from google-api-python-client<2,>=1.7.8->kfp==1.6.2) (3.0.1)\n",
      "Requirement already satisfied: httplib2<1dev,>=0.9.2 in /opt/conda/lib/python3.7/site-packages (from google-api-python-client<2,>=1.7.8->kfp==1.6.2) (0.18.1)\n",
      "Requirement already satisfied: rsa<5,>=3.1.4 in /opt/conda/lib/python3.7/site-packages (from google-auth<2,>=1.6.1->kfp==1.6.2) (4.6)\n",
      "Requirement already satisfied: cachetools<5.0,>=2.0.0 in /opt/conda/lib/python3.7/site-packages (from google-auth<2,>=1.6.1->kfp==1.6.2) (4.1.1)\n",
      "Requirement already satisfied: pyasn1-modules>=0.2.1 in /opt/conda/lib/python3.7/site-packages (from google-auth<2,>=1.6.1->kfp==1.6.2) (0.2.8)\n",
      "Requirement already satisfied: setuptools>=40.3.0 in /opt/conda/lib/python3.7/site-packages (from google-auth<2,>=1.6.1->kfp==1.6.2) (51.1.0)\n",
      "Requirement already satisfied: google-resumable-media<2.0dev,>=1.0.0 in /opt/conda/lib/python3.7/site-packages (from google-cloud-storage<2,>=1.20.0->kfp==1.6.2) (1.3.0)\n",
      "Requirement already satisfied: google-cloud-core<2.0dev,>=1.4.1 in /opt/conda/lib/python3.7/site-packages (from google-cloud-storage<2,>=1.20.0->kfp==1.6.2) (1.6.0)\n",
      "Requirement already satisfied: requests<3.0.0dev,>=2.18.0 in /opt/conda/lib/python3.7/site-packages (from google-cloud-storage<2,>=1.20.0->kfp==1.6.2) (2.25.1)\n",
      "Requirement already satisfied: google-api-core<2.0.0dev,>=1.21.0 in /opt/conda/lib/python3.7/site-packages (from google-cloud-core<2.0dev,>=1.4.1->google-cloud-storage<2,>=1.20.0->kfp==1.6.2) (1.22.4)\n",
      "Requirement already satisfied: pytz in /opt/conda/lib/python3.7/site-packages (from google-api-core<2.0.0dev,>=1.21.0->google-cloud-core<2.0dev,>=1.4.1->google-cloud-storage<2,>=1.20.0->kfp==1.6.2) (2020.5)\n",
      "Requirement already satisfied: googleapis-common-protos<2.0dev,>=1.6.0 in /opt/conda/lib/python3.7/site-packages (from google-api-core<2.0.0dev,>=1.21.0->google-cloud-core<2.0dev,>=1.4.1->google-cloud-storage<2,>=1.20.0->kfp==1.6.2) (1.52.0)\n",
      "Requirement already satisfied: google-crc32c<2.0dev,>=1.0 in /opt/conda/lib/python3.7/site-packages (from google-resumable-media<2.0dev,>=1.0.0->google-cloud-storage<2,>=1.20.0->kfp==1.6.2) (1.1.0)\n",
      "Requirement already satisfied: cffi>=1.0.0 in /opt/conda/lib/python3.7/site-packages (from google-crc32c<2.0dev,>=1.0->google-resumable-media<2.0dev,>=1.0.0->google-cloud-storage<2,>=1.20.0->kfp==1.6.2) (1.14.4)\n",
      "Requirement already satisfied: pycparser in /opt/conda/lib/python3.7/site-packages (from cffi>=1.0.0->google-crc32c<2.0dev,>=1.0->google-resumable-media<2.0dev,>=1.0.0->google-cloud-storage<2,>=1.20.0->kfp==1.6.2) (2.20)\n",
      "Requirement already satisfied: importlib-metadata in /opt/conda/lib/python3.7/site-packages (from jsonschema<4,>=3.0.1->kfp==1.6.2) (3.3.0)\n",
      "Requirement already satisfied: attrs>=17.4.0 in /opt/conda/lib/python3.7/site-packages (from jsonschema<4,>=3.0.1->kfp==1.6.2) (20.3.0)\n",
      "Requirement already satisfied: pyrsistent>=0.14.0 in /opt/conda/lib/python3.7/site-packages (from jsonschema<4,>=3.0.1->kfp==1.6.2) (0.17.3)\n",
      "Collecting kfp-pipeline-spec<0.2.0,>=0.1.7\n",
      "  Downloading kfp_pipeline_spec-0.1.8-py3-none-any.whl (27 kB)\n",
      "Requirement already satisfied: certifi in /opt/conda/lib/python3.7/site-packages (from kfp-server-api<2.0.0,>=1.1.2->kfp==1.6.2) (2020.12.5)\n",
      "Requirement already satisfied: python-dateutil in /opt/conda/lib/python3.7/site-packages (from kfp-server-api<2.0.0,>=1.1.2->kfp==1.6.2) (2.8.1)\n",
      "Requirement already satisfied: urllib3>=1.15 in /opt/conda/lib/python3.7/site-packages (from kfp-server-api<2.0.0,>=1.1.2->kfp==1.6.2) (1.26.2)\n",
      "Requirement already satisfied: websocket-client!=0.40.0,!=0.41.*,!=0.42.*,>=0.32.0 in /opt/conda/lib/python3.7/site-packages (from kubernetes<13,>=8.0.0->kfp==1.6.2) (0.57.0)\n",
      "Requirement already satisfied: requests-oauthlib in /opt/conda/lib/python3.7/site-packages (from kubernetes<13,>=8.0.0->kfp==1.6.2) (1.3.0)\n",
      "Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /opt/conda/lib/python3.7/site-packages (from pyasn1-modules>=0.2.1->google-auth<2,>=1.6.1->kfp==1.6.2) (0.4.8)\n",
      "Requirement already satisfied: chardet<5,>=3.0.2 in /opt/conda/lib/python3.7/site-packages (from requests<3.0.0dev,>=2.18.0->google-cloud-storage<2,>=1.20.0->kfp==1.6.2) (4.0.0)\n",
      "Requirement already satisfied: idna<3,>=2.5 in /opt/conda/lib/python3.7/site-packages (from requests<3.0.0dev,>=2.18.0->google-cloud-storage<2,>=1.20.0->kfp==1.6.2) (2.10)\n",
      "Requirement already satisfied: wheel in /opt/conda/lib/python3.7/site-packages (from strip-hints<1,>=0.1.8->kfp==1.6.2) (0.36.2)\n",
      "Requirement already satisfied: typing-extensions>=3.6.4 in /opt/conda/lib/python3.7/site-packages (from importlib-metadata->jsonschema<4,>=3.0.1->kfp==1.6.2) (3.7.4.3)\n",
      "Requirement already satisfied: zipp>=0.5 in /opt/conda/lib/python3.7/site-packages (from importlib-metadata->jsonschema<4,>=3.0.1->kfp==1.6.2) (3.4.0)\n",
      "Requirement already satisfied: oauthlib>=3.0.0 in /opt/conda/lib/python3.7/site-packages (from requests-oauthlib->kubernetes<13,>=8.0.0->kfp==1.6.2) (3.1.0)\n",
      "Building wheels for collected packages: kfp\n",
      "  Building wheel for kfp (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for kfp: filename=kfp-1.6.2-py3-none-any.whl size=301630 sha256=2649863f6d907d343affabccc94acb09a86555d0693a6ef71d25201b77396842\n",
      "  Stored in directory: /home/jupyter/.cache/pip/wheels/ff/cf/15/30d12f6521420ad0ca072497ddccf8e86457c6bf0a2f8a18b2\n",
      "Successfully built kfp\n",
      "Installing collected packages: kfp-pipeline-spec, absl-py, kfp\n",
      "  Attempting uninstall: kfp-pipeline-spec\n",
      "    Found existing installation: kfp-pipeline-spec 0.1.6\n",
      "    Uninstalling kfp-pipeline-spec-0.1.6:\n",
      "      Successfully uninstalled kfp-pipeline-spec-0.1.6\n",
      "  Attempting uninstall: absl-py\n",
      "    Found existing installation: absl-py 0.8.1\n",
      "    Uninstalling absl-py-0.8.1:\n",
      "      Successfully uninstalled absl-py-0.8.1\n",
      "  Attempting uninstall: kfp\n",
      "    Found existing installation: kfp 1.4.0\n",
      "    Uninstalling kfp-1.4.0:\n",
      "      Successfully uninstalled kfp-1.4.0\n",
      "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "witwidget 1.7.0 requires oauth2client>=4.1.3, but you have oauth2client 3.0.0 which is incompatible.\n",
      "tfx 0.21.4 requires absl-py<0.9,>=0.1.6, but you have absl-py 0.11.0 which is incompatible.\n",
      "tfx 0.21.4 requires kubernetes<11,>=10.0.1, but you have kubernetes 11.0.0 which is incompatible.\n",
      "tfx 0.21.4 requires pyarrow<0.16,>=0.15, but you have pyarrow 2.0.0 which is incompatible.\n",
      "tfx-bsl 0.21.4 requires absl-py<0.9,>=0.7, but you have absl-py 0.11.0 which is incompatible.\n",
      "tfx-bsl 0.21.4 requires pyarrow<0.16.0,>=0.15.0, but you have pyarrow 2.0.0 which is incompatible.\n",
      "tensorflow-transform 0.21.2 requires absl-py<0.9,>=0.7, but you have absl-py 0.11.0 which is incompatible.\n",
      "tensorflow-model-analysis 0.21.6 requires absl-py<0.9,>=0.7, but you have absl-py 0.11.0 which is incompatible.\n",
      "tensorflow-model-analysis 0.21.6 requires pyarrow<1,>=0.15, but you have pyarrow 2.0.0 which is incompatible.\n",
      "tensorflow-data-validation 0.21.5 requires absl-py<0.9,>=0.7, but you have absl-py 0.11.0 which is incompatible.\n",
      "tensorflow-data-validation 0.21.5 requires joblib<0.15,>=0.12, but you have joblib 1.0.0 which is incompatible.\n",
      "tensorflow-data-validation 0.21.5 requires pandas<1,>=0.24, but you have pandas 1.2.1 which is incompatible.\n",
      "tensorflow-data-validation 0.21.5 requires scikit-learn<0.22,>=0.18, but you have scikit-learn 0.22 which is incompatible.\n",
      "explainable-ai-sdk 1.1.0 requires matplotlib>=3.2.2, but you have matplotlib 3.2.1 which is incompatible.\u001b[0m\n",
      "Successfully installed absl-py-0.11.0 kfp-1.6.2 kfp-pipeline-spec-0.1.8\n"
     ]
    }
   ],
   "source": [
    "!python3 -m pip install kfp==1.6.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "environment": {
   "name": "tf2-gpu.2-1.m61",
   "type": "gcloud",
   "uri": "gcr.io/deeplearning-platform-release/tf2-gpu.2-1:m61"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
