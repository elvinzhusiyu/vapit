{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !python3 -m pip install kfp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import kfp\n",
    "import yaml\n",
    "import kfp.components as comp\n",
    "import kfp.dsl as dsl\n",
    "from typing import NamedTuple\n",
    "from kfp.compiler import compiler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_preprocess(\n",
    "    bucket_name: str,\n",
    "    input_blob_name: str,\n",
    "    target_column: str,\n",
    "    ) -> NamedTuple('PreprocessOutput', \n",
    "              [\n",
    "                  ('x_train_name', str),\n",
    "                  ('x_test_name', str),\n",
    "                  ('y_train_name', str),\n",
    "                  ('y_test_name', str),\n",
    "                  ('n_classes', int),\n",
    "              ]):\n",
    "    \n",
    "    from collections import namedtuple\n",
    "    from sklearn.model_selection import train_test_split\n",
    "    import pandas as pd\n",
    "    import os\n",
    "    import logging\n",
    "\n",
    "    input_file = 'gs://{}/{}'.format(\n",
    "        bucket_name, \n",
    "        input_blob_name, \n",
    "        )\n",
    "    logging.info(\"Loading {}\".format(input_file))\n",
    "    dataset = pd.read_csv(input_file)\n",
    "    # drop unique id column which is not useful for ML\n",
    "    dataset.drop(['LOAN_SEQUENCE_NUMBER'], axis=1, inplace=True)\n",
    "\n",
    "    # Convert categorical columns into one-hot encodings\n",
    "    str_cols = [col for col in dataset.columns if dataset[col].dtype == 'object']\n",
    "    dataset = pd.get_dummies(dataset, columns=str_cols)\n",
    "    n_classes = dataset[target_column].nunique()\n",
    "    logging.info(\"No. of Classes: {}\".format(n_classes))\n",
    "\n",
    "    # Split with a small test size so as to allow our model to train on more data\n",
    "    x_train, x_test, y_train, y_test = train_test_split(\n",
    "        dataset.drop(target_column, axis=1), \n",
    "        dataset[target_column], \n",
    "        test_size=0.1,\n",
    "        random_state=1,\n",
    "        shuffle=True, \n",
    "        stratify=dataset[target_column], \n",
    "        )\n",
    "\n",
    "    logging.info(\"x_train shape = {}\".format(x_train.shape))\n",
    "    logging.info(\"x_test shape = {}\".format(x_test.shape))\n",
    "    logging.info(\"y_train shape = {}\".format(y_train.shape))\n",
    "    logging.info(\"y_test shape = {}\".format(y_test.shape))\n",
    "\n",
    "    base_name, ext_name = os.path.splitext(input_file)\n",
    "    x_train_name = \"{}_x_train{}\".format(base_name, ext_name)\n",
    "    x_test_name = \"{}_x_test{}\".format(base_name, ext_name)\n",
    "    y_train_name = \"{}_y_train{}\".format(base_name, ext_name)\n",
    "    y_test_name = \"{}_y_test{}\".format(base_name, ext_name)\n",
    "\n",
    "    x_train.to_csv(x_train_name, index=False)\n",
    "    x_test.to_csv(x_test_name, index=False)\n",
    "    y_train.to_csv(y_train_name, index=False)\n",
    "    y_test.to_csv(y_test_name, index=False)\n",
    "\n",
    "    logging.info(\"x_train saved to {}\".format(x_train_name))\n",
    "    logging.info(\"x_test saved to {}\".format(x_test_name))\n",
    "    logging.info(\"y_train saved to {}\".format(y_train_name))\n",
    "    logging.info(\"y_test saved to {}\".format(y_test_name))\n",
    "    logging.info(\"finished\")\n",
    "    \n",
    "    PreprocessOutput = namedtuple('PreprocessOutput', \n",
    "        ['x_train_name', 'x_test_name', 'y_train_name', 'y_test_name', 'n_classes'])\n",
    "    return PreprocessOutput(\n",
    "        x_train_name=x_train_name,\n",
    "        x_test_name=x_test_name,\n",
    "        y_train_name=y_train_name,\n",
    "        y_test_name=y_test_name,\n",
    "        n_classes=n_classes,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(\n",
    "        job_name: str,\n",
    "        project_id: str,\n",
    "        user_name: str,\n",
    "        bucket_name: str,\n",
    "        job_folder_name: str,\n",
    "        region: str,\n",
    "        train_feature_path: str,\n",
    "        train_label_path: str,\n",
    "        val_feature_path: str,\n",
    "        val_label_path: str,\n",
    "    ) -> NamedTuple('TrainOutput', \n",
    "              [('response', str)]):\n",
    "    from collections import namedtuple\n",
    "    import subprocess\n",
    "\n",
    "\n",
    "    job_dir = 'gs://{}/{}/jobdir'.format(\n",
    "        bucket_name,\n",
    "        job_folder_name,\n",
    "        )\n",
    "    package_path = \"/pipelines/component/trainer\"\n",
    "    job_config = \"/pipelines/component/config/config_hpt.yaml\"\n",
    "    print(\"JOB_NAME = \", job_name)\n",
    "    print(\"JOB_DIR = \", job_dir)\n",
    "    print(\"JOB_CONFIG = \", job_config)\n",
    "        \n",
    "    response = subprocess.run([\n",
    "        \"gcloud\", \"ai-platform\", \"jobs\", \"submit\", \"training\",\n",
    "        job_name,\n",
    "        \"--package-path\", package_path,\n",
    "        \"--module-name\", \"trainer.train_hpt\",\n",
    "        \"--python-version\", \"3.7\",\n",
    "        \"--runtime-version\", \"2.2\",\n",
    "        \"--job-dir\", job_dir,\n",
    "        \"--region\", region,\n",
    "        \"--config\", job_config,\n",
    "        \"--\",\n",
    "        \"--train_feature_name\", train_feature_path,\n",
    "        \"--train_label_name\", train_label_path,\n",
    "        \"--val_feature_name\", val_feature_path,\n",
    "        \"--val_label_name\", val_label_path\n",
    "    ], stdout=subprocess.PIPE)\n",
    "    \n",
    "    response = subprocess.run([\n",
    "        \"gcloud\", \"ai-platform\", \"jobs\", \"describe\", job_name,\n",
    "    ], stdout=subprocess.PIPE)\n",
    "    \n",
    "    TrainOutput = namedtuple('TrainOutput',['response'])\n",
    "        \n",
    "    return TrainOutput(response=response.stdout.decode())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compile python functions to components"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "component_dir = \"./components\"\n",
    "\n",
    "base_image = \"gcr.io/deeplearning-platform-release/tf2-gpu.2-1\"\n",
    "yaml_name = '{}/preprocess.yaml'.format(component_dir)\n",
    "\n",
    "preprocess_op = comp.func_to_container_op(\n",
    "    data_preprocess, \n",
    "    output_component_file=yaml_name,\n",
    "    base_image=base_image)\n",
    "\n",
    "\n",
    "base_image = \"gcr.io/img-seg-3d/trainer:v1\"\n",
    "yaml_name = '{}/train.yaml'.format(component_dir)\n",
    "\n",
    "train_op = comp.func_to_container_op(\n",
    "    train, \n",
    "    output_component_file=yaml_name,\n",
    "    base_image=base_image)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compile KFP pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "@dsl.pipeline(\n",
    "   name='generic prediction pipeline',\n",
    "   description='A pipeline that performs generic seismic image segmentation.'\n",
    ")\n",
    "def train_pipeline(\n",
    "    job_name: str,\n",
    "    project_id: str,\n",
    "    region: str,\n",
    "    user_name: str,\n",
    "    bucket_name: str,\n",
    "    input_blob_name: str,\n",
    "    job_folder_name: str,\n",
    "    target_column: str,\n",
    "    ):\n",
    "    preprocess_task = preprocess_op(\n",
    "        bucket_name = bucket_name,\n",
    "        input_blob_name = input_blob_name,\n",
    "        target_column = target_column,\n",
    "    )\n",
    "    \n",
    "    train_task = train_op(\n",
    "        job_name = job_name,\n",
    "        project_id = project_id,\n",
    "        user_name = user_name,\n",
    "        bucket_name = bucket_name,\n",
    "        job_folder_name = job_folder_name,\n",
    "        region = region,\n",
    "        train_feature_path = preprocess_task.outputs['x_train_name'],\n",
    "        train_label_path = preprocess_task.outputs['y_train_name'],\n",
    "        val_feature_path = preprocess_task.outputs['x_test_name'],\n",
    "        val_label_path = preprocess_task.outputs['y_test_name'],\n",
    "    )\n",
    "    \n",
    "pipeline_pkg_path=\"./train_pipeline.tar.gz\"\n",
    "\n",
    "compiler.Compiler().compile(train_pipeline, package_path=pipeline_pkg_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run KFP pipeline on AI Platform hosted Kubernetes cluster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<a href=\"http://6ff530db99970db2-dot-us-central2.pipelines.googleusercontent.com/#/experiments/details/6f7fbd64-4ef1-405b-8f4c-7a1f1c8e952c\" target=\"_blank\" >Experiment details</a>."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<a href=\"http://6ff530db99970db2-dot-us-central2.pipelines.googleusercontent.com/#/runs/details/9f1c9a80-8f60-4169-87ce-430509152b67\" target=\"_blank\" >Run details</a>."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from datetime import datetime\n",
    "from pytz import timezone\n",
    "my_timezone = 'US/Pacific'\n",
    "\n",
    "params = {\n",
    "    \"job_name\": 'xgb_train_elvinzhu_{}_hpt'.format(\n",
    "        datetime.now(timezone(my_timezone)).strftime(\"%m%d%y_%H%M\")\n",
    "        ),\n",
    "    \"project_id\": 'img-seg-3d',\n",
    "    \"region\": 'us-central1',\n",
    "    \"user_name\": 'elvinzhu',\n",
    "    \"job_folder_name\": 'xgb_train_job',\n",
    "    \"bucket_name\": 'tuti_asset',\n",
    "    \"input_blob_name\": 'datasets/mortgage_structured.csv',\n",
    "    \"target_column\": 'TARGET',\n",
    "}\n",
    "kfp_host_name = '6ff530db99970db2-dot-us-central2.pipelines.googleusercontent.com'\n",
    "kfp_exp_name = 'xgboost_ai_platform'\n",
    "kfp_run_name = 'demo_xgboost'\n",
    "\n",
    "client = kfp.Client(host=kfp_host_name) \n",
    "# Create Experiment GROUP\n",
    "exp = client.create_experiment(name = kfp_exp_name)\n",
    "# Create Experiment RUN\n",
    "run = client.run_pipeline(exp.id, kfp_run_name, pipeline_pkg_path, params=params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "environment": {
   "name": "tf2-gpu.2-1.m61",
   "type": "gcloud",
   "uri": "gcr.io/deeplearning-platform-release/tf2-gpu.2-1:m61"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
