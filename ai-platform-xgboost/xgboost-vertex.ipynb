{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ============================================================================== \\\n",
    " Copyright 2020 Google LLC. This software is provided as-is, without warranty \\\n",
    " or representation for any use or purpose. Your use of it is subject to your \\\n",
    " agreement with Google. \\\n",
    " ============================================================================== \n",
    " \n",
    " Author: Elvin Zhu, Chanchal Chatterjee \\\n",
    " Email: elvinzhu@google.com \\\n",
    "<img src=\"img/google-cloud-icon.jpg\" alt=\"Drawing\" style=\"width: 200px;\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !python3 -m pip install google-cloud-aiplatform\n",
    "# !python3 -m pip install google-cloud-storage==1.32\n",
    "# !gcloud components update"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import packages\n",
    "\n",
    "import json\n",
    "import logging\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from datetime import datetime\n",
    "from pytz import timezone\n",
    "from googleapiclient import discovery\n",
    "from google.cloud import aiplatform"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Configure Global Variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configure your global variables\n",
    "PROJECT = 'img-seg-3d'          # Replace with your project ID\n",
    "USER = 'elvinzhu'               # Replace with your user name\n",
    "BUCKET_NAME = 'vapit_job'  # Replace with your gcs bucket name\n",
    "FOLDER_NAME = 'xgb_train_job'   # Replace with your gcs folder name\n",
    "REGION = 'us-central1'          # Replace with your GCP region\n",
    "TIMEZONE = 'US/Pacific'         # Replace with your local timezone\n",
    "PACKAGE_URIS = f\"gs://{BUCKET_NAME}/trainer/trainer-0.1.tar.gz\" # Replace with your python pakcage uri\n",
    "\n",
    "TRAIN_FEATURE_PATH = f\"gs://{BUCKET_NAME}/data_split/mortgage_structured_x_train.csv\" # Update with your gcs path\n",
    "TRAIN_LABEL_PATH = f\"gs://{BUCKET_NAME}/data_split/mortgage_structured_y_train.csv\" # Update with your gcs path\n",
    "TEST_FEATURE_PATH = f\"gs://{BUCKET_NAME}/data_split/mortgage_structured_x_test.csv\" # Update with your gcs path\n",
    "TEST_LABEL_PATH = f\"gs://{BUCKET_NAME}/data_split/mortgage_structured_y_test.csv\" # Update with your gcs path\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "List your current GCP project name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "img-seg-3d\n"
     ]
    }
   ],
   "source": [
    "!gcloud config list --format 'value(core.project)' 2>/dev/null"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create your bucket"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CommandException: \"mb\" command does not support \"file://\" URLs. Did you mean to use a gs:// URL?\n"
     ]
    }
   ],
   "source": [
    "!gsutil mb gs://$BUCKET_NAME -l $REGION "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Build python package and upload to your bucket"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !cd /home/jupyter/vapit/ai-platform-xgboost\n",
    "# !python3 -m build\n",
    "# !gsutil cp ./dist/trainer-0.1.tar.gz $PACKAGE_URIS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "metadata": {},
   "outputs": [],
   "source": [
    "# freddie mac public mortgage data (Don't change it)\n",
    "INPUT_DATA = \"gs://tuti_asset/datasets/mortgage_structured.csv\" # public mortgage data \n",
    "TARGET_COLUMN = \"TARGET\" # Column name for target labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-----------\n",
    "### Dataset preprocessing\n",
    "\n",
    "Preprocess input data by\n",
    "\n",
    "    1. Dropping unique ID column;\n",
    "    2. Convert categorical into one-hot encodings;\n",
    "    3. Count number of unique classes;\n",
    "    4. Split train/test\n",
    "    5. Save process data into gcs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:root:Preprocessing raw data:\n",
      "INFO:root: => Drop id column:\n",
      "INFO:root: => One hot encoding categorical features\n",
      "INFO:root: => Count number of classes\n",
      "INFO:root: => Perform train/test split\n",
      "INFO:root:Reading raw data file: gs://tuti_asset/datasets/mortgage_structured.csv\n",
      "INFO:root:Drop unique id column which is not an useful feature for ML: LOAN_SEQUENCE_NUMBER\n",
      "INFO:root:Convert categorical columns into one-hot encodings\n",
      "INFO:root:categorical feature: first_time_home_buyer_flag\n",
      "INFO:root:categorical feature: occupancy_status\n",
      "INFO:root:categorical feature: channel\n",
      "INFO:root:categorical feature: property_state\n",
      "INFO:root:categorical feature: property_type\n",
      "INFO:root:categorical feature: loan_purpose\n",
      "INFO:root:categorical feature: seller_name\n",
      "INFO:root:categorical feature: service_name\n",
      "INFO:root:Count number of unique classes ...\n",
      "INFO:root:No. of Classes: 4\n",
      "INFO:root:Perform train/test split ...\n",
      "INFO:root:Get feature/label shapes ...\n",
      "INFO:root:x_train shape = (93639, 149)\n",
      "INFO:root:x_test shape = (10405, 149)\n",
      "INFO:root:y_train shape = (93639,)\n",
      "INFO:root:y_test shape = (10405,)\n",
      "INFO:root:Saving data ...\n",
      "INFO:root:x_train saved to gs://vapit_job/data_split/mortgage_structured_x_train.csv\n",
      "INFO:root:x_test saved to gs://vapit_job/data_split/mortgage_structured_x_test.csv\n",
      "INFO:root:y_train saved to gs://vapit_job/data_split/mortgage_structured_y_train.csv\n",
      "INFO:root:y_test saved to gs://vapit_job/data_split/mortgage_structured_y_test.csv\n",
      "INFO:root:finished\n"
     ]
    }
   ],
   "source": [
    "!python3 preprocessing.py \\\n",
    "    --input_file $INPUT_DATA \\\n",
    "    --x_train_name $TRAIN_FEATURE_PATH \\\n",
    "    --x_test_name $TEST_FEATURE_PATH \\\n",
    "    --y_train_name $TRAIN_LABEL_PATH \\\n",
    "    --y_test_name $TEST_LABEL_PATH \\\n",
    "    --target_column $TARGET_COLUMN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "------\n",
    "### Training with Google Vertex AI "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the full article, please visit: https://cloud.google.com/vertex-ai/docs\n",
    "\n",
    "Where Vertex AI fits in the ML workflow \\\n",
    "The diagram below gives a high-level overview of the stages in an ML workflow. The blue-filled boxes indicate where Vertex AI provides managed services and APIs:\n",
    "\n",
    "<img src=\"img/ml-workflow.svg\" alt=\"Drawing\">\n",
    "\n",
    "As the diagram indicates, you can use Vertex AI to manage the following stages in the ML workflow:\n",
    "\n",
    "- Train an ML model on your data:\n",
    " - Train model\n",
    " - Evaluate model accuracy\n",
    " - Tune hyperparameters\n",
    " \n",
    " \n",
    "- Deploy your trained model.\n",
    "\n",
    "- Send prediction requests to your model:\n",
    " - Online prediction\n",
    " - Batch prediction (for TensorFlow only)\n",
    " \n",
    " \n",
    "- Monitor the predictions on an ongoing basis.\n",
    "\n",
    "- Manage your models and model versions.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "JOB_NAME =  xgb_train_elvinzhu_061321_2020\n",
      "JOB_DIR =  gs://vapit_job/xgb_train_job/xgb_train_elvinzhu_061321_2020\n"
     ]
    }
   ],
   "source": [
    "# Google Cloud AI Platform requires each job to have unique name, \n",
    "# Therefore, we use prefix + timestamp to form job names.\n",
    "JOBNAME = 'xgb_train_{}_{}'.format(\n",
    "    USER,\n",
    "    datetime.now(timezone(TIMEZONE)).strftime(\"%m%d%y_%H%M\")\n",
    "    ) # Unique job name\n",
    "\n",
    "# We use the job names as folder names to store outputs.\n",
    "JOB_DIR = 'gs://{}/{}/{}'.format(\n",
    "    BUCKET_NAME,\n",
    "    FOLDER_NAME,\n",
    "    JOBNAME,\n",
    "    ) # gcs path to hold the outputs\n",
    "\n",
    "# Get the initial set of hyperparameters\n",
    "N_CLASSES = '4' \n",
    "BOOSTER = 'gbtree' # Booster type\n",
    "MAX_DEPTH = '2'      # Depth of trees\n",
    "N_ESTIMATORS = '10'  # No of estimators\n",
    "\n",
    "print(\"JOB_NAME = \", JOBNAME)\n",
    "print(\"JOB_DIR = \", JOB_DIR)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Train at local\n",
    "\n",
    "Before submitting training jobs to Cloud AI Platform, you can test your train.py code in the local environment. You can test by running your python script in command line, but another and maybe better choice is to use `gcloud ai-platform local train` command. The latter method could make sure your your entire python package are ready to be submitted to the remote VMs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train on local machine with python command\n",
    "!python3 trainer/train.py \\\n",
    "    --job-dir ./models \\\n",
    "    --train_feature_name $TRAIN_FEATURE_PATH \\\n",
    "    --train_label_name $TRAIN_LABEL_PATH \\\n",
    "    --no_classes $N_CLASSES \\\n",
    "    --n_estimators $N_ESTIMATORS \\\n",
    "    --max_depth $MAX_DEPTH \\\n",
    "    --booster $BOOSTER"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Submit job to Vertex AI"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Using aiplatform python SDK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "response: name: \"projects/122476304848/locations/us-central1/customJobs/6139747696291872768\"\n",
      "display_name: \"xgb_train_elvinzhu_061321_2020\"\n",
      "job_spec {\n",
      "  worker_pool_specs {\n",
      "    machine_spec {\n",
      "      machine_type: \"n1-standard-4\"\n",
      "    }\n",
      "    replica_count: 1\n",
      "    disk_spec {\n",
      "      boot_disk_type: \"pd-ssd\"\n",
      "      boot_disk_size_gb: 100\n",
      "    }\n",
      "    python_package_spec {\n",
      "      executor_image_uri: \"us-docker.pkg.dev/vertex-ai/training/tf-cpu.2-2:latest\"\n",
      "      package_uris: \"gs://vapit_job/trainer/trainer-0.1.tar.gz\"\n",
      "      python_module: \"trainer.train\"\n",
      "      args: \"--job-dir\"\n",
      "      args: \"gs://vapit_job/xgb_train_job/xgb_train_elvinzhu_061321_2020\"\n",
      "      args: \"--train_feature_name\"\n",
      "      args: \"gs://vapit_job/data_split/mortgage_structured_x_train.csv\"\n",
      "      args: \"--train_label_name\"\n",
      "      args: \"gs://vapit_job/data_split/mortgage_structured_y_train.csv\"\n",
      "      args: \"--no_classes\"\n",
      "      args: \"4\"\n",
      "      args: \"--n_estimators\"\n",
      "      args: \"10\"\n",
      "      args: \"--max_depth\"\n",
      "      args: \"2\"\n",
      "      args: \"--booster\"\n",
      "      args: \"gbtree\"\n",
      "    }\n",
      "  }\n",
      "}\n",
      "state: JOB_STATE_PENDING\n",
      "create_time {\n",
      "  seconds: 1623640846\n",
      "  nanos: 561201000\n",
      "}\n",
      "update_time {\n",
      "  seconds: 1623640846\n",
      "  nanos: 561201000\n",
      "}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "executor_image_uri = 'us-docker.pkg.dev/vertex-ai/training/tf-cpu.2-2:latest'\n",
    "python_module = \"trainer.train\"\n",
    "api_endpoint = \"us-central1-aiplatform.googleapis.com\"\n",
    "machine_type = \"n1-standard-4\"\n",
    "        \n",
    "# The AI Platform services require regional API endpoints.\n",
    "client_options = {\"api_endpoint\": api_endpoint}\n",
    "# Initialize client that will be used to create and send requests.\n",
    "# This client only needs to be created once, and can be reused for multiple requests.\n",
    "client = aiplatform.gapic.JobServiceClient(client_options=client_options)\n",
    "custom_job = {\n",
    "    \"display_name\": JOBNAME,\n",
    "    \"job_spec\": {\n",
    "        \"worker_pool_specs\": [\n",
    "            {\n",
    "                \"machine_spec\": {\n",
    "                    \"machine_type\": machine_type,\n",
    "                },\n",
    "                \"replica_count\": 1,\n",
    "                \"python_package_spec\": {\n",
    "                    \"executor_image_uri\": executor_image_uri,\n",
    "                    \"package_uris\": [PACKAGE_URIS],\n",
    "                    \"python_module\": python_module,\n",
    "                    \"args\": [\n",
    "                      '--job-dir',\n",
    "                      JOB_DIR,\n",
    "                      '--train_feature_name',\n",
    "                      TRAIN_FEATURE_PATH,\n",
    "                      '--train_label_name',\n",
    "                      TRAIN_LABEL_PATH,\n",
    "                      '--no_classes',\n",
    "                      N_CLASSES,\n",
    "                      '--n_estimators',\n",
    "                      N_ESTIMATORS,\n",
    "                      '--max_depth',\n",
    "                      MAX_DEPTH,\n",
    "                      '--booster',\n",
    "                      BOOSTER\n",
    "                    ],\n",
    "                },\n",
    "            }\n",
    "        ]\n",
    "    },\n",
    "}\n",
    "parent = f\"projects/{project}/locations/{REGION}\"\n",
    "response = client.create_custom_job(parent=parent, custom_job=custom_job)\n",
    "print(\"response:\", response)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "------\n",
    "### Hyperparameter Tuning\n",
    "\n",
    "To use hyperparameter tuning in your training job you must perform the following steps:\n",
    "\n",
    "- Specify the hyperparameter tuning configuration for your training job by including a HyperparameterSpec in your TrainingInput object.\n",
    "\n",
    "- Include the following code in your training application:\n",
    "\n",
    " - Parse the command-line arguments representing the hyperparameters you want to tune, and use the values to set the hyperparameters for your training trial.\n",
    " - Add your hyperparameter metric to the summary for your graph.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Gcloud training config\n",
    "\n",
    "# Google Vertex AI requires each job to have unique name, \n",
    "# Therefore, we use prefix + timestamp to form job names.\n",
    "JOBNAME = 'xgb_train_{}_{}_hpt'.format(\n",
    "    USER,\n",
    "    datetime.now(timezone(TIMEZONE)).strftime(\"%m%d%y_%H%M\")\n",
    "    ) # define unique job name\n",
    "\n",
    "# We use the job names as folder names to store outputs.\n",
    "JOB_DIR = 'gs://{}/{}/jobdir'.format(\n",
    "    BUCKET_NAME,\n",
    "    FOLDER_NAME,\n",
    "    ) # define unique job dir on gcs\n",
    "\n",
    "N_CLASSES = '4'\n",
    "\n",
    "print(\"JOB_NAME = \", JOBNAME)\n",
    "print(\"JOB_DIR = \", JOB_DIR)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Submit the hyperparameter job to vertex AI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "executor_image_uri = 'us-docker.pkg.dev/vertex-ai/training/tf-cpu.2-2:latest'\n",
    "python_module =  \"trainer.train_hpt\"\n",
    "api_endpoint = \"us-central1-aiplatform.googleapis.com\"\n",
    "machine_type = \"n1-standard-4\"\n",
    "\n",
    "# The AI Platform services require regional API endpoints.\n",
    "client_options = {\"api_endpoint\": api_endpoint}\n",
    "# Initialize client that will be used to create and send requests.\n",
    "# This client only needs to be created once, and can be reused for multiple requests.\n",
    "client = aiplatform.gapic.JobServiceClient(client_options=client_options)\n",
    "\n",
    "# study_spec\n",
    "metric = {\n",
    "    \"metric_id\": \"roc_auc\",\n",
    "    \"goal\": aiplatform.gapic.StudySpec.MetricSpec.GoalType.MAXIMIZE,\n",
    "}\n",
    "\n",
    "max_depth = {\n",
    "        \"parameter_id\": \"max_depth\",\n",
    "        \"integer_value_spec\": {\"min_value\": 2, \"max_value\": 20},\n",
    "        \"scale_type\": aiplatform.gapic.StudySpec.ParameterSpec.ScaleType.UNIT_LINEAR_SCALE,\n",
    "}\n",
    "n_estimators = {\n",
    "        \"parameter_id\": \"n_estimators\",\n",
    "        \"integer_value_spec\": {\"min_value\": 10, \"max_value\": 200},\n",
    "        \"scale_type\": aiplatform.gapic.StudySpec.ParameterSpec.ScaleType.UNIT_LINEAR_SCALE,\n",
    "}\n",
    "batch_size = {\n",
    "    \"parameter_id\": \"booster\",\n",
    "    \"categorical_value_spec\": {\"values\": [\"gbtree\",\"gblinear\",\"dart\"]},\n",
    "}\n",
    "\n",
    "# trial_job_spec\n",
    "machine_spec = {\n",
    "    \"machine_type\": machine_type,\n",
    "}\n",
    "worker_pool_spec = {\n",
    "    \"machine_spec\": machine_spec,\n",
    "    \"replica_count\": 1,\n",
    "    \"python_package_spec\": {\n",
    "        \"executor_image_uri\": executor_image_uri,\n",
    "        \"package_uris\": [PACKAGE_URIS],\n",
    "        \"python_module\": python_module,\n",
    "        \"args\": [\n",
    "            '--job-dir',\n",
    "            JOB_DIR,\n",
    "            '--train_feature_name',\n",
    "            TRAIN_FEATURE_PATH,\n",
    "            '--train_label_name',\n",
    "            TRAIN_LABEL_PATH,\n",
    "            '--val_feature_name',\n",
    "            TEST_FEATURE_PATH,\n",
    "            '--val_label_name',\n",
    "            TEST_LABEL_PATH,\n",
    "            '--no_classes',\n",
    "            N_CLASSES,\n",
    "        ],\n",
    "    },\n",
    "}\n",
    "\n",
    "# hyperparameter_tuning_job\n",
    "hyperparameter_tuning_job = {\n",
    "    \"display_name\": JOBNAME,\n",
    "    \"max_trial_count\": 4,\n",
    "    \"parallel_trial_count\": 2,\n",
    "    \"study_spec\": {\n",
    "        \"metrics\": [metric],\n",
    "        \"parameters\": [max_depth, n_estimators, batch_size],\n",
    "#         \"algorithm\": aiplatform.gapic.StudySpec.Algorithm.RANDOM_SEARCH,\n",
    "    },\n",
    "    \"trial_job_spec\": {\"worker_pool_specs\": [worker_pool_spec]},\n",
    "}\n",
    "parent = f\"projects/{project}/locations/{REGION}\"\n",
    "response = client.create_hyperparameter_tuning_job(\n",
    "    parent=parent, hyperparameter_tuning_job=hyperparameter_tuning_job\n",
    ")\n",
    "# print(\"response:\", response)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Check the status of Long Running Operation (LRO) with Google API Client\n",
    "\n",
    "Send an API request to Vertex AI to get the detailed information. The most interesting piece of information is the hyperparameter values in the trial with best performance metric."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "client_options = {\"api_endpoint\": api_endpoint}\n",
    "client = aiplatform.gapic.JobServiceClient(client_options=client_options)\n",
    "name = client.hyperparameter_tuning_job_path(\n",
    "    project=project,\n",
    "    location=location,\n",
    "    hyperparameter_tuning_job=3537793011578568704,\n",
    ")\n",
    "response = client.get_hyperparameter_tuning_job(name=name)\n",
    "# print(\"response:\", response)\n",
    "# print(\"response state: \", str(response.state))\n",
    "if \"JobState.JOB_STATE_SUCCEEDED\" == str(response.state):\n",
    "    print(\"Job state succeeded.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Get the hyperparameters associated with the best metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_ind = 0\n",
    "max_val = 0\n",
    "for ind, trials in enumerate(response.trials):\n",
    "    value = trials.final_measurement.metrics[0].value\n",
    "    print(\"Metrics Value (larger is better):\", value)\n",
    "    if value > max_val:\n",
    "        max_val = value\n",
    "        max_ind = ind\n",
    "        \n",
    "param_dict = {}\n",
    "for params in response.trials[max_ind].parameters:\n",
    "    param_dict[params.parameter_id] = params.value\n",
    "    \n",
    "BOOSTER=param_dict['booster']\n",
    "MAX_DEPTH=param_dict['max_depth']\n",
    "N_ESTIMATORS=param_dict['n_estimators']\n",
    "\n",
    "print(\"BOOSTER\", BOOSTER)\n",
    "print(\"MAX_DEPTH\", MAX_DEPTH)\n",
    "print(\"N_ESTIMATORS\",N_ESTIMATORS)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "------\n",
    "### Training with Tuned Parameters\n",
    "\n",
    "Once your hyperparameter training jobs are done. You can use the optimized combination of hyperparameters from your trials and start a single training job on Cloud AI Platform to train your final model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "JOB_NAME =  xgb_train_elvinzhu_061321_2019\n",
      "JOB_DIR =  gs://vapit_job/xgb_train_job/xgb_train_elvinzhu_061321_2019\n",
      "TRAIN_FEATURE_PATH =  gs://vapit_job/data_split/mortgage_structured_x_train.csv\n",
      "TRAIN_LABEL_PATH =  gs://vapit_job/data_split/mortgage_structured_y_train.csv\n",
      "N_CLASSES =  4\n",
      "BOOSTER =  gbtree\n",
      "MAX_DEPTH =  19.0\n",
      "N_ESTIMATORS =  110.0\n"
     ]
    }
   ],
   "source": [
    "# Google Cloud AI Platform requires each job to have unique name, \n",
    "# Therefore, we use prefix + timestamp to form job names.\n",
    "JOBNAME = 'xgb_train_{}_{}'.format(\n",
    "    USER,\n",
    "    datetime.now(timezone(TIMEZONE)).strftime(\"%m%d%y_%H%M\")\n",
    "    )\n",
    "# We use the job names as folder names to store outputs.\n",
    "JOB_DIR = 'gs://{}/{}/{}'.format(\n",
    "    BUCKET_NAME,\n",
    "    FOLDER_NAME,\n",
    "    JOBNAME,\n",
    "    )\n",
    "\n",
    "print(\"JOB_NAME = \", JOBNAME)\n",
    "print(\"JOB_DIR = \", JOB_DIR)\n",
    "\n",
    "N_CLASSES = '4'\n",
    "print(\"TRAIN_FEATURE_PATH = \", TRAIN_FEATURE_PATH)\n",
    "print(\"TRAIN_LABEL_PATH = \", TRAIN_LABEL_PATH)\n",
    "print(\"N_CLASSES = \", N_CLASSES)\n",
    "print(\"BOOSTER = \", BOOSTER)\n",
    "print(\"MAX_DEPTH = \", MAX_DEPTH)\n",
    "print(\"N_ESTIMATORS = \", N_ESTIMATORS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "executor_image_uri = 'us-docker.pkg.dev/vertex-ai/training/tf-cpu.2-2:latest'\n",
    "python_module = \"trainer.train\"\n",
    "api_endpoint = \"us-central1-aiplatform.googleapis.com\"\n",
    "machine_type = \"n1-standard-4\"\n",
    "        \n",
    "# The AI Platform services require regional API endpoints.\n",
    "client_options = {\"api_endpoint\": api_endpoint}\n",
    "# Initialize client that will be used to create and send requests.\n",
    "# This client only needs to be created once, and can be reused for multiple requests.\n",
    "client = aiplatform.gapic.JobServiceClient(client_options=client_options)\n",
    "custom_job = {\n",
    "    \"display_name\": JOBNAME,\n",
    "    \"job_spec\": {\n",
    "        \"worker_pool_specs\": [\n",
    "            {\n",
    "                \"machine_spec\": {\n",
    "                    \"machine_type\": machine_type,\n",
    "                },\n",
    "                \"replica_count\": 1,\n",
    "                \"python_package_spec\": {\n",
    "                    \"executor_image_uri\": executor_image_uri,\n",
    "                    \"package_uris\": [PACKAGE_URIS],\n",
    "                    \"python_module\": python_module,\n",
    "                    \"args\": [\n",
    "                      '--job-dir',\n",
    "                      JOB_DIR,\n",
    "                      '--train_feature_name',\n",
    "                      TRAIN_FEATURE_PATH,\n",
    "                      '--train_label_name',\n",
    "                      TRAIN_LABEL_PATH,\n",
    "                      '--no_classes',\n",
    "                      N_CLASSES,\n",
    "                      '--n_estimators',\n",
    "                      N_ESTIMATORS,\n",
    "                      '--max_depth',\n",
    "                      MAX_DEPTH,\n",
    "                      '--booster',\n",
    "                      BOOSTER\n",
    "                    ],\n",
    "                },\n",
    "            }\n",
    "        ]\n",
    "    },\n",
    "}\n",
    "parent = f\"projects/{project}/locations/{REGION}\"\n",
    "response = client.create_custom_job(parent=parent, custom_job=custom_job)\n",
    "print(\"response:\", response)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check the training job status"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "JobState.JOB_STATE_SUCCEEDED\n"
     ]
    }
   ],
   "source": [
    "# check the training job status\n",
    "client_options = {\"api_endpoint\": api_endpoint}\n",
    "client = aiplatform.gapic.JobServiceClient(client_options=client_options)\n",
    "name = client.custom_job_path(\n",
    "    project=PROJECT,\n",
    "    location=REGION,\n",
    "    custom_job=6139747696291872768,\n",
    ")\n",
    "response = client.get_custom_job(name=name)\n",
    "print(response.state)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "--------\n",
    "### Deploy the Model\n",
    "\n",
    "Vertex AI provides tools to upload your trained ML model to the cloud, so that you can send prediction requests to the model.\n",
    "\n",
    "In order to deploy your trained model on Vertex AI, you must save your trained model using the tools provided by your machine learning framework. This involves serializing the information that represents your trained model into a file which you can deploy for prediction in the cloud.\n",
    "\n",
    "Then you upload the saved model to a Cloud Storage bucket, and create a model resource on Vertex AI, specifying the Cloud Storage path to your saved model.\n",
    "\n",
    "When you deploy your model, you can also provide custom code (beta) to customize how it handles prediction requests.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Import model artifacts to Vertex AI \n",
    "\n",
    "When you import a model, you associate it with a container for Vertex AI to run prediction requests. You can use pre-built containers provided by Vertex AI, or use your own custom containers that you build and push to Container Registry or Artifact Registry.\n",
    "\n",
    "You can use a pre-built container if your model meets the following requirements:\n",
    "\n",
    "- Trained in Python 3.7 or later\n",
    "- Trained using TensorFlow, scikit-learn, or XGBoost\n",
    "- Exported to meet framework-specific requirements for one of the pre-built prediction containers\n",
    "\n",
    "The link to the list of pre-built predict container images:\n",
    "\n",
    "https://cloud.google.com/vertex-ai/docs/predictions/pre-built-containers?_ga=2.125143370.-1302053296.1620920844&_gac=1.221340266.1622086653.CjwKCAjw47eFBhA9EiwAy8kzNOkCqVAmokRvQaxBDOoa8AhGOpzzW69x64rRzfgWxogIn3m6moQoBRoCuOsQAvD_BwE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:google.cloud.aiplatform.models:Creating Model\n",
      "INFO:google.cloud.aiplatform.models:Create Model backing LRO: projects/122476304848/locations/us-central1/models/1143360151491706880/operations/4675220473204703232\n",
      "INFO:google.cloud.aiplatform.models:Model created. Resource name: projects/122476304848/locations/us-central1/models/1143360151491706880\n",
      "INFO:google.cloud.aiplatform.models:To use this Model in another session:\n",
      "INFO:google.cloud.aiplatform.models:model = aiplatform.Model('projects/122476304848/locations/us-central1/models/1143360151491706880')\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<google.cloud.aiplatform.models.Model object at 0x7f119a4fced0> \n",
       "resource name: projects/122476304848/locations/us-central1/models/1143360151491706880"
      ]
     },
     "execution_count": 185,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "MODEL_NAME = \"my_first_XGBoost_model\"\n",
    "\n",
    "aiplatform.Model.upload(\n",
    "    display_name = MODEL_NAME,\n",
    "    serving_container_image_uri = 'us-docker.pkg.dev/vertex-ai/prediction/xgboost-cpu.1-3:latest',\n",
    "    artifact_uri = \"gs://vapit_job/xgb_train_job/xgb_train_elvinzhu_061321_2020\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create Endpoint\n",
    "\n",
    "You need the endpoint ID to deploy the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:google.cloud.aiplatform.models:Creating Endpoint\n",
      "INFO:google.cloud.aiplatform.models:Create Endpoint backing LRO: projects/122476304848/locations/us-central1/endpoints/7700645189408260096/operations/8358320543463636992\n",
      "INFO:google.cloud.aiplatform.models:Endpoint created. Resource name: projects/122476304848/locations/us-central1/endpoints/7700645189408260096\n",
      "INFO:google.cloud.aiplatform.models:To use this Endpoint in another session:\n",
      "INFO:google.cloud.aiplatform.models:endpoint = aiplatform.Endpoint('projects/122476304848/locations/us-central1/endpoints/7700645189408260096')\n"
     ]
    }
   ],
   "source": [
    "MODEL_ENDPOINT_DISPLAY_NAME = \"my_first_XGBoost_model_endpoint\"\n",
    "\n",
    "aiplatform.init(project=PROJECT, location=REGION)\n",
    "endpoint = aiplatform.Endpoint.create(\n",
    "    display_name=MODEL_ENDPOINT_DISPLAY_NAME, project=PROJECT, location=REGION,\n",
    ")\n",
    "\n",
    "endpoint_id = endpoint.resource_name.split('/')[-1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Deploy Model to the endpoint\n",
    "\n",
    "You must deploy a model to an endpoint before that model can be used to serve online predictions; deploying a model associates physical resources with the model so it can serve online predictions with low latency. An undeployed model can serve batch predictions, which do not have the same low latency requirements."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL_NAME = \"my_first_XGBoost_model\"\n",
    "DEPLOYED_MODEL_DISPLAY_NAME = \"my_first_XGBoost_model_deployed\"\n",
    "aiplatform.init(project=PROJECT, location=REGION)\n",
    "\n",
    "model = aiplatform.Model(model_name='1143360151491706880')\n",
    "\n",
    "# The explanation_metadata and explanation_parameters should only be\n",
    "# provided for a custom trained model and not an AutoML model.\n",
    "model.deploy(\n",
    "    endpoint=endpoint,\n",
    "    deployed_model_display_name=DEPLOYED_MODEL_DISPLAY_NAME,\n",
    "    machine_type = \"n1-standard-4\",\n",
    "    sync=True\n",
    ")\n",
    "\n",
    "model.wait()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "------\n",
    "### Send inference requests to your model\n",
    "\n",
    "Vertex AI provides the services you need to request predictions from your model in the cloud.\n",
    "\n",
    "There are two ways to get predictions from trained models: online prediction (sometimes called HTTP prediction) and batch prediction. In both cases, you pass input data to a cloud-hosted machine-learning model and get inferences for each data instance.\n",
    "\n",
    "Vertex AI online prediction is a service optimized to run your data through hosted models with as little latency as possible. You send small batches of data to the service and it returns your predictions in the response."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load testing data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load test feature and labels\n",
    "x_test = pd.read_csv(TEST_FEATURE_PATH)\n",
    "y_test = pd.read_csv(TEST_LABEL_PATH)\n",
    "\n",
    "# Fill nan value with zeros (Prediction lacks the ability to handle nan values for now)\n",
    "x_test = x_test.fillna(0)\n",
    "\n",
    "# Create a temporary json file to contain data to be predicted\n",
    "JSON_TEMP = 'xgb_test_data.json' # temp json file name to hold the inference data\n",
    "batch_size = 100                # data batch size\n",
    "start = 0\n",
    "end = min(ind+batch_size, len(x_test))\n",
    "body={'instances': x_test.iloc[start:end].values.tolist()}\n",
    "# body = json.dumps(body).encode().decode()\n",
    "body = json.dumps(body)\n",
    "with open(JSON_TEMP, 'w') as fp:\n",
    "    fp.write(body)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Call Google API for online inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 225,
   "metadata": {},
   "outputs": [],
   "source": [
    "!curl \\\n",
    "-X POST \\\n",
    "-H \"Authorization: Bearer $(gcloud auth print-access-token)\" \\\n",
    "-H \"Content-Type: application/json\" \\\n",
    "https://us-central1-aiplatform.googleapis.com/v1/projects/$PROJECT_ID/locations/us-central1/endpoints/$endpoint_id:predict \\\n",
    "-d \"@$JSON_TEMP\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "environment": {
   "name": "tf2-gpu.2-1.m61",
   "type": "gcloud",
   "uri": "gcr.io/deeplearning-platform-release/tf2-gpu.2-1:m61"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
